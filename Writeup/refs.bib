
@article{Fama1970,
	ISSN = {00221082, 15406261},
	URL = {http://www.jstor.org/stable/2325486},
	author = {Eugene F. Fama},
	journal = {The Journal of Finance},
	number = {2},
	pages = {383-417},
	publisher = {[American Finance Association, Wiley]},
	title = {Efficient Capital Markets: A Review of Theory and Empirical Work},
	volume = {25},
	year = {1970}
}


@article {Geweke1993,
	author = {Geweke, J.},
	title = {Bayesian treatment of the independent student-t linear model},
	journal = {Journal of Applied Econometrics},
	volume = {8},
	number = {S1},
	publisher = {Wiley Subscription Services, Inc., A Wiley Company},
	issn = {1099-1255},
	url = {http://dx.doi.org/10.1002/jae.3950080504},
	doi = {10.1002/jae.3950080504},
	pages = {S19--S40},
	year = {1993},
}

 %===========AAA=========================================
@Inbook{Akaike1998,
	author="Akaike, Hirotogu",
	editor="Parzen, Emanuel
	and Tanabe, Kunio
	and Kitagawa, Genshiro",
	title="Information Theory and an Extension of the Maximum Likelihood Principle",
	bookTitle="Selected Papers of Hirotugu Akaike",
	year="1998",
	publisher="Springer New York",
	address="New York, NY",
	pages="199-213",
}
@ARTICLE{Alvarez2006,
	author = {Alvarez, Antonio and Amsler, Christine and Orea, Luis and Schmidt,
	Peter},
	title = {Interpreting and Testing the Scaling Property in Models where Inefficiency
	Depends on Firm Characteristics},
	journal = {Journal of Productivity Analysis},
	year = {2006},
	volume = {25},
	pages = {201-212},
	number = {3},
	doi = {10.1007/s11123-006-7639-3},
	issn = {0895-562X},
	keywords = {Stochastic frontier model; Scaling property; Technical inefficiency;
	C12; C31; C52},
	language = {English},
	publisher = {Kluwer Academic Publishers},
	url = {http://dx.doi.org/10.1007/s11123-006-7639-3}
}

@article{Adkins2002,
	jstor_articletype = {research-article},
	title = {Institutions, Freedom, and Technical Efficiency},
	author = {Adkins, Lee C. and Moomaw, Ronald L. and Savvides, Andreas},
	journal = {Southern Economic Journal},
	jstor_issuetitle = {},
	volume = {69},
	number = {1},
	jstor_formatteddate = {Jul., 2002},
	pages = {pp. 92-108},
	url = {http://www.jstor.org/stable/1061558},
	ISSN = {00384038},
	abstract = {The impact of institutions on economic performance is currently the subject of much research. In this study we use panel data to estimate a stochastic production frontier and the sources of inefficiency for a broad set of countries. A maximum-likelihood procedure is used to estimate the parameters for the stochastic production frontier and the determinants of inefficiency simultaneously. Our results show that institutions that promote greater economic freedom in turn promote efficiency.},
	language = {English},
	year = {2002},
	publisher = {Southern Economic Association},
	copyright = {Copyright © 2002 Southern Economic Association},
}

@ARTICLE{Ahn2000,
	author = {Ahn, Seung C. and Sickles, Robin C.},
	title = {Estimation of long-run inefficiency levels: a dynamic frontier approach},
	journal = {Econometric Reviews},
	year = {2000},
	volume = {19},
	pages = {461-492},
	number = {4},
	doi = {10.1080/07474930008800482},
	eprint = { http://dx.doi.org/10.1080/07474930008800482 },
	url = { http://dx.doi.org/10.1080/07474930008800482 
	}
}


@TechReport{Antonio2012,
	author={A. Peyrache and A. N. Rambaldi},
	title={{A State-Space Stochastic Frontier Panel Data Model}},
	year=2012,
	month=Apr,
	institution={School of Economics, University of Queensland, Australia},
	type={CEPA Working Papers Series},
	url={http://ideas.repec.org/p/qld/uqcepa/77.html},
	number={WP012012},
	abstract={In this paper we introduce a state-space approach to the econometric modelling of cross-sectional specific trends (temporal variation in individual heterogeneity) and time varying slopes in the context of panel data regressions. We show that our state-space panel stochastic frontier model nests some of the popular models proposed in the literature on stochastic frontier to accommodate time varying inefficiency and its dynamic version (productivity). A detailed discussion of alternative model specifications is provided and estimation (along with testing procedures for model selection) is presented. The empirical application uses the EU-KLEMS dataset which provides data in the period 1977-2007 for 13 countries and 20 sectors of each economy. Our main empirical interest is centered on productivity analysis and thus we focus on the stochastic frontier interpretation of this cross-sectional specific temporal variation. A post-estimation growth accounting is introduced in order to provide a quantitative assessment of the main factors behind sectoral labour productivity growth for each country.},
	keywords={},
}


@article{Aitchison1976,
	jstor_articletype = {research-article},
	title = {Multivariate Binary Discrimination by the Kernel Method},
	author = {Aitchison, J. and Aitken, C. G. G.},
	journal = {Biometrika},
	volume = {63},
	number = {3},
	jstor_formatteddate = {Dec., 1976},
	pages = { 413-420},
	url = {http://www.jstor.org/stable/2335719},
	ISSN = {00063444},
	abstract = {An extension of the kernel method of density estimation from continuous to multivariate binary spaces is described. Its simple nonparametric nature together with its consistency properties make it an attractive tool in discrimination problems, with some advantages over already proposed parametric counterparts. The method is illustrated by an application to a particular medical diagnostic problem. Simple extensions of the method to the categorical data and to data of mixed binary and continuous form are indicated.},
	language = {English},
	year = {1976},
	publisher = {Biometrika Trust},
	copyright = {Copyright Â© 1976 Biometrika Trust},
} 

@ARTICLE{Aigner1977,
	author = {Dennis Aigner and C.A.Knox Lovell and Peter Schmidt},
	title = {Formulation and estimation of stochastic frontier production function
	models },
	journal = {Journal of Econometrics },
	year = {1977},
	volume = {6},
	pages = {21-37},
	number = {1},
	doi = {http://dx.doi.org/10.1016/0304-4076(77)90052-5},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/0304407677900525}
}

%============BBB============================
@ARTICLE{Badunenko2013,
	author = {Badunenko, Oleg and Henderson, Daniel J. and Russell, R. Robert},
	title = {Polarization of the worldwide distribution of productivity},
	journal = {Journal of Productivity Analysis},
	year = {2013},
	volume = {40},
	pages = {153--171},
	number = {2},
	abstract = {We employ data envelopment analysis (DEA) methods to construct the
	world production frontier, which is in turn used to decompose (labor)
	productivity growth into components attributable to technological
	change (shift of the production frontier), efficiency change (movements
	toward or away from the frontier), physical capital deepening, and
	human capital accumulation over the 1965--2007 period. Using this
	decomposition, we provide new findings on the causes of polarization
	(the emergence of bimodality) and divergence (increased variance)
	of the world productivity distribution. First, unlike earlier studies,
	we find that efficiency change is the unique driver of the emergence
	of a second (higher) mode. Second, while earlier studies attributed
	the overall change in the distribution exclusively to physical capital
	accumulation, we find that technological change and human capital
	accumulation are also significant factors explaining this change
	in the distribution (most notably the emergence of a long right-hand
	tail). Robustness exercises indicate that these revisions of earlier
	findings are attributable to the addition of (more recent) years
	and a much greater number of countries included in our sample. We
	also check to see whether our results are changed by a correction
	for the downward bias in the DEA construction of the frontier, concluding
	that these corrections affect none of our major findings (essentially
	because the level correction roughly washes out in changes.)},
	doi = {10.1007/s11123-012-0328-5},
	issn = {1573-0441},
	url = {http://dx.doi.org/10.1007/s11123-012-0328-5}
}
@article {Bech2017,
	author = {Bech, Morten L., and Garratt, Rodney.},
	title = {Central Bank Cryptocurrencies},
	journal = {Bank International Statements Quarterly Review},
	volume = {},
	number = {},
	year = {2017},
}
@article{Bekaer2000,
	issn = "0893-9454",
	abstract = "It appears that volatility in equity markets is asymmetric: returns and conditional volatility are negatively correlated. We provide a unified framework to simultaneously investigate asymmetric volatility at the firm and the market level and to examine two potential explanations of the asymmetry: leverage effects and volatility feedback. Our empirical application uses the market portfolio and portfolios with different leverage constructed from Nikkei 225 stocks. We reject the pure leverage model of Christie (1982) and find support for a volatility feedback story. Volatility feedback at the firm level is enhanced by strong asymmetries in conditional covariances. Conditional betas do not show significant asymmetries. We document the risk premium implications of these findings.",
	journal = "The Review of Financial Studies",
	pages = "1--42",
	volume = "13",
	publisher = "Oxford University Press",
	number = "1",
	year = "2000",
	title = "Asymmetric Volatility and Risk in Equity Markets",
	author = "Bekaert, Geert and Wu, Guojun",
	keywords = "Studies ; Securities Markets ; Volatility ; Risk Premiums ; Rates of Return ; Stock Prices ; Economic Models ; Capm ; Experimental/Theoretical ; Investment Analysis & Personal Finance ; Economic Theory;",
	month = "January",
}
@BOOK{Bruno1985,
	title = {The Economics of Worlwide Stagflation},
	publisher = {Oxford:Basil Blackwell},
	year = {1985},
	author = {Bruno, M., and Sachs, J.},
	owner = {uqttrin2},
	timestamp = {2016.08.02}
}
@article{Brunk1955,
	author = "Brunk, H. D.",
	doi = "10.1214/aoms/1177728420",
	fjournal = "The Annals of Mathematical Statistics",
	journal = "Ann. Math. Statist.",
	month = "12",
	number = "4",
	pages = "607--616",
	publisher = "The Institute of Mathematical Statistics",
	title = "Maximum Likelihood Estimates of Monotone Parameters",
	url = "http://dx.doi.org/10.1214/aoms/1177728420",
	volume = "26",
	year = "1955"
}

@Article{Broeck1994,
	author={van den Broeck, Julien and Koop, Gary and Osiewalski, Jacek and Steel, Mark F. J.},
	title={{Stochastic frontier models : A Bayesian perspective}},
	journal={Journal of Econometrics},
	year=1994,
	volume={61},
	number={2},
	pages={273-303},
	month={April},
	keywords={},
	abstract={No abstract is available for this item.},
	url={http://ideas.repec.org/a/eee/econom/v61y1994i2p273-303.html}
}
@ARTICLE{Bowan1984,
	author = {Bowman, Adrian W.},
	title = {An Alternative Method of Cross-Validation for the Smoothing of Density
	Estimates},
	journal = {Biometrika},
	year = {1984},
	volume = {71},
	pages = {353-360},
	number = {2},
	abstract = {Cross-validation with Kullback-Leibler loss function has been applied
	to the choice of a smoothing parameter in the kernel method of density
	estimation. A framework for this problem is constructed and used
	to derive an alternative method of cross-validation, based on integrated
	squared error, recently also proposed by Rudemo (1982). Hall (1983)
	has established the consistency and asymptotic optimality of the
	new method. For small and moderate sized samples, the performances
	of the two methods of cross-validation are compared on simulated
	data and specific examples.},
	copyright = {Copyright © 1984 Biometrika Trust},
	issn = {00063444},
	jstor_articletype = {research-article},
	jstor_formatteddate = {Aug., 1984},
	language = {English},
	publisher = {Biometrika Trust}
}

@article{Banker1984,
	abstract = {In management contexts, mathematical programming is usually used to evaluate a collection of possible alternative courses of action en route to selecting one which is best. In this capacity, mathematical programming serves as a planning aid to management. Data Envelopment Analysis reverses this role and employs mathematical programming to obtain ex post facto evaluations of the relative efficiency of management accomplishments, however they may have been planned or executed. Mathematical programming is thereby extended for use as a tool for control and evaluation of past accomplishments as well as a tool to aid in planning future activities. The CCR ratio form introduced by Charnes, Cooper and Rhodes, as part of their Data Envelopment Analysis approach, comprehends both technical and scale inefficiencies via the optimal value of the ratio form, as obtained directly from the data without requiring a priori specification of weights and/or explicit delineation of assumed functional forms of relations between inputs and outputs. A separation into technical and scale efficiencies is accomplished by the methods developed in this paper without altering the latter conditions for use of DEA directly on observational data. Technical inefficiencies are identified with failures to achieve best possible output levels and/or usage of excessive amounts of inputs. Methods for identifying and correcting the magnitudes of these inefficiencies, as supplied in prior work, are illustrated. In the present paper, a new separate variable is introduced which makes it possible to determine whether operations were conducted in regions of increasing, constant or decreasing returns to scale (in multiple input and multiple output situations). The results are discussed and related not only to classical (single output) economics but also to more modern versions of economics which are identified with "contestable market theories."},
	author = {Banker, R. D. , and Charnes,A.,  and  Cooper,W. W.},
	journal = {Management Science},
	number = {9},
	pages = {1078-1092},
	publisher = {INFORMS},
	title = {Some Models for Estimating Technical and Scale Inefficiencies in Data Envelopment Analysis},
	volume = {30},
	year = {1984}
}


@ARTICLE{Battese1995,
	author = {Battese, G.E. and Coelli, T.J.},
	title = {A model for technical inefficiency effects in a stochastic frontier
	production function for panel data},
	journal = {Empirical Economics},
	year = {1995},
	volume = {20},
	pages = {325-332},
	number = {2},
	doi = {10.1007/BF01205442},
	issn = {0377-7332},
	keywords = {C12; C13; C23; C24; C87},
	language = {English},
	publisher = {Physica-Verlag},
	url = {http://dx.doi.org/10.1007/BF01205442}
}
@article{Barro1999,
	ISSN = {13814338, 15737020},
	abstract = {Growth accounting breaks down economic growth into components associated with changes in factor inputs and the Solow residual, which reflects technological progress and other elements. After a presentation of the standard model, the analysis considers dual approaches to growth accounting (which considers changes in factor prices rather than quantities), spillover effects and increasing returns, taxes, and multiple types of factor inputs. Later sections place the growth-accounting exercise within the context of two recent strands of endogenous growth theory--varieties-of-products models and quality-ladders models. Within these settings, the Solow residual can be interpreted in terms of measures of the endogenously changing level of technology.},
	author = {Robert J. Barro},
	journal = {Journal of Economic Growth},
	number = {2},
	pages = {119-137},
	publisher = {Springer},
	title = {Notes on Growth Accounting},
	volume = {4},
	year = {1999}
}


@ARTICLE{Borensztein1998,
	author = {E. Borensztein and J. De Gregorio and J-W. Lee},
	title = {How does foreign direct investment affect economic growth?1 },
	journal = {Journal of International Economics },
	year = {1998},
	volume = {45},
	pages = {115 - 135},
	number = {1},
	doi = {http://dx.doi.org/10.1016/S0022-1996(97)00033-0},
	issn = {0022-1996},
	keywords = {Foreign direct investment},
	url = {http://www.sciencedirect.com/science/article/pii/S0022199697000330}
}
@ARTICLE{Badunenko2008,
	author = {Badunenko, Oleg and Henderson, Daniel J. and Zelenyuk, Valentin},
	title = {Technological Change and Transition: Relative Contributions to Worldwide
	Growth During the 1990s},
	journal = {Oxford Bulletin of Economics and Statistics},
	year = {2008},
	volume = {70},
	pages = {461--492},
	number = {4},
	doi = {10.1111/j.1468-0084.2008.00508.x},
	issn = {1468-0084},
	keywords = {O47, P27, P52},
	publisher = {Blackwell Publishing Ltd},
	url = {http://dx.doi.org/10.1111/j.1468-0084.2008.00508.x}
}
@article{Blanchard1989,
	ISSN = {00028282},
	URL = {http://www.jstor.org/stable/1827924},
	abstract = {We interpret fluctuations in GNP and unemployment as due to two types of disturbances: disturbances that have a permanent effect on output and disturbances that do not. We interpret the first as supply disturbances, the second as demand disturbances. Demand disturbances have a hump-shaped mirror-image effect on output and unemployment. The effect of supply disturbances on output increases steadily over time, peaking after two years and reaching a plateau after five years.},
	author = {Olivier Jean Blanchard and Danny Quah},
	journal = {The American Economic Review},
	number = {4},
	pages = {655-673},
	publisher = {American Economic Association},
	title = {The Dynamic Effects of Aggregate Demand and Supply Disturbances},
	volume = {79},
	year = {1989}
}



@Article{Baumol1986,
	author={Baumol, William J},
	title={{Productivity Growth, Convergence, and Welfare: What the Long-run Data Show}},
	journal={American Economic Review},
	year=1986,
	volume={76},
	number={5},
	pages={1072-85},
	month={December},
	keywords={},
	abstract={ Maddison's 1870-1979 data are analyzed, showing the historically unprecedented growth in productivity, GDP per capita and exports and the remarkable convergence of productivities of industrialized market economies, with convergence apparently shared by planned economies but not less developed countries. Productivity lag's relation to \&quot;deindustrialization,\&quot; unemployment and balance of payments is examined. The data confirm that U.S. productivity growth fell behind its extraordinary postwar peak but probably not below its long term level. It is also shown that more rapid productivity growth of other countries may only be a normal concommitant of convergence. Copyright 1986 by American Economic Association.},
	url={http://ideas.repec.org/a/aea/aecrev/v76y1986i5p1072-85.html}
}
@ARTICLE{Battese1988,
	author = {George E. Battese and Tim J. Coelli},
	title = {Prediction of firm-level technical efficiencies with a generalized
	frontier production function and panel data},
	journal = {Journal of Econometrics},
	year = {1988},
	volume = {38},
	pages = {387-399},
	number = {3},
	doi = {http://dx.doi.org/10.1016/0304-4076(88)90053-X},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/030440768890053X}
}

@ARTICLE{Bickel2008,
	author = {Bickel, Peter J, and Sakov, Anat},
	title = {On the choice of m in the m out of n bootstrap and confidence bounds for extrema},
	journal = {Statistica Sinica},
	year = {2008},
	volume = {18},
	pages = {967-985},
	owner = {uqttrin2},
	timestamp = {2014.04.11}
}
@article{Bowman1984,
	jstor_articletype = {research-article},
	title = {An Alternative Method of Cross-Validation for the Smoothing of Density Estimates},
	author = {Bowman, Adrian W.},
	journal = {Biometrika},
	jstor_issuetitle = {},
	volume = {71},
	number = {2},
	jstor_formatteddate = {Aug., 1984},
	pages = {353-360},
	url = {http://www.jstor.org/stable/2336252},
	ISSN = {00063444},
	abstract = {Cross-validation with Kullback-Leibler loss function has been applied to the choice of a smoothing parameter in the kernel method of density estimation. A framework for this problem is constructed and used to derive an alternative method of cross-validation, based on integrated squared error, recently also proposed by Rudemo (1982). Hall (1983) has established the consistency and asymptotic optimality of the new method. For small and moderate sized samples, the performances of the two methods of cross-validation are compared on simulated data and specific examples.},
	language = {English},
	year = {1984},
	publisher = {Biometrika Trust},
	copyright = {Copyright Â© 1984 Biometrika Trust},
}
@ARTICLE{Battese1992,
	author = {Battese, G.E. and Coelli, T.J.},
	title = {{Frontier production functions, technical efficiency and panel data:
	With application to paddy farmers in India}},
	journal = {Journal of Productivity Analysis},
	year = {1992},
	volume = {3},
	pages = {153-169},
	number = {1-2},
	doi = {10.1007/BF00158774},
	issn = {0895-562X},
	language = {English},
	publisher = {Kluwer Academic Publishers},
	url = {http://dx.doi.org/10.1007/BF00158774}
}

@ARTICLE{Banker1984,
	author = {Banker, R. D. and Charnes, A. and Cooper, W. W.},
	title = {Some Models for Estimating Technical and Scale Inefficiencies in
	Data Envelopment Analysis},
	journal = {Management Science},
	year = {1984},
	volume = {30},
	pages = {1078-1092},
	number = {9},
	doi = {10.1287/mnsc.30.9.1078},
	eprint = { http://dx.doi.org/10.1287/mnsc.30.9.1078 },
	url = { http://dx.doi.org/10.1287/mnsc.30.9.1078 
	}
}
@INCOLLECTION{Badunenko2017,
	author = {Oleg Badunenko and Daniel J. Henderson and Valentin Zelenyuk},
	title = {{The Productivity of Nations}},
	booktitle = {{The Oxford Handbook of Productivity Analysis}},
	year = {2017},
	editor= { Emili Grifell-Tatj\'{e}, C.A. Knox Lovell and Robin C.Sickles},
	chapter = {24},
}

@article{Bernanke1988,
	ISSN = {00028282},
	URL = {http://www.jstor.org/stable/1818164},
	author = {Ben S. Bernanke and Alan S. Blinder},
	journal = {The American Economic Review},
	number = {2},
	pages = {435-439},
	publisher = {American Economic Association},
	title = {Credit, Money, and Aggregate Demand},
	volume = {78},
	year = {1988}
}



@ARTICLE{Banker1986,
	author = {Banker, Rajiv D. and Morey, Richard C.},
	title = {Efficiency Analysis for Exogenously Fixed Inputs and Outputs},
	journal = {Operations Research},
	year = {1986},
	volume = {34},
	pages = {513-521},
	number = {4},
	abstract = { We evaluate, by means of mathematical programming formulations, the
	relative technical and scale efficiencies of decision making units
	(DMUs) when some of the inputs or outputs are exogenously fixed and
	beyond the discretionary control of DMU managers. This approach further
	develops the work on efficiency evaluation and on estimation of efficient
	production frontiers known as data envelopment analysis (DEA). We
	also employ the model to provide efficient input and output targets
	for DMU managers in a way that specifically accounts for the fixed
	nature of some of the inputs or outputs. We illustrate the approach,
	using real data, for a network of fast food restaurants. },
	doi = {10.1287/opre.34.4.513},
	eprint = { http://dx.doi.org/10.1287/opre.34.4.513 },
	url = { http://dx.doi.org/10.1287/opre.34.4.513 
	}
}

@article{Barro1992,
	ISSN = {00223808, 1537534X},
	URL = {http://www.jstor.org/stable/2138606},
	abstract = {A key economic issue is whether poor countries or regions tend to grow faster than rich ones: are there automatic forces that lead to convergence over time in the levels of per capita income and product? We use the neoclassical growth model as a framework to study convergence across the 48 contiguous U.S. states. We exploit data on personal income since 1840 and on gross state product since 1963. The U.S. states provide clear evidence of convergence, but the findings can be reconciled quantitatively with the neoclassical model only if diminishing returns to capital set in very slowly. The results for per capita gross domestic product from a broad sample of countries are similar if we hold constant a set of variables that proxy for differences in steady-state characteristics.},
	author = {Robert J. Barro, and  Sala-i-Martin, Xavier},
	journal = {Journal of Political Economy},  
	number = {2},
	pages = {223-251},  
	publisher = {University of Chicago Press},
	title = {Convergence},
	volume = {100},
	year = {1992}
}


@BOOK{Barro1995,
	title = {Economic Growth},
	publisher = {McGraw-Hill, New York},
	year = {1995},
	author = {Barro, R.J., and Sala-i-Martin,X},
	owner = {uqttrin2},
	timestamp = {2014.12.13}
}
@ARTICLE{Badin2012,
	author = {Luiza B\u{a}din and Cinzia Daraio and LÃ©opold Simar},
	title = {How to measure the impact of environmental factors in a nonparametric
	production model },
	journal = {European Journal of Operational Research },
	year = {2012},
	volume = {223},
	pages = {818-833},
	number = {3},
	doi = {http://dx.doi.org/10.1016/j.ejor.2012.06.028},
	issn = {0377-2217},
	keywords = {Data Envelopment Analysis (DEA)},
	url = {http://www.sciencedirect.com/science/article/pii/S0377221712004833}
}
@article{Braun2001,
	jstor_articletype = {research-article},
	title = {Data Sharpening for Nonparametric Inference Subject to Constraints},
	author = {Braun, W. John and Hall, Peter},
	journal = {Journal of Computational and Graphical Statistics},
	jstor_issuetitle = {},
	volume = {10},
	number = {4},
	jstor_formatteddate = {Dec., 2001},
	pages = {pp. 786-806},
	url = {http://www.jstor.org/stable/1390972},
	ISSN = {10618600},
	abstract = {Data sharpening involves perturbing the data to improve the performance of a statistical method. The versions of it that have been proposed in the past have been for bias reduction in curve estimation, and the amount of perturbation of each datum has been determined by an explicit formula. This article suggests a distance-based form of data sharpening, in which the sum of the distances that data are moved is minimized subject to a constraint imposed on an estimator. The constraint could be one that leads to bias reduction, or to variance or variability reduction, or to a curve estimator being monotone or unimodal. In contrast to earlier versions of the method, in the form presented in this article the amount and extent of sharpening is determined implicitly by a formula that is typically given as the solution of a Lagrange-multiplier equation. Sometimes the solution can be found by Newton-Raphson iteration, although when qualitative constraints are imposed it usually requires quadratic programming or a related method.},
	language = {English},
	year = {2001},
	publisher = {Taylor & Francis, Ltd. on behalf of the American Statistical Association, Institute of Mathematical Statistics, and Interface Foundation of America},
	copyright = {Copyright © 2001 American Statistical Association, Institute of Mathematical Statistics and Interface Foundation of America},
}
%    usepackage[backend=biber]{biblatex}

@techreport{Barth2000,
	title = "The Cost Channel of Monetary Transmission",
	author = "Barth III, Marvin J. and Valerie A. Ramey",
	institution = "National Bureau of Economic Research",
	type = "Working Paper",
	series = "Working Paper Series",
	number = "7675",
	year = "2000",
	month = "04",
	doi = {10.3386/w7675},
	URL = "http://www.nber.org/papers/w7675",
	abstract = {This paper presents evidence that the  cost channel' may be an important part of the monetary transmission mechanism. We argue that if working capital is an essential component of production and distribution, monetary contractions can affect output through a supply channel as well as the traditional demand-type channels. We specify an industry equilibrium model and use it to interpret the results of a VAR analysis. We find that following a monetary contraction, many industries exhibit periods of falling output and rising price-wage ratios, consistent with a supply shock in our model. We also show that the effects are noticeably more pronounced during the period before 1979.},
}
@inproceedings{Black1976,
	author = {Black, Fischer},
	booktitle = {Proceedings of the 1976 Meetings of the American Statistical Association, Business and Economics Statistics Section},
	citeulike-article-id = {7337801},
	keywords = {diss, econometrics, methodology, time-series, volatility},
	pages = {177--181},
	posted-at = {2010-06-17 17:00:34},
	priority = {2},
	title = {{Studies of stock price volatility changes}},
	year = {1976}
}
@article{Brunk1955,
	jstor_articletype = {research-article},
	title = {Maximum Likelihood Estimates of Monotone Parameters},
	author = {Brunk, H. D.},
	journal = {The Annals of Mathematical Statistics},
	jstor_issuetitle = {},
	volume = {26},
	number = {4},
	jstor_formatteddate = {Dec., 1955},
	pages = {pp. 607-616},
	url = {http://www.jstor.org/stable/2236374},
	ISSN = {00034851},
	abstract = {The maximum likelihood estimators of distribution parameters subject to certain order relations are determined for simultaneous sampling from a number of populations, when (i) the order relations may be specified by regarding the distribution parameters, of which one is associated with each population, as values at specified points of a function of n variables (n a positive integer), monotone in each variable separately; (ii) the distributions of the populations from which sample values are taken belong to the exponential family defined below. This family includes, in particular, the binomial, the normal with fixed standard deviation and variable mean, the normal with fixed mean and variable standard deviation, and the Poisson distributions.},
	language = {English},
	year = {1955},
	publisher = {Institute of Mathematical Statistics},
	copyright = {Copyright © 1955 Institute of Mathematical Statistics},
}


%============CCC===============================
@ARTICLE{Charnes1986,
	author = {Charnes, A., and W. Cooper, and B. Golany, and R. Halek, and G. Klopp,
	and E. Schmitz, and D. Thomas,},
	title = {Two Phase Data Envelopment Analysis Approach to Policy Evaluation
	and Management of Army Recruiting Activities:Tradeoffs between Joint
	Services and Army Advertising},
	journal = {Research Report CCS no, 532, Center for Cybernetic studies, The University
	of Texas at Austin Texas},
	year = {1986},
	owner = {uqttrin2},
	timestamp = {2015.04.04}
}
@article{Cox1976,
	title = "The valuation of options for alternative stochastic processes",
	journal = "Journal of Financial Economics",
	volume = "3",
	number = "1",
	pages = "145 - 166",
	year = "1976",
	issn = "0304-405X",
	doi = "https://doi.org/10.1016/0304-405X(76)90023-4",
	url = "http://www.sciencedirect.com/science/article/pii/0304405X76900234",
	author = "John C. Cox and Stephen A. Ross"
}

@Article{Chua2017,
	Title                    = {A Bayesian Approach to Modeling Time-Varying Cointegration and Cointegrating Rank},
	Author                   = {Chew Lian Chua and Sarantis Tsiaplias},
	Journal                  = {Journal of Business \& Economic Statistics},
	Year                     = {2017},
	Number                   = {0},
	Pages                    = {1-11},
	Volume                   = {0},
	
	Doi                      = {10.1080/07350015.2016.1166117},
	Eprint                   = { 
	http://dx.doi.org/10.1080/07350015.2016.1166117
	
	},
	Url                      = { 
	http://dx.doi.org/10.1080/07350015.2016.1166117
	
	}
}

@Article{Christiano1999,
	Title                    = {Chapter 2 Monetary policy shocks: What have we learned and to what end?},
	Author                   = {Lawrence J. Christiano and Martin Eichenbaum and Charles L. Evans},
	Journal                  = {Handbook of Macroeconomics},
	Year                     = {1999},
	Pages                    = {65 - 148},
	Volume                   = {1},
	
	Doi                      = {http://dx.doi.org/10.1016/S1574-0048(99)01005-8},
	ISSN                     = {1574-0048},
	Keywords                 = {monetary policy shocks},
	Url                      = {http://www.sciencedirect.com/science/article/pii/S1574004899010058}
}

@Article{Caves1982,
	author={Caves, Douglas W and Christensen, Laurits R and Diewert, W Erwin},
	title={{The Economic Theory of Index Numbers and the Measurement of Input, Output, and Productivity}},
	journal={Econometrica},
	year=1982,
	volume={50},
	number={6},
	pages={1393-1414},
	month={November},
	keywords={},
	abstract={No abstract is available for this item.},
	url={http://ideas.repec.org/a/ecm/emetrp/v50y1982i6p1393-1414.html}
}
@article{Chib2002,
	title = "Markov chain Monte Carlo methods for stochastic volatility models",
	journal = "Journal of Econometrics",
	volume = "108",
	number = "2",
	pages = "281 - 316",
	year = "2002",
	issn = "0304-4076",
	doi = "https://doi.org/10.1016/S0304-4076(01)00137-3",
	url = "http://www.sciencedirect.com/science/article/pii/S0304407601001373",
	author = "Siddhartha Chib and Federico Nardari and Neil Shephard",
	keywords = "Bayes factor, Markov chain Monte Carlo, Marginal likelihood, Mixture models, Particle filters, Simulation-based inference, Stochastic volatility"
}
@ARTICLE{Chen2010,
	author = {Yao Chen and Wade D. Cook and Joe Zhu},
	title = {Deriving the {DEA} frontier for two-stage processes },
	journal = {European Journal of Operational Research },
	year = {2010},
	volume = {202},
	pages = {138-142},
	number = {1},
	abstract = {Data envelopment analysis (DEA) is a method for measuring the efficiency
	of peer decision making units (DMUs). Recently \{DEA\} has been extended
	to examine the efficiency of two-stage processes, where all the outputs
	from the first stage are intermediate measures that make up the inputs
	to the second stage. The resulting two-stage \{DEA\} model provides
	not only an overall efficiency score for the entire process, but
	as well yields an efficiency score for each of the individual stages.
	Due to the existence of intermediate measures, the usual procedure
	of adjusting the inputs or outputs by the efficiency scores, as in
	the standard \{DEA\} approach, does not necessarily yield a frontier
	projection. The current paper develops an approach for determining
	the frontier points for inefficient \{DMUs\} within the framework
	of two-stage DEA. },
	doi = {http://dx.doi.org/10.1016/j.ejor.2009.05.012},
	issn = {0377-2217},
	keywords = {Data envelopment analysis (DEA)}
}

@Article{Cornwell1990,
	author={Cornwell, Christopher and Schmidt, Peter and Sickles, Robin C.},
	title={{Production frontiers with cross-sectional and time-series variation in efficiency levels}},
	journal={Journal of Econometrics},
	year=1990,
	volume={46},
	number={1-2},
	pages={185-200},
	month={},
	keywords={},
	abstract={No abstract is available for this item.},
	url={http://ideas.repec.org/a/eee/econom/v46y1990i1-2p185-200.html}
}

@Article{Cazals2002,
	author={Cazals, Catherine and Florens, Jean-Pierre and Simar, L\'{e}opold},
	title={{Nonparametric frontier estimation: a robust approach}},
	journal={Journal of Econometrics},
	year=2002,
	volume={106},
	number={1},
	pages={1-25},
	month={January},
	keywords={},
	abstract={No abstract is available for this item.},
	url={http://ideas.repec.org/a/eee/econom/v106y2002i1p1-25.html}
}
@ARTICLE{Chris2012,
	author = {O'Donnell, C.},
	title = {An aggregate quantity framework for measuring and decomposing productivity
	change},
	journal = {Journal of Productivity Analysis},
	year = {2012},
	volume = {38},
	pages = {255-272},
	number = {3},
	doi = {10.1007/s11123-012-0275-1},
	issn = {0895-562X},
	keywords = {Index numbers; Total factor productivity; Efficiency; Malmquist index;
	Hicks-Moorsteen index; Fisher index},
	language = {English},
	publisher = {Springer US},
	url = {http://dx.doi.org/10.1007/s11123-012-0275-1}
}
@ARTICLE{Chris2010,
	author = {O'Donnell, Christopher },
	title = {Measuring and decomposing agricultural productivity and profitability
	change*},
	journal = {Australian Journal of Agricultural and Resource Economics},
	year = {2010},
	volume = {54},
	pages = {527--560},
	number = {4},
	doi = {10.1111/j.1467-8489.2010.00512.x},
	issn = {1467-8489},
	keywords = {economies of scale, economies of scope, mix efficiency, scale efficiency,
	technical change, technical efficiency},
	publisher = {Blackwell Publishing Ltd},
	url = {http://dx.doi.org/10.1111/j.1467-8489.2010.00512.x}
}

@ARTICLE{Chris2015,
	author = {O'Donnell, Christopher },
	title = {Using information about technologies, markets and firm behaviour to estimate and decompose a proper productivity index},
	journal = {Journal of Econometrics},
	year = {2015},
	pages = {},
	number = {},
}
@ARTICLE{Chris2008,
	author = {O'Donnell, ChristopherJ. and Rao, D.S.Prasada and Battese, George E.},
	title = {Metafrontier frameworks for the study of firm-level efficiencies
	and technology ratios},
	journal = {Empirical Economics},
	year = {2008},
	volume = {34},
	pages = {231-255},
	number = {2},
	doi = {10.1007/s00181-007-0119-4},
	issn = {0377-7332},
	keywords = {Stochastic frontier analysis; Data envelopment analysis; Technical
	efficiency},
	language = {English},
	publisher = {Springer-Verlag},
	url = {http://dx.doi.org/10.1007/s00181-007-0119-4}
}

@ARTICLE{Chirikos1994,
	author = {Thomas N. Chirikos and Alan M. Sear},
	title = {Technical efficiency and the competitive behavior of hospitals },
	journal = {Socio-Economic Planning Sciences },
	year = {1994},
	volume = {28},
	pages = {219 - 227},
	number = {4},
	doi = {http://dx.doi.org/10.1016/0038-0121(94)90027-2},
	issn = {0038-0121},
	url = {http://www.sciencedirect.com/science/article/pii/0038012194900272}
}

@article{Christoffersen1998,
	ISSN = {07350015},
	URL = {http://www.jstor.org/stable/1392613},
	abstract = {We consider the forecasting of cointegrated variables, and we show that at long horizons nothing is lost by ignoring cointegration when forecasts are evaluated using standard multivariate forecast accuracy measures. In fact, simple univariate Box-Jenkins forecasts are just as accurate. Our results highlight a potentially important deficiency of standard forecast accuracy measures--they fail to value the maintenance of cointegrating relationships among variables--and we suggest alternatives that explicitly do so.},
	author = {Peter F. Christoffersen and Francis X. Diebold},
	journal = {Journal of Business & Economic Statistics},
	number = {4},
	pages = {450-458},
	publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
	title = {Cointegration and Long-Horizon Forecasting},
	volume = {16},
	year = {1998}
}


@ARTICLE{Charnes1978,
	author = {A. Charnes and W.W. Cooper and E. Rhodes},
	title = {Measuring the efficiency of decision making units },
	journal = {European Journal of Operational Research },
	year = {1978},
	volume = {2},
	pages = {429 - 444},
	number = {6},
	abstract = {A nonlinear (nonconvex) programming model provides a new definition
	of efficiency for use in evaluating activities of not-for-profit
	entities participating in public programs. A scalar measure of the
	efficiency of each participating unit is thereby provided, along
	with methods for objectively determining weights by reference to
	the observational data for the multiple outputs and multiple inputs
	that characterize such programs. Equivalences are established to
	ordinary linear programming models for effecting computations. The
	duals to these linear programming models provide a new way for estimating
	extremal relations from observational data. Connections between engineering
	and economic approaches to efficiency are delineated along with new
	interpretations and ways of using them in evaluating and controlling
	managerial behavior in public programs. },
	doi = {http://dx.doi.org/10.1016/0377-2217(78)90138-8},
	issn = {0377-2217},
	url = {http://www.sciencedirect.com/science/article/pii/0377221778901388}
}

@ARTICLE{Chambers1998,
	author = {Chambers, R.G. and Chung, Y. and F\"{a}re, R.},
	title = {Profit, Directional Distance Functions, and Nerlovian Efficiency},
	journal = {Journal of Optimization Theory and Applications},
	year = {1998},
	volume = {98},
	pages = {351-364},
	number = {2},
	doi = {10.1023/A:1022637501082},
	issn = {0022-3239},
	keywords = {Directional distance functions; Nerlovian efficiency; duality; profit
	functions},
	language = {English},
	publisher = {Kluwer Academic Publishers-Plenum Publishers},
	url = {http://dx.doi.org/10.1023/A%3A1022637501082}
}

@ARTICLE{Charnes1985,
	author = {A Charnes and W.W Cooper and B Golany and L Seiford and J Stutz},
	title = {{Foundations of data envelopment analysis for Pareto-Koopmans efficient
	empirical production functions }},
	journal = {Journal of Econometrics },
	year = {1985},
	volume = {30},
	pages = {91-107},
	abstract = {The construction and analysis of Pareto-efficient frontier production
	functions by a new Data Envelopment Analysis method is presented
	in the context of new theoretical characterizations of the inherent
	structure and capabilities of such empirical production functions.
	Contrasts and connections with other developments, including solutions
	of some remaining problems, are made re aspects such as informatics,
	economics of scale, isotonicity and non-concavity, discretionary
	and non-discretionary inputs, piecewise linearity, partial derivatives
	and Cobb-Douglas properties of the functions. Non-Archimedean constructs
	are not required. },
	doi = {http://dx.doi.org/10.1016/0304-4076(85)90133-2},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/0304407685901332}
}

@TechReport{Cherchye2005,
	author={Cherchye, L. and B. De Borger and T. Van Puyenbroeck },
	title={Nonparametric tests of optimizing behavior in public service provision: Methodology and an application to local safety},
	year=2005,
	month=Jan,
	institution={University of Antwerp, Faculty of Applied Economics},
	type={Working Papers},
	url={http://ideas.repec.org/p/ant/wpaper/2005002.html},
	number={2005002},
	abstract={We develop a positive non-parametric model of public sector production that allows us to test whether an implicit procedure of cost minimization at shadow prices can rationalize the outcomes of public sector activities. The basic model focuses on multiple C-outputs and does not imply any explicit or implicit assumption regarding the trade-offs between the different inputs (in terms of relative shadow prices) or outputs (in terms of relative valuation). The proposed methodology is applied to a cross-section sample of 546 Belgian municipal police forces. Drawing on detailed task-allocation data and controlling, among others, for the presence of state police forces, the cost minimization hypothesis is found to provide a good fit of the data. Imposing additional structure on output valuation, derived from available ordinal information, yields equally convincing goodness-of-fit results. By contrast, we find that aggregating the labor input over task specializations, a common practice in efficiency assessments of police departments, entails a significantly worse fit of the data.},
	keywords={},
}

@article{Caudill1995,
	jstor_articletype = {research-article},
	title = {Frontier Estimation and Firm-Specific Inefficiency Measures in the Presence of Heteroscedasticity},
	author = {Caudill, Steven B. and Jon M. Ford and Gropper, Daniel M.},
	journal = {Journal of Business & Economic Statistics},
	jstor_issuetitle = {},
	volume = {13},
	number = {1},
	jstor_formatteddate = {Jan., 1995},
	pages = {105-111},
	url = {http://www.jstor.org/stable/1392525},
	ISSN = {07350015},
	abstract = {The purpose of this article is to illustrate a straightforward and useful method for addressing the problem of heteroscedasticity in the estimation of frontiers. A heteroscedastic cost-frontier model is developed and estimated using bank cost data similar to that used by Ferrier and Lovell. Our results show dramatic changes in the estimated cost frontier and in the inefficiency measures when accounting for heteroscedasticity in the estimation process. We find that the rankings of firms by their inefficiency measures is affected markedly by the correction for heteroscedasticity but not by alternative distributional assumptions about the one-sided error term.},
	language = {English},
	year = {1995},
	publisher = {Taylor & Francis, Ltd. on behalf of American Statistical Association},
	copyright = {Copyright © 1995 American Statistical Association},
}
@ARTICLE{Caudill1993,
	author = {Steven B. Caudill and Jon M. Ford},
	title = {Biases in frontier estimation due to heteroscedasticity },
	journal = {Economics Letters },
	year = {1993},
	volume = {41},
	pages = {17 - 20},
	number = {1},
	doi = {http://dx.doi.org/10.1016/0165-1765(93)90104-K},
	issn = {0165-1765},
	url = {http://www.sciencedirect.com/science/article/pii/016517659390104K}
}


@article{Cressie1984,
	jstor_articletype = {research-article},
	title = {Multinomial Goodness-of-Fit Tests},
	author = {Cressie, Noel and Read, Timothy R. C.},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	jstor_issuetitle = {},
	volume = {46},
	number = {3},
	jstor_formatteddate = {1984},
	pages = {pp. 440-464},
	url = {http://www.jstor.org/stable/2345686},
	ISSN = {00359246},
	abstract = {This article investigates the family {I<sup>λ</sup>;λ ∈ R} of power divergence statistics for testing the fit of observed frequencies {X<sub>i</sub>;i = 1,...,k} to expected frequencies {E<sub>i</sub>;i = 1,...,k}. From the definition 2nI<sup>λ</sup> = 2/λ(λ + 1) ∑<sup>k</sup><sub>i = 1</sub> X<sub>i</sub>{(X<sub>i</sub>/E<sub>i</sub>)<sup>λ</sup> - 1}; λ ∈ R, it can easily be seen that Pearson's X<sup>2</sup> (λ = 1), the log likelihood ratio statistic (λ = 0), the Freeman-Tukey statistic (λ = -1/2) the modified log likelihood ratio statistic (λ = -1) and the Neyman modified X<sup>2</sup> (λ = -2), are all special cases. Most of the work presented is devoted to an analytic study of the asymptotic difference between different I<sup>λ</sup>, however finite sample results have been presented as a check and a supplement to our conclusions. A new goodness-of-fit statistic, where λ = 2/3, emerges as an excellent and compromising alternative to the old warriors, I<sup>0</sup> and I<sup>1</sup>.},
	language = {English},
	year = {1984},
	publisher = {Wiley for the Royal Statistical Society},
	copyright = {Copyright © 1984 Royal Statistical Society},
}

@ARTICLE{Cornwell1990,
	author = {Christopher Cornwell and Peter Schmidt and Robin C. Sickles},
	title = {Production frontiers with cross-sectional and time-series variation
	in efficiency levels },
	journal = {Journal of Econometrics },
	year = {1990},
	volume = {46}, 
	pages = {185 - 200},
	number = {1-2},
	abstract = {In this paper we consider the efficient instrumental variables estimation
	of a panel data model with heterogeneity in slopes as well as intercepts.
	Using a panel of U.S. airlines, we apply our methodology to a frontier
	production function with cross-sectional and temporal variation in
	levels of technical efficiency. Our approach allows us to estimate
	time-varying efficiency levels for individual firms without invoking
	strong distributional assumptions for technical inefficiency or random
	noise. We do so by including in the production function a flexible
	function of time whose parameterization depends on the firm. We also
	generalize the results of Hausman and Taylor (1981) to exploit assumptions
	about the uncorrelatedness of certain exogenous variables with the
	temporal pattern of the firm's technical inefficiency. Our empirical
	analysis of the airline industry over two periods of regulation yields
	believable evidence on the pattern of changes in efficiency across
	regulatory environments. },
	doi = {http://dx.doi.org/10.1016/0304-4076(90)90054-W},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/030440769090054W}
}
@Book{Campbell1997,
	Title                    = {The econometrics of financial markets / John Y. Campbell, Andrew W. Lo, A. Craig MacKinlay.},
	Author                   = {Campbell, John Y and Lo, Andrew W. (Andrew Wen-Chuan) and MacKinlay, Archie Craig},
	Publisher                = {Princeton University Press},
	Year                     = {1997},
	
	Address                  = {Princeton, N.J.},
	
	Booktitle                = {The econometrics of financial markets},
	ISBN                     = {0691043019},
	Keywords                 = {Capital market -- Econometric models},
	Language                 = {eng},
	Lccn                     = {96027868}
}


@article{Choi1999,
	jstor_articletype = {research-article},
	title = {Data Sharpening as a Prelude to Density Estimation},
	author = {Choi, Edwin and Hall, Peter},
	journal = {Biometrika},
	jstor_issuetitle = {},
	volume = {86},
	number = {4},
	jstor_formatteddate = {Dec., 1999},
	pages = {pp. 941-947},
	url = {http://www.jstor.org/stable/2673598},
	ISSN = {00063444},
	abstract = {We introduce a data-perturbation method for reducing bias of a wide variety of density estimators, in univariate, multivariate, spatial and spherical data settings. The method involves `sharpening' the data by making them slightly more clustered than before, and then computing the estimator in the usual way, but from the sharpened data rather than the original data. The transformation depends in a simple, explicit way on the smoothing parameter employed for the density estimator, which may be based on classical kernel methods, orthogonal series, histosplines, singular integrals or other linear or approximately-linear methods. Bias is reduced by an order of magnitude, at the expense of a constant-factor increase in variance.},
	language = {English},
	year = {1999},
	publisher = {Biometrika Trust},
	copyright = {Copyright © 1999 Biometrika Trust},
}

@TechReport{Chernozhukov2007,
	author={Victor Chernozhukov and Ivan Fernandez-Val and Alfred Galichon},
	title={{Improving point and interval estimates of monotone functions by rearrangement}},
	year=2008,
	month=Jul,
	institution={Centre for Microdata Methods and Practice, Institute for Fiscal Studies},
	type={CeMMAP working papers},
	url={http://ideas.repec.org/p/ifs/cemmap/17-08.html},
	number={CWP17/08},
	abstract={Suppose that a target function is monotonic, namely weakly increasing, and an original estimate of this target function is available, which is not weakly increasing. Many common estimation methods used in statistics produce such estimates. We show that these estimates can always be improved with no harm by using rearrangement techniques: The rearrangement methods, univariate and multivariate, transform the original estimate to a monotonic estimate, and the resulting estimate is closer to the true curve in common metrics than the original estimate. The improvement property of the rearrangement also extends to the construction of confidence bands for monotone functions. Let l and u be the lower and upper endpoint functions of a simultaneous confidence interval [l,u] that covers the true function with probability (1-a), then the rearranged confidence interval, defined by the rearranged lower and upper end-point functions, is shorter in length in common norms than the original interval and covers the true function with probability greater or equal to (1-a). We illustrate the results with a computational example and an empirical example dealing with age-height growth charts. Please note: This paper is a revised version of cemmap working Paper CWP09/07.},
	keywords={},
}
@article{Campbell1992,
	title = "No news is good news: An asymmetric model of changing volatility in stock returns",
	journal = "Journal of Financial Economics",
	volume = "31",
	number = "3",
	pages = "281 - 318",
	year = "1992",
	issn = "0304-405X",
	doi = "https://doi.org/10.1016/0304-405X(92)90037-X",
	url = "http://www.sciencedirect.com/science/article/pii/0304405X9290037X",
	author = "John Y. Campbell and Ludger Hentschel"
}
@ARTICLE{Coe1995,
	author = {David T. Coe and Elhanan Helpman},
	title = {{International R\&D spillovers}},
	journal = {European Economic Review },
	year = {1995},
	volume = {39},
	pages = {859 - 887},
	number = {5},
	doi = {http://dx.doi.org/10.1016/0014-2921(94)00100-E},
	issn = {0014-2921}, 
	keywords = {Productivity},
	url = {http://www.sciencedirect.com/science/article/pii/001429219400100E}
}

@ARTICLE{Cogley2005,
	author = {Timothy Cogley and Thomas J. Sargent},
	title = {Drifts and volatilities: monetary policies and outcomes in the post
	\{WWII\} \{US\} },
	journal = {Review of Economic Dynamics },
	year = {2005},
	volume = {8},
	pages = {262 - 302},
	number = {2},
	note = {Monetary Policy and Learning },
	doi = {http://dx.doi.org/10.1016/j.red.2004.10.009},
	issn = {1094-2025},
	url = {http://www.sciencedirect.com/science/article/pii/S1094202505000049}
}
@ARTICLE{Chib1995,
	author = {Chib, Siddhartha and Greenberg, Edward},
	title = {Understanding the {Metropolis-Hastings} Algorithm},
	journal = {The American Statistician},
	year = {1995},
	volume = {49},
	pages = {327-335},
	number = {4},
	jstor_articletype = {research-article},
	jstor_formatteddate = {Nov., 1995},
	language = {English},
	publisher = {Taylor \& Francis, Ltd. on behalf of the American Statistical Association},
	url = {http://www.jstor.org/stable/2684568}
}

@INCOLLECTION{Caselli2005,
	author = {Caselli, Francesco},
	title = {{Accounting for Cross-Country Income Differences}},
	booktitle = {{Handbook of Economic Growth}},
	publisher = {Elsevier},
	year = {2005},
	editor = {Philippe Aghion and Steven Durlauf},
	volume = {1},
	series = {Handbook of Economic Growth},
	chapter = {9},
	pages = {679-741},
	abstract = {Why are some countries so much richer than others? Development Accounting
	is a first-pass attempt at organizing the answer around two proximate
	determinants: factors of production and efficiency. It answers the
	question \&quot;how much of the cross-country income variance can
	be attributed to differences in (physical and human) capital, and
	how much to differences in the efficiency with which capital is used?\&quot;
	Hence, it does for the cross-section what growth accounting does
	in the time series. The current consensus is that efficiency is at
	least as important as capital in explaining income differences. I
	survey the data and the basic methods that lead to this consensus,
	and explore several extensions. I argue that some of these extensions
	may lead to a reconsideration of the evidence.},
	url = {http://ideas.repec.org/h/eee/grochp/1-09.html}
} 
@ARTICLE{Cochrane1998,
	author = {John H. Cochrane},
	title = {{What do the {VARs} mean? Measuring the output effects of monetary
	policy }},
	journal = {Journal of Monetary Economics },
	year = {1998},
	volume = {41},
	pages = {277 - 300},
	number = {2},
	doi = {http://dx.doi.org/10.1016/S0304-3932(97)00075-5},
	issn = {0304-3932},
	keywords = {Monetary policy},
	url = {http://www.sciencedirect.com/science/article/pii/S0304393297000755}
}

%============DDD===============================
@article{Deo2000,
	title = "Spectral tests of the martingale hypothesis under conditional heteroscedasticity",
	journal = "Journal of Econometrics",
	volume = "99",
	number = "2",
	pages = "291 - 315",
	year = "2000",
	issn = "0304-4076",
	doi = "https://doi.org/10.1016/S0304-4076(00)00027-0",
	url = "http://www.sciencedirect.com/science/article/pii/S0304407600000270",
	author = "Rohit S. Deo",
	keywords = "Sample spectral distribution function",
	keywords = "Martingale difference",
	keywords = "Conditional heteroscedasticity",
	keywords = "Cramér von-Mises statistic"
}
@article{Darlauf1991,
	title = "Spectral based testing of the martingale hypothesis",
	journal = "Journal of Econometrics",
	volume = "50",
	number = "3",
	pages = "355 - 376",
	year = "1991",
	issn = "0304-4076",
	doi = "https://doi.org/10.1016/0304-4076(91)90025-9",
	url = "http://www.sciencedirect.com/science/article/pii/0304407691900259",
	author = "Steven N. Durlauf"
}
@INPROCEEDINGS{Doucet2011,
	author = {Doucet, Arnaud and Johansen, Adam M.},
	title = {{A tutorial on particle filtering and smoothing: fifteen years later}},
	booktitle = {Oxford Handbook of nonlinear filtering},
	year = {2011},
	pages = {656-704}
}
@TechReport{Duygun2013,
	author={ Duygun,Meryem; and Levent, Kutlu; and Robin C.Sickles},
	title={{Measuring Efficiency: A Kalman Filter Approach}},
	year=2013,
	month=07,
	institution={},
	type={ Working Paper}
}
@article{Du2013,
	title = "Nonparametric kernel regression with multiple predictors and multiple shape constraints",
	publisher = "Academia Sinica Institute of Statistical Science",
	author = "Peng Du and C.F. Parmeter and J.S. Racine",
	year = "2013",
	doi = "10.5705/ss.2012.024",
	volume = "23",
	number = "3",
	pages = "1347--1371",
	journal = "Statistica Sinica",
	issn = "1017-0405",
}
@BOOK{Doucet2001,
	title = {Sequential Monte Carlo Methods in Practice},
	publisher = {Springer, New York.},
	year = {2001},
	author = {Doucet, Arnaud, De Freitas, A., and Gordon, N.J. and Johansen, Adam M.},
	owner = {uqttrin2},
	timestamp = {2014.07.18}
}
@article{Durbin1997,
	author = {Durbin, J. and Koopman, S. J.}, 
	title = {Monte Carlo maximum likelihood estimation for non-Gaussian state space models},
	volume = {84}, 
	number = {3}, 
	pages = {669-684}, 
	year = {1997}, 
	doi = {10.1093/biomet/84.3.669}, 
	abstract ={State space models are considered for observations which have non-Gaussian distributions. We obtain accurate approximations to the loglikelihood for such models by Monte Carlo simulation. Devices are introduced which improve the accuracy of the approximations and which increase computational efficiency. The loglikelihood function is maximised numerically to obtain estimates of the unknown hyperparameters. Standard errors of the estimates due to simulation are calculated. Details are given for the important special cases where the observations come from an exponential family distribution and where the observation equation is linear but the observation errors are non-Gaussian. The techniques are illustrated with a series for which the observations have a Poisson distribution and a series for which the observation errors have a t-distribution.}, 
	URL = {http://biomet.oxfordjournals.org/content/84/3/669.abstract}, 
	eprint = {http://biomet.oxfordjournals.org/content/84/3/669.full.pdf+html}, 
	journal = {Biometrika} 
}
@ARTICLE{Desli2003,
	author = {Desli, Evangelia and Ray, Subhash C. and Kumbhakar, Subal C.},
	title = {A dynamic stochastic frontier production model with time-varying
	efficiency},
	journal = {Applied Economics Letters},
	year = {2003},
	volume = {10},
	pages = {623-626},
	number = {10},
	doi = {10.1080/1350485032000133291},
	eprint = { http://dx.doi.org/10.1080/1350485032000133291 },
	url = { http://dx.doi.org/10.1080/1350485032000133291 
	}
}


@ARTICLE{Daouia2007,
	author = {Abdelaati Daouia and Simar, L\'{e}opold},
	title = {Nonparametric efficiency analysis: A multivariate conditional quantile
	approach },
	journal = {Journal of Econometrics },
	year = {2007},
	volume = {140},
	pages = {375 - 400},
	number = {2},
	doi = {http://dx.doi.org/10.1016/j.jeconom.2006.07.002},
	issn = {0304-4076},
	keywords = {Efficiency},
	url = {http://www.sciencedirect.com/science/article/pii/S0304407606001400}
}

@article{Daraio2005,
	author = {Daraio, Cinzia and Simar,  L\'{e}opold},
	title = {Introducing Environmental Variables in Nonparametric Frontier Models:
	a Probabilistic Approach},
	journal = {Journal of Productivity Analysis},
	year = {2005},
	volume = {24},
	pages = {93-121},
	number = {1},
	doi = {10.1007/s11123-005-3042-8},
	issn = {0895-562X},
	keywords = {production function; frontier; nonparametric estimation; environmental
	factors; robust estimation},
	language = {English},
	publisher = {Kluwer Academic Publishers},
	url = {http://dx.doi.org/10.1007/s11123-005-3042-8}
}

@ARTICLE{Daraio2007,
	author = {Daraio, Cinzia and Simar, L\'{e}opold},
	title = {Conditional nonparametric frontier models for convex and nonconvex
	technologies: a unifying approach},
	journal = {Journal of Productivity Analysis},
	year = {2007},
	volume = {28},
	pages = {13-32},
	number = {1-2},
	doi = {10.1007/s11123-007-0049-3},
	issn = {0895-562X},
	keywords = {Convexity; External-environmental factors; Production frontier; Nonparametric
	estimation; Robust estimation; C13; C14; D20},
	language = {English},
	publisher = {Springer US},
	url = {http://dx.doi.org/10.1007/s11123-007-0049-3}
}
@ARTICLE{Duin1976, 
	author={R. P. W. Duin}, 
	journal={IEEE Transactions on Computers}, 
	title={On the Choice of Smoothing Parameters for Parzen Estimators of Probability Density Functions}, 
	year={1976}, 
	volume={C-25}, 
	number={11}, 
	pages={1175-1179}, 
	keywords={Data analysis, multidimensional probability density function, multimodal probability density function, Parzen estimators, pattern recognition, smoothing.;Covariance matrix;Kernel;Optimization methods;Parameter estimation;Physics;Probability density function;Smoothing methods;Yield estimation;Data analysis, multidimensional probability density function, multimodal probability density function, Parzen estimators, pattern recognition, smoothing.}, 
	doi={10.1109/TC.1976.1674577}, 
	ISSN={0018-9340}, 
	month={Nov},}
@ARTICLE{Deprins1984,
	author = {De Prins, D. and Simar, L\'{e}opold, and H. Tulkens  },
	title = { Measuring labour efficiency in post offices},
	journal = {The Performance of Public Enterprises: Concepts and Measurement (Eds.) M. Marchand, P. 
	Pestieau and H. Tulkens, North Holland, Amsterdam},
	year = {1984},
	volume = {},
	pages = {243-267},
}

@article{Diewert1971,
	jstor_articletype = {research-article},
	title = {{An Application of the Shephard Duality Theorem: A Generalized Leontief Production Function}},
	author = {Diewert, W. E.},
	journal = {Journal of Political Economy},
	jstor_issuetitle = {},
	volume = {79},
	number = {3},
	jstor_formatteddate = {May - Jun., 1971},
	pages = { 481-507},
	url = {http://www.jstor.org/stable/1830768},
	ISSN = {00223808},
	abstract = {The paper indicates how the Shephard duality theorem may be utilized in order to obtain a system of derived demand equations which are linear in the technological parameters, thus facilitating econometric estimation. This theorem states that technology may be equivalently represented by either a production function or a cost function, and a proof of the theorem is given. The chosen functional form is a quadratic form in the square roots of input prices and is a generalization of the Leontief cost function. The generalization has the property that it can attain any set of partial elasticities of substitution using a minimal number of parameters.},
	language = {English},
	year = {1971},
	publisher = {The University of Chicago Press},
	copyright = {Copyright Â© 1971 The University of Chicago Press},
}
@article{Delgado2001,
	author = "Delgado, Miguel A. and Manteiga, Wenceslao Gonz\'{a}lez",
	doi = "10.1214/aos/1013203462",
	journal = "The Annals of Statistics",
	month = "10",
	number = "5",
	pages = "1469--1507",
	publisher = "The Institute of Mathematical Statistics",
	title = "Significance testing in nonparametric regression based on the
	bootstrap",
	url = "http://dx.doi.org/10.1214/aos/1013203462",
	volume = "29",
	year = "2001"
}


@article{Diewert1974,
	jstor_articletype = {research-article},
	title = {{A Note on Aggregation and Elasticities of Substitution}},
	author = {Diewert, W.E.},
	journal = {The Canadian Journal of Economics / Revue canadienne d'Economique},
	jstor_issuetitle = {},
	volume = {7},
	number = {1},
	jstor_formatteddate = {Feb., 1974},
	pages = {12-20},
	url = {http://www.jstor.org/stable/134211},
	ISSN = {00084085},
	abstract = {A note on agÇµregation and elasticities of substitution. Hicks' Aggregation Theorem states that if the prices of a group of commodities vary in strict proportion over time, then that group of commodities can be aggregated into a single composite commodity without any aggregation error occurring on the markets which were not aggregated. The paper shows that an approximate version of Hicks' Aggregation Theorem also holds; i.e., if the prices of a group of commodities vary in proportion except for small deviations, then aggregation errors will also be small. The paper also develops some relationships between elasticities of substitution under varying degrees of aggregation. /// Note sur l'agrÃ©gation et les Ã©lasticitÃ©s de substitution. L'un des problÃ¨mes les plus difficiles auquel on fait face en Ã©conomie appliquÃ©e quand on cherche Ã  reprÃ©senter certains marchÃ©s spÃ©cifiques (comme les marchÃ©s du travail), consiste Ã  dÃ©terminer jusqu'Ã  quel point les autres marchÃ©s, dont l'intÃ©rÃªt est moins important, peuvent Ãªtre agrÃ©gÃ©s sans diminuer la prÃ©cision des prÃ©visions des comportements sur les marchÃ©s auxquels on s'intÃ©resse. Supposons que l'analyse soit faite dans le contexte de la thÃ©orie de la production et que l'on cherche Ã  prÃ©voir la demande pour diffÃ©rents types de main-d'Å“uvre comme Ã©tant fonction de la production, des taux de rÃ©munÃ©ration et des prix des autres facteurs. Dans ce cas, le thÃ©orÃ¨me de Hicks sur l'agrÃ©gation nous dit que si les prix des autres biens varient de faÃ§on strictement proportionnelle dans le temps, cet ensemble de biens peut Ãªtre considÃ©rÃ© globalement sans conduire Ã  des erreurs de prÃ©vision en ce qui concerne les marchÃ©s qui nous intÃ©ressent. Le prÃ©sent article montre qu'une version moins exigeante du thÃ©orÃ¨me de Hicks est aussi valable. De faÃ§on plus prÃ©cise, si les prix des autres biens varient dans la mÃªme proportion Ã  l'exception de variations petites, les erreurs reliÃ©es Ã  l'agrÃ©gation sont aussi petites. On montre aussi dans l'article que les Ã©lasticitÃ©s de substitution ont tendance Ã  devenir d'autant plus grandes que le degrÃ© de dÃ©sagrÃ©gation est plus poussÃ©. Finalement, on montre que l'erreur de prÃ©vision sur le MiÃ¨me marchÃ© du travail due Ã  l'agrÃ©gation des autres facteurs quand il y a une variation non proportionnelle du prix du premier de ces autres facteurs, est Ã©gale au chiffre de l'Ã©lasticitÃ© de substitution entre le MiÃ¨me type de main-d'Å“uvre et le premier parmi les autres facteurs (X1), multipliÃ© par la part de X1 dans le coÃ»t total, multipliÃ© par le changement en pourcentage du prix de X1 relativement Ã  l'Ã©volution gÃ©nÃ©rale des prix des autres facteurs. L'auteur Ã©labore sur quelques implications de cette formule.},
	language = {English},
	year = {1974},
	publisher = {Wiley on behalf of the Canadian Economics Association},
	copyright = {Copyright Â© 1974 Canadian Economics Association},
}


@article{Dette2006,
	jstor_articletype = {research-article},
	title = {A Simple Nonparametric Estimator of a Strictly Monotone Regression Function},
	author = {Dette, Holger and Neumeyer, Natalie and Pilz, Kay F.},
	journal = {Bernoulli},
	jstor_issuetitle = {},
	volume = {12},
	number = {3},
	jstor_formatteddate = {Jun., 2006},
	pages = {pp. 469-490},
	url = {http://www.jstor.org/stable/25464816},
	ISSN = {13507265},
	abstract = {A new method for monotone estimation of a regression function is proposed, which is potentially attractive to users of conventional smoothing methods. The main idea of the new approach is to construct a density estimate from the estimated values m̂(i/N) (i = 1,..., N) of the regression function and to use these 'data' for the calculation of an estimate of the inverse of the regression function. The final estimate is then obtained by a numerical inversion. Compared to the currently available techniques for monotone estimation the new method does not require constrained optimization. We prove asymptotic normality of the new estimate and compare the asymptotic properties with the unconstrained estimate. In particular, it is shown that for kernel estimates or local polynomials the bandwidths in the procedure can be chosen such that the monotone estimate is first-order asymptotically equivalent to the unconstrained estimate. We also illustrate the performance of the new procedure by means of a simulation study.},
	language = {English},
	year = {2006},
	publisher = {International Statistical Institute (ISI) and Bernoulli Society for Mathematical Statistics and Probability},
	copyright = {Copyright © 2006 International Statistical Institute (ISI) and Bernoulli Society for Mathematical Statistics and Probability},
}

@ARTICLE{Dierckx1980,
	author = {Dierckx, P.},
	title = {Algorithm/algorithmus 42 an algorithm for cubic spline fitting with
	convexity constraints},
	journal = {Computing},
	year = {1980},
	volume = {24},
	pages = {349-371},
	number = {4},
	doi = {10.1007/BF02237820},
	issn = {0010-485X},
	language = {English},
	publisher = {Springer-Verlag},
	url = {http://dx.doi.org/10.1007/BF02237820}
}

%============EEE================================

@Article{Efron1979,
	author={Efron,Bradley},
	title={Another Look at the Jackknife},
	journal={The Annals of Statistics},
	year=1979,
	volume={7},
	number={1},
	pages={1-26},
	month={},
	keywords={},
	
}

@article{Emvalomatis2011,
	author = {Emvalomatis, Grigorios and Stefanou, Spiro E. and Lansink, Alfons Oude}, 
	title = {A Reduced-Form Model for Dynamic Efficiency Measurement: Application to Dairy Farms in {Germany} and The {Netherlands}},
	volume = {93}, 
	number = {1}, 
	pages = {161-174}, 
	year = {2011}, 
	doi = {10.1093/ajae/aaq125}, 
	URL = {http://ajae.oxfordjournals.org/content/93/1/161.abstract}, 
	eprint = {http://ajae.oxfordjournals.org/content/93/1/161.full.pdf+html}, 
	journal = {American Journal of Agricultural Economics} 
}

@Article{Es1991,
	Title                    = {Likelihood cross-validation bandwidth selection for nonparametric kernel density estimators},
	Author                   = {Bert Van Es},
	Journal                  = {Journal of Nonparametric Statistics},
	Year                     = {1991},
	Number                   = {1-2},
	Pages                    = {83-110},
	Volume                   = {1},
	
	Doi                      = {10.1080/10485259108832513},
	Eprint                   = { 
	http://dx.doi.org/10.1080/10485259108832513
	
	},
	Url                      = { 
	http://dx.doi.org/10.1080/10485259108832513
	
	}
}

@BOOK{Eichhron1976,
	title = {Theory of the Price Index},
	publisher = {Lecture Notes in Economics and Mathematical Systems, 140, Berlin:
	Springer.},
	year = {1976},
	author = {Eichhorn, W., J. Voeller},
	owner = {uqttrin2},
	timestamp = {2014.07.18}
}

@BOOK{Efron1982,
	title = {The Jackknife, the Bootstrap and Other Resampling Plans},
	publisher = {Society for Industrial and Applied Mathematics},
	year = {1982},
	author = {Efron, B.},
	doi = {10.1137/1.9781611970319},
	eprint = {http://epubs.siam.org/doi/pdf/10.1137/1.9781611970319},
	url = {http://epubs.siam.org/doi/abs/10.1137/1.9781611970319}
}

@article{Engle1993,
	ISSN = {00221082, 15406261},
	URL = {http://www.jstor.org/stable/2329066},
	abstract = {This paper defines the news impact curve which measures how new information is incorporated into volatility estimates. Various new and existing ARCH models including a partially nonparametric one are compared and estimated with daily Japanese stock return data. New diagnostic tests are presented which emphasize the asymmetry of the volatility response to news. Our results suggest that the model by Glosten, Jagannathan, and Runkle is the best parametric model. The EGARCH also can capture most of the asymmetry; however, there is evidence that the variability of the conditional variance implied by the EGARCH is too high.},
	author = {Robert F. Engle and Victor K. Ng},
	journal = {The Journal of Finance},
	number = {5},
	pages = {1749-1778},
	publisher = {[American Finance Association, Wiley]},
	title = {Measuring and Testing the Impact of News on Volatility},
	volume = {48},
	year = {1993}
}



%============FFF=================================
@Article{Fare1994,
	author={F\"{a}re, Rolf and Shawna Grosskopf and Mary Norris and Zhongyang Zhang},
	title={{Productivity Growth, Technical Progress, and Efficiency Change in Industrialized Countries}},
	journal={American Economic Review},
	year=1994,
	volume={84},
	number={1},
	pages={66-83},
	month={March},
	keywords={},
	abstract={ This paper analyzes productivity growth in seventeen OECD countries over the period 1979-88. A nonparametric programming method (activity analysis) is used to compute Malmquist productivity indexes. These are decomposed into two component measures, namely, technical change and efficiency change. The authors find that U.S. productivity growth is slightly higher than average, all of which is due to technical change. Japan's productivity growth is the highest in the sample with almost half due to efficiency change. Copyright 1994 by American Economic Association.},
	url={http://ideas.repec.org/a/aea/aecrev/v84y1994i1p66-83.html}
}

@TECHREPORT{Fare1996a,
	author={F\"{a}re, Rolf and Shawna Grosskopf and Roos P.},
	title={{Network and production models of Swedish pharmacies}},
	institution ={Mimeo},
	year=1996,
}
@BOOK{Fare1997,
	title = {Efficiency and productivity in rich and poor countries},
	publisher = {Springer Verlag, New York},
	year = {1997},
	editor = {In: jensen, B.S., Wong, K.Y. (Eds.), Dynamics, Economics growth,
	and International Trade},
	author = {F\"{a}re, Rolf,   and Shawna Grosskopf, and Whittaker, Gerald},
	pages = {209-240},
	owner = {uqttrin2},
	timestamp = {2014.08.20}
}


@article{Fare1989,
	jstor_articletype = {research-article},
	title = {Multilateral Productivity Comparisons When Some Outputs are Undesirable: A Nonparametric Approach},
	author = {F\"{a}re, Rolf, and Shawna Grosskopf, and Lovell, C. A. K. and Pasurka, Carl},
	journal = {The Review of Economics and Statistics},
	jstor_issuetitle = {},
	volume = {71},
	number = {1},
	jstor_formatteddate = {Feb., 1989},
	pages = {90-98},
	url = {http://www.jstor.org/stable/1928055},
	ISSN = {00346535},
	abstract = {Multilateral productivity comparisons of firms producing multiple outputs, some of which are undesirable, are obtained by making two modifications to the standard Farrell approach to efficiency measurement. The restriction that production technology satisfy strong disposability of outputs is relaxed to allow for the fact that undesirable outputs may be freely disposable, and the efficiency measures are modified to allow for an asymmetric treatment of desirable and undesirable outputs. Performance measures that satisfy these requirements are calculated as solutions to programming problems. The methodology is applied to a sample of mills producing paper and pollutants.},
	language = {English},
	year = {1989},
	publisher = {The MIT Press},
	copyright = {Copyright © 1989 The MIT Press},
}
@ARTICLE{Fare1995a,
	author = {F\"{a}re, Rolf and Whittaker, Gerald},
	title = {AN INTERMEDIATE INPUT MODEL OF DAIRY PRODUCTION USING COMPLEX SURVEY
	DATA},
	journal = {Journal of Agricultural Economics},
	year = {1995},
	volume = {46},
	pages = {201--213},
	number = {2},
	doi = {10.1111/j.1477-9552.1995.tb00766.x},
	issn = {1477-9552},
	publisher = {Blackwell Publishing Ltd},
	url = {http://dx.doi.org/10.1111/j.1477-9552.1995.tb00766.x}
}

@BOOK{Fare1995,
	title = {Multi-ouput production and duality:theory and applications},
	publisher = {New York: Kluwer Academic Publishers},
	year = {1995},
	author = {F\"{a}re, Rolf, and Primont, Daniel},
	owner = {uqttrin2},
	timestamp = {2014.04.09}
}

@BOOK{Fare1996,
	title = {Intertemporal Production Frontiers: With Dynamic DEA},
	publisher = {New York: Kluwer Academic Publishers},
	year = {1996},
	author = {F\"{a}re, Rolf, and Shawna Grosskopf},
	owner = {uqttrin2},
	timestamp = {2014.04.09}
}
@article{Fan1995,
	jstor_articletype = {research-article},
	title = {Local Polynomial Kernel Regression for Generalized Linear Models and Quasi-Likelihood Functions},
	author = {Fan, Jianqing and Heckman, Nancy E. and Wand, M. P.},
	journal = {Journal of the American Statistical Association},
	jstor_issuetitle = {},
	volume = {90},
	number = {429},
	jstor_formatteddate = {Mar., 1995},
	pages = {141-150},
	url = {http://www.jstor.org/stable/2291137},
	
	year = {1995},
	publisher = {American Statistical Association},
	copyright = {Copyright Â© 1995 American Statistical Association},
}

@article{Fan1992,
	jstor_articletype = {research-article},
	title = {Design-adaptive Nonparametric Regression},
	author = {Fan, Jianqing},
	journal = {Journal of the American Statistical Association},
	jstor_issuetitle = {},
	volume = {87},
	number = {420},
	jstor_formatteddate = {Dec., 1992},
	pages = {pp. 998-1004},
	url = {http://www.jstor.org/stable/2290637},
	ISSN = {01621459},
	abstract = {In this article we study the method of nonparametric regression based on a weighted local linear regression. This method has advantages over other popular kernel methods. Moreover, such a regression procedure has the ability of design adaptation: It adapts to both random and fixed designs, to both highly clustered and nearly uniform designs, and even to both interior and boundary points. It is shown that the local linear regression smoothers have high asymptotic efficiency (i.e., can be 100% with a suitable choice of kernel and bandwidth) among all possible linear smoothers, including those produced by kernel, orthogonal series, and spline methods. The finite sample property of the local linear regression smoother is illustrated via simulation studies. Nonparametric regression is frequently used to explore the association between covariates and responses. There are many versions of kernel regression smoothers. Some estimators are not good for random designs, such as in observational studies, and others are not good for nonequispaced designs. Furthermore, most nonparametric regression smoothers have "boundary effects" and require modifications at boundary points. However, the local linear regression smoothers do not share these disadvantages. They adapt to almost all regression settings and do not require any modifications even at boundary. Besides, this method has higher efficiency than other traditional nonparametric regression methods.},
	language = {English},
	year = {1992},
	publisher = {American Statistical Association},
	copyright = {Copyright Â© 1992 American Statistical Association},
}

@article{Fan1992b,
	jstor_articletype = {research-article},
	title = {Variable Bandwidth and Local Linear Regression Smoothers},
	author = {Fan, Jianqing and Gijbels, Irene},
	journal = {The Annals of Statistics},
	jstor_issuetitle = {},
	volume = {20},
	number = {4},
	jstor_formatteddate = {Dec., 1992},
	pages = {pp. 2008-2036},
	url = {http://www.jstor.org/stable/2242378},
	ISSN = {00905364},
	abstract = {In this paper we introduce an appealing nonparametric method for estimating the mean regression function. The proposed method combines the ideas of local linear smoothers and variable bandwidth. Hence, it also inherits the advantages of both approaches. We give expressions for the conditional MSE and MISE of the estimator. Minimization of the MISE leads to an explicit formula for an optimal choice of the variable bandwidth. Moreover, the merits of considering a variable bandwidth are discussed. In addition, we show that the estimator does not have boundary effects, and hence does not require modifications at the boundary. The performance of a corresponding plug-in estimator is investigated. Simulations illustrate the proposed estimation method.},
	language = {English},
	year = {1992},
	publisher = {Institute of Mathematical Statistics},
	copyright = {Copyright Â© 1992 Institute of Mathematical Statistics},
}

@article{Fare2006,
	jstor_articletype = {research-article},
	title = {Directional Duality Theory},
	author = {F\"{a}re, Rolf and Primont, Daniel},
	journal = {Economic Theory},
	jstor_issuetitle = {},
	volume = {29},
	number = {1},
	jstor_formatteddate = {Sep., 2006},
	pages = { 239-247},
	url = {http://www.jstor.org/stable/25056120},
	ISSN = {09382259},
	abstract = {In this paper we introduce a complete duality theory based on directional distance functions. This duality theory parallels the duality theory based on radial distance functions in FÃ¤re and Primont (1995).},
	language = {English},
	year = {2006},
	publisher = {Springer},
	copyright = {Copyright Â© 2006 Springer},
}

@ARTICLE{Fare1978,
	author = {F\"{a}re, Rolf  and C.A Knox Lovell},
	title = {Measuring the technical efficiency of production },
	journal = {Journal of Economic Theory },
	year = {1978},
	volume = {19},
	pages = {150 - 162},
	number = {1},
	doi = {http://dx.doi.org/10.1016/0022-0531(78)90060-1},
	issn = {0022-0531},
	url = {http://www.sciencedirect.com/science/article/pii/0022053178900601}
}

@article{Farrell1957,
	jstor_articletype = {research-article},
	title = {The Measurement of Productive Efficiency},
	author = {Farrell, M. J.},
	journal = {Journal of the Royal Statistical Society. Series A (General)},
	jstor_issuetitle = {},
	volume = {120},
	number = {3},
	jstor_formatteddate = {1957},
	pages = { 253-290},
	url = {http://www.jstor.org/stable/2343100},
	ISSN = {00359238},
	
	language = {English},
	year = {1957},
	publisher = {Wiley for the Royal Statistical Society},
	copyright = {Copyright Â© 1957 Royal Statistical Society},
}

@article{Fama1965,
	ISSN = {00219398, 15375374},
	URL = {http://www.jstor.org/stable/2350752},
	author = {Eugene F. Fama},
	journal = {The Journal of Business},
	number = {1},
	pages = {34-105},
	publisher = {University of Chicago Press},
	title = {The Behavior of Stock-Market Prices},
	volume = {38},
	year = {1965}
}




@ARTICLE{Fecher1993,
	author = {Fecher, F. and Kessler, D. and Perelman, S. and Pestieau, P.},
	title = {Productive performance of the French insurance industry},
	journal = {Journal of Productivity Analysis},
	year = {1993},
	volume = {4},
	pages = {77-93},
	number = {1-2},
	doi = {10.1007/BF01073467},
	issn = {0895-562X},
	language = {English},
	publisher = {Kluwer Academic Publishers},
	url = {http://dx.doi.org/10.1007/BF01073467}
}


%=============GGG===================================
@techreport{Gali2010,
	title = "Monetary Policy and Unemployment",
	author = "Jordi Gal\'{i}",
	institution = "National Bureau of Economic Research",
	type = "Working Paper",
	series = "Working Paper Series",
	number = "15871",
	year = "2010",
	month = "April",
	doi = {10.3386/w15871},
	URL = "http://www.nber.org/papers/w15871",
	abstract = {Much recent research has focused on the development and analysis of extensions of the New Keynesian framework that model labor market frictions and unemployment explicitly. The present paper describes some of the essential ingredients and properties of those models, and their implications for monetary policy.},
}
@article{Giacomini2008,
	title = "Mixtures of t-distributions for finance and forecasting",
	journal = "Journal of Econometrics",
	volume = "144",
	number = "1",
	pages = "175 - 192",
	year = "2008",
	issn = "0304-4076",
	doi = "https://doi.org/10.1016/j.jeconom.2008.01.004",
	url = "http://www.sciencedirect.com/science/article/pii/S0304407608000055",
	author = "Raffaella Giacomini and Andreas Gottschling and Christian Haefke and Halbert White",
	keywords = "ARMA–GARCH models, Neural networks, Nonparametric density estimation, Forecast accuracy, Option pricing, Risk-neutral density"
}
@ARTICLE{Grubb1983,
	author = {Dennis Grubb and Richard Jackman and Richard Layard},
	title = {{Wage rigidity and unemployment in OECD countries}},
	journal = {European Economic Review},
	year = {1983},
	volume = {21},
	pages = {11 - 39},
	number = {1},
	doi = {http://dx.doi.org/10.1016/S0014-2921(83)80003-8},
	issn = {0014-2921},
	url = {http://www.sciencedirect.com/science/article/pii/S0014292183800038}
}
@MISC{Gu2010,
	author = {Waner Gu and R. Robert Russell},
	title = {Foreign Direct Investment and Convergence: A Nonparametric Production Frontier Approach},
	year = {2010},
	note={Unpublished manuscript, University of California, Riverside}
}
@Article{Griliches1979,
	author={Zvi Griliches},
	title={{Issues in Assessing the Contribution of Research and Development to Productivity Growth}},
	journal={Bell Journal of Economics},
	year=1979,
	volume={10},
	number={1},
	pages={92-116},
	month={Spring},
	keywords={},
	abstract={ This article outlines the production function approach to the estimation of the returns to R\&D and then proceeds to discuss in turn two very difficult problems: the measurement of output in R\&D intensive industries and the definition and measurement of the stock R\&D \&quot;capital.\&quot; The latter concept leads to a discussion of modeling of the spillover effects of R\&D and to suggestions for possible measurement of such effects via the concept of technological distance between firms and industries. Somewhat more familiar econometric problems (multicollinearity and simultaneity) are taken up in the next section and another section is devoted to estimation and inference problems arising more specifically in the R\&D context. Several recent studies of returns to R\&D are then surveyed, and the paper concludes with a plea for a lowering of expectations as to what the available data can tell us and with suggestions for ways of expanding the current data base in this field.},
	url={http://ideas.repec.org/a/rje/bellje/v10y1979ispringp92-116.html}
}

@ARTICLE{Grund1994,
	author = {Grund, Birgit and Hall, Peter and Marron, J. S.},
	title = {Loss and risk in smoothing parameter selection},
	journal = {Journal of Nonparametric Statistics},
	year = {1994},
	volume = {4},
	pages = {133-147},
	number = {2},
	doi = {10.1080/10485259408832605},
	eprint = { http://dx.doi.org/10.1080/10485259408832605 },
	url = { http://dx.doi.org/10.1080/10485259408832605 
	}
}

@article{Gozalo1993,
	jstor_articletype = {research-article},
	title = {A Consistent Model Specification Test for Nonparametric Estimation of Regression Function Models},
	author = {Gozalo, Pedro L.},
	journal = {Econometric Theory},
	jstor_issuetitle = {},
	volume = {9},
	number = {3},
	jstor_formatteddate = {Sep., 1993},
	pages = {451-477},
	url = {http://www.jstor.org/stable/3532231},
	ISSN = {02664666},
	abstract = {This paper proposes a general framework for specification testing of the regression function in a nonparametric smoothing estimation context. The same analysis can be applied to cases as varied as testing for omission of variables, testing certain nonlinear restrictions in the regressors, and testing the correct specification of some parametric or semiparametric model of interest, for example, testing a certain type of nonlinearity of the regression function. Furthermore, the test can be applied to i.i.d. and time-series data, and some or all of the regressors are allowed to be discrete. A Monte Carlo simulation is used to assess the performance of the test in small and medium samples.},
	language = {English},
	year = {1993},
	publisher = {Cambridge University Press},
	copyright = {Copyright © 1993 Cambridge University Press},
}

@Article{Gallant1982,
	author={Gallant, A. Ronald},
	title={{Unbiased determination of production technologies}},
	journal={Journal of Econometrics},
	year=1982,
	volume={20},
	number={2},
	pages={285-323},
	month={November},
	keywords={},
	abstract={No abstract is available for this item.},
	url={http://ideas.repec.org/a/eee/econom/v20y1982i2p285-323.html}
}
@ARTICLE{Glass1998,
	author = {Amy Jocelyn Glass and Kamal Saggi},
	title = {International technology transfer and the technology gap },
	journal = {Journal of Development Economics },
	year = {1998},
	volume = {55},
	pages = {369 - 398},
	number = {2},
	doi = {http://dx.doi.org/10.1016/S0304-3878(98)00041-8},
	issn = {0304-3878},
	keywords = {R&amp;D},
	url = {http://www.sciencedirect.com/science/article/pii/S0304387898000418}
}

@ARTICLE{Griffin2008,
	author = {Griffin, J.E. and Steel, M.F.J.},
	title = {Flexible mixture modelling of stochastic frontiers},
	journal = {Journal of Productivity Analysis},
	year = {2008},
	volume = {29},
	pages = {33-50},
	number = {1},
	doi = {10.1007/s11123-007-0064-4},
	issn = {0895-562X},
	keywords = {Centring; Efficiency; Generalized gamma distribution; Prior elicitation;
	Skewness; C11; C23; D24},
	language = {English},
	publisher = {Springer US},
	url = {http://dx.doi.org/10.1007/s11123-007-0064-4}
}

@ARTICLE{Gallant1984,
	author = {A.Ronald Gallant and Gene H. Golub},
	title = {Imposing curvature restrictions on flexible functional forms },
	journal = {Journal of Econometrics },
	year = {1984},
	volume = {26},
	pages = {295 - 321},
	number = {3},
	abstract = {A general computational method for estimating the parameters of a
	flexible functional form subject to convexity, quasi-convexity, concavity,
	or quasi-concavity at a point, at several points, or over a region,
	is set forth and illustrated with an example. },
	doi = {http://dx.doi.org/10.1016/0304-4076(84)90024-1},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/0304407684900241}
}


%=============HHH=======================================
@article{Hall1983,
	author = "Hall, Peter",
	doi = "10.1214/aos/1176346329",
	fjournal = "The Annals of Statistics",
	journal = "Ann. Statist.",
	month = "12",
	number = "4",
	pages = "1156--1174",
	publisher = "The Institute of Mathematical Statistics",
	title = "Large Sample Optimality of Least Squares Cross-Validation in Density Estimation",
	url = "http://dx.doi.org/10.1214/aos/1176346329",
	volume = "11",
	year = "1983"
}

@article{Hanson1973,
	author = "Hanson, D. L. and Pledger, Gordon and Wright, F. T.",
	doi = "10.1214/aos/1176342407",
	fjournal = "The Annals of Statistics",
	journal = "Ann. Statist.",
	month = "05",
	number = "3",
	pages = "401--421",
	publisher = "The Institute of Mathematical Statistics",
	title = "On Consistency in Monotonic Regression",
	url = "http://dx.doi.org/10.1214/aos/1176342407",
	volume = "1",
	year = "1973"
}

@article{Hultberg1999,
	ISSN = {0769489X, 22726497},
	URL = {http://www.jstor.org/stable/20076207},
	author = {Patrick T. Hultberg and M. Ishaq Nadiri and Robin C. Sickles and Patrik T. Hultberg},
	journal = {Annales d'Économie et de Statistique},
	number = {55/56},
	pages = {449-474},
	publisher = {[GENES, ADRES]},
	title = {An International Comparison of Technology Adoption and Efficiency: A Dynamic Panel Model},
	year = {1999}
}



@BOOK{Hastie1991,
	title = {Generalized Additive Model},
	publisher = {Chapman and Hall, London},
	year = {1991},
	author = {Hastie, Trevor and Tibshirani, Robert}}


@BOOK{Hill2008,
	title = {Principles of Econometrics, third ed.},
	publisher = {John Wiley and Sons, New Jersey},
	year = {2008},
	author = {Hill, R., Griffiths, and W., Lim, G.},
	owner = {uqttrin2},
	timestamp = {2014.07.18}
}
@Article{Hyndman2006,  
	Title                    = {Another look at measures of forecast accuracy },
	Author                   = {Rob J. Hyndman and Anne B. Koehler},
	Journal                  = {International Journal of Forecasting },
	Year                     = {2006},
	Number                   = {4},
	Pages                    = {679 - 688},
	Volume                   = {22},
	
	Doi                      = {https://doi.org/10.1016/j.ijforecast.2006.03.001},
	ISSN                     = {0169-2070},
	Keywords                 = {Forecast accuracy},
	Url                      = {http://www.sciencedirect.com/science/article/pii/S0169207006000239}
}


@article{Hanson1973,
	jstor_articletype = {research-article},
	title = {On Consistency in Monotonic Regression},
	author = {Hanson, D. L. and Gordon Pledger and Wright, F. T.},
	journal = {The Annals of Statistics},
	jstor_issuetitle = {},
	volume = {1},
	number = {3},
	jstor_formatteddate = {May, 1973},
	pages = {pp. 401-421},
	url = {http://www.jstor.org/stable/2958100},
	ISSN = {00905364},
	abstract = {For each t in some subset T of N-dimensional Euclidean space let Ft be a distribution function with mean m(t). Suppose m(t) is non-decreasing in each of the coordinates of t. Let t1, t2,⋯ be a sequence of points in T and let Y1, Y2,⋯ be an independent sequence of random variables such that the distribution function of Yk is Ftk . Estimators m̂n(t; Y1,⋯, Yn) of m(t) which are monotone in each coordinate of t and which minimize ∑n i=1 [m̂n(ti; Y1,⋯, Yn) - Yi]2 are already known. Brunk has investigated their consistency when N = 1. In this paper additional consistency results are obtained when N = 1 and some results are obtained in the case N = 2. In addition, we prove several lemmas about the law of large numbers which we believe to be of independent interest.},
	language = {English},
	year = {1973},
	publisher = {Institute of Mathematical Statistics},
	copyright = {Copyright © 1973 Institute of Mathematical Statistics},
}


@article{Henderson2005,
	jstor_articletype = {research-article},
	title = {Human Capital and Convergence: A Production-Frontier Approach},
	author = {Henderson, Daniel J. and Russell, R. Robert},
	journal = {International Economic Review},
	jstor_issuetitle = {},
	volume = {46},
	number = {4},
	jstor_formatteddate = {Nov., 2005},
	pages = {1167-1205},
	url = {http://www.jstor.org/stable/3663664},
	ISSN = {00206598},
	abstract = {Using nonparametric, production-frontier methods, we decompose labor productivity growth into components attributable to technological change (shifts in the world production frontier), technological catch-up (movements toward or away from the frontier), and physical and human capital accumulation (movements along the frontier). We find that (1) technological change is decidedly nonneutral, (2) productivity growth is driven primarily by physical and human capital accumulation, (3) the increased international dispersion of productivity is explained primarily by physical capital accumulation, and (4) international polarization (the shift from a unimodal to a bimodal distribution) is brought about primarily by efficiency changes (technological catch-up).},
	language = {English},
	year = {2005},
	publisher = {Wiley for the Economics Department of the University of Pennsylvania and Institute of Social and Economic Research -- Osaka University},
	copyright = {Copyright Â© 2005 Economics Department of the University of Pennsylvania},
}

@article{Henderson2007,
	jstor_articletype = {research-article},
	title = {Testing for {(Efficiency)} Catching-Up},
	author = {Henderson, Daniel J. and Zelenyuk, Valentin},
	journal = {Southern Economic Journal},
	jstor_issuetitle = {},
	volume = {73},
	number = {4},
	jstor_formatteddate = {Apr., 2007},
	pages = {1003-1019},
	url = {http://www.jstor.org/stable/20111939},
	ISSN = {00384038},
	abstract = {We used advances in data envelopment analysis (DEA) techniques to examine efficiency scores and investigate the issue of convergence and divergence in a cross-country analysis. Specifically, we used bootstrapping techniques to examine a data set of 52 developed and developing countries. We found that whe using the standard DEA model, some of the results were not robust. Further, we broke the sample into groups to examine club convergence. We found that efficiecy scores were significantly different between groups and that there was some evidence of convergence of efficiency scores within each group.},
	language = {English},
	year = {2007},
	publisher = {Southern Economic Association},
	copyright = {Copyright Â© 2007 Southern Economic Association},
}
@TechReport{Henderson2009,
	author={Henderson, Daniel J. and Parmeter, Christopher F.},
	title={{Imposing Economic Constraints in Nonparametric Regression: Survey, Implementation and Extension}},
	year=2009,
	month=Mar,
	institution={Institute for the Study of Labor (IZA)},
	type={IZA Discussion Papers},
	url={http://ideas.repec.org/p/iza/izadps/dp4103.html},
	number={4103},
	abstract={Economic conditions such as convexity, homogeneity, homotheticity, and monotonicity are all important assumptions or consequences of assumptions of economic functionals to be estimated. Recent research has seen a renewed interest in imposing constraints in nonparametric regression. We survey the available methods in the literature, discuss the challenges that present themselves when empirically implementing these methods and extend an existing method to handle general nonlinear constraints. A heuristic discussion on the empirical implementation for methods that use sequential quadratic programming is provided for the reader and simulated and empirical evidence on the distinction between constrained and unconstrained nonparametric regression surfaces is covered.},
	keywords={identification; concavity; Hessian; constraint weighted bootstrapping; earnings function},
}
@book{Henderson2015, 
	place={Cambridge}, 
	title={Applied Nonparametric Econometrics}, 
	DOI={10.1017/CBO9780511845765}, publisher={Cambridge University Press}, 
	author={Henderson, Daniel J. and Parmeter, Christopher F.}, year={2015}}
@ARTICLE{Huang1994,
	author = {Huang, Clief.J. and Liu, Jin-Tan},
	title = {Estimation of a non-neutral stochastic frontier production function},
	journal = {Journal of Productivity Analysis},
	year = {1994},
	volume = {5},
	pages = {171-180},
	number = {2},
	doi = {10.1007/BF01073853},
	issn = {0895-562X},
	language = {English},
	publisher = {Kluwer Academic Publishers},
	url = {http://dx.doi.org/10.1007/BF01073853}
}
@ARTICLE{Hayashi2002,
	author = {Fumio Hayashi and Edward C Prescott},
	title = {The 1990s in {Japan}: A Lost Decade},
	journal = {Review of Economic Dynamics},
	year = {2002},
	volume = {5},
	pages = {206 - 235},
	number = {1},
	doi = {http://dx.doi.org/10.1006/redy.2001.0149},
	issn = {1094-2025},
	keywords = {growth model},
	url = {http://www.sciencedirect.com/science/article/pii/S1094202501901498}
}


@article{Hall2001,
	jstor_articletype = {research-article},
	title = {Nonparametric Kernel Regression Subject to Monotonicity Constraints},
	author = {Hall, Peter and Huang, Li-Shan},
	journal = {The Annals of Statistics},
	jstor_issuetitle = {},
	volume = {29},
	number = {3},
	jstor_formatteddate = {Jun., 2001},
	pages = {pp. 624-647},
	url = {http://www.jstor.org/stable/2673965},
	ISSN = {00905364},
	abstract = {We suggest a method for monotonizing general kernel-type estimators, for example local linear estimators and Nadaraya-Watson estimators. Attributes of our approach include the fact that it produces smooth estimates, indeed with the same smoothness as the unconstrained estimate. The method is applicable to a particularly wide range of estimator types, it can be trivially modified to render an estimator strictly monotone and it can be employed after the smoothing step has been implemented. Therefore, an experimenter may use his or her favorite kernel estimator, and their favorite bandwidth selector, to construct the basic nonparametric smoother and then use our technique to render it monotone in a smooth way. Implementation involves only an off-the-shelf programming routine. The method is based on maximizing fidelity to the conventional empirical approach, subject to monotonicity. We adjust the unconstrained estimator by tilting the empirical distribution so as to make the least possible change, in the sense of a distance measure, subject to imposing the constraint of monotonicity.},
	language = {English},
	year = {2001},
	publisher = {Institute of Mathematical Statistics},
	copyright = {Copyright © 2001 Institute of Mathematical Statistics},
}

@ARTICLE{Hall1984,
	author = {Peter Hall},
	title = {Central limit theorem for integrated square error of multivariate
	nonparametric density estimators },
	journal = {Journal of Multivariate Analysis },
	year = {1984},
	volume = {14},
	pages = {1-16},
	number = {1},
	abstract = {Martingale theory is used to obtain a central limit theorem for degenerate
	U-statistics with variable kernels, which is applied to derive central
	limit theorems for the integrated square error of multivariate nonparametric
	density estimators. Previous approaches to this problem have employed
	KomlÃ³s-Major-TusnÃ¡dy type approximations to the empiric distribution
	function, and have required the following two restrictive assumptions
	which are not necessary using the present approach: (i) the data
	are in one or two dimensions, and (ii) the estimator is constructed
	suboptimally. },
	doi = {http://dx.doi.org/10.1016/0047-259X(84)90044-7},
	issn = {0047-259X},
	keywords = {central limit theorem},
	url = {http://www.sciencedirect.com/science/article/pii/0047259X84900447}
}


@article{Hardle1993,
	author = {H\"{a}rdle, W. and Mammen, E.},
	doi = "10.1214/aos/1176349403",
	journal = "The Annals of Statistics",
	month = "12",
	number = "4",
	pages = "1926--1947",
	publisher = "The Institute of Mathematical Statistics",
	title = "Comparing Nonparametric Versus Parametric Regression Fits",
	url = "http://dx.doi.org/10.1214/aos/1176349403",
	volume = "21",
	year = "1993"
}
@BOOK{Helpman1998,
	title = {General Purpose Technologies and Economic Growth},
	author={Helpman, Elhanan},
	publisher = {The MIT Press},
	year = {1998},
	address = {Elhanan Helpman (ed). Cambridge, MA},
	organization = {The MIT Press},
	url = {http://mitpress.mit.edu/catalog/item/default.asp?ttype=2\&tid=12234}
}

@ARTICLE{Helpman1999,
	author = {Helpman, Elhanan and Rangel, Antonio},
	title = {Adjusting to a New Technology: Experience and Training},
	journal = {Journal of Economic Growth},
	year = {1999},
	volume = {4},
	pages = {359--383},
	number = {4},
	doi = {10.1023/A:1009888907797},
	issn = {1573-7020},
	url = {http://dx.doi.org/10.1023/A:1009888907797}
}


%=============III======================================
@Article{Ingrid2014,
	Title                    = {Nonparametric least squares methods for stochastic frontier models},
	Author                   = {L\'{e}opold  Simar and Ingrid Van Keilegom and Valentin Zelenyuk},
	Journal                  = {Journal of Productivity Analysis},
	Year                     = {2017},
	
	Month                    = {06},
	Number                   = {3},
	Pages                    = {189-204},
	Volume                   = {47},
}

@ARTICLE{Iyer2008,
	author = {Iyer, Krishna G. and Rambaldi, Alicia N. and Tang, Kam Ki},
	title = {Efficiency externalities of trade and alternative forms of foreign
	investment in OECD countries},
	journal = {Journal of Applied Econometrics},
	year = {2008},
	volume = {23},
	pages = {749--766},
	number = {6},
	doi = {10.1002/jae.1024},
	issn = {1099-1255},
	publisher = {John Wiley \& Sons, Ltd.},
	url = {http://dx.doi.org/10.1002/jae.1024}
}

%=============JJJ====================================
@ARTICLE{Jondrow1982,
	author = {James Jondrow and C.A. Knox Lovell and Ivan S. Materov and Peter
	Schmidt},
	title = {On the estimation of technical inefficiency in the stochastic frontier
	production function model },
	journal = {Journal of Econometrics },
	year = {1982},
	volume = {19},
	pages = {233 - 238},
	
}

@BOOK{Joshua2014a,
	title = {Statistical Modeling and Computation},
	publisher = {Springer, New York},
	year = {2014},
	author = {Dirk P. Kroese, and Chan, Joshua},
	owner = {uqttrin2},
	timestamp = {2014.06.23}
}


@BOOK{Jorgenson1987,
	title = {Productivity and U.S. Economic Growth},
	publisher = {Harvard University Press},
	year = {1987},
	author = {Dale W. Jorgenson and F.M. Gollop and B.M. Fraumeni},
	note = {Reprinted- Universe, 1999},
	organization = {Harvard University Press},
	url = {http://www.iuniverse.com/bookstore/BookDetail.aspx?BookId=SKU-000072032}
}

@ARTICLE{Jorgenson1967,
	author = {Dale Jorgenson and Z. Griliches},
	title = {The Explanation of Productivity Change},
	journal = {The Review of Economic Studies},
	year = {1967},
	volume = {34},
	pages = {249-280},
	note = {Reprinted in A.K. Sen (ed.), Growth Economics , Hammondsworth, Penguin
	Books, 1970, pp. 420-473; reprinted in Survey of Current Business,
	Vol. 52, No. 5, Part II, May 1972, pp. 3-63. Productivity 1, ch.
	3, pp. 51-98. }
}

@ARTICLE{Joshua2016,
	author = {Joshua C. C. Chan},
	title = {Specification tests for time-varying parameter models with stochastic
	volatility},
	journal = {Econometric Reviews},
	year = {2016},
	volume = {0},
	pages = {1-17},
	number = {0},
	abstract = { We propose an easy technique to test for time-variation in
	coefficients and volatilities. Specifically, by using a noncentered
	parameterization for state space models, we develop a method to directly
	calculate the relevant Bayes factor using the Savageâ€“Dickey density
	ratioâ€”thus avoiding the computation of the marginal likelihood
	altogether. The proposed methodology is illustrated via two empirical
	applications. In the first application, we test for time-variation
	in the volatility of inflation in the G7 countries. The second application
	investigates if there is substantial time-variation in the nonaccelerating
	inflation rate of unemployment (NAIRU) in the United States. },
	doi = {10.1080/07474938.2016.1167948},
	eprint = { http://dx.doi.org/10.1080/07474938.2016.1167948 },
	url = { http://dx.doi.org/10.1080/07474938.2016.1167948 
	}
}


%=============KKK==================================
@ARTICLE{Kolmogorov1933,
	author = {Kolmogorov, A. N.},
	title = {{Sulla Determinazione Empirica di una Legge di Distribuzione}},
	journal = {Giornale dell'Istituto Italiano degli Attuari},
	year = {1933},
	volume = {4},
	pages = {83--91},
	citeulike-article-id = {1213059},
	keywords = {bibtex-import},
	posted-at = {2007-04-06 19:41:59},
	priority = {2}
}
@ARTICLE{Jin2012,
	author = {Hui Jin and Dale W. Jorgenson},
	title = {Econometric modeling of technical change },
	journal = {Journal of Econometrics },
	year = {2010},
	volume = {157},
	pages = {205 - 219},
	number = {2},
	doi = {http://dx.doi.org/10.1016/j.jeconom.2009.12.002},
	issn = {0304-4076},
	keywords = {Technical change},
	url = {http://www.sciencedirect.com/science/article/pii/S0304407609003005}
}
@ARTICLE{Kao2009a,
	author = {Chiang Kao},
	title = {Efficiency measurement for parallel production systems },
	journal = {European Journal of Operational Research },
	year = {2009},
	volume = {196},
	pages = {1107-1112},
	number = {3},
	abstract = {In the real world there are systems which are composed of independent
	production units. The conventional data envelopment analysis (DEA)
	model uses the sum of the respective inputs and outputs of all component
	units of a system to calculate its efficiency. This paper develops
	a parallel \{DEA\} model which takes the operation of individual
	components into account in calculating the efficiency of the system.
	A property owned by this parallel model is that the inefficiency
	slack of the system can be decomposed into the inefficiency slacks
	of its component units. This helps the decision maker identify inefficient
	components and make subsequent improvements. Another property is
	that the efficiency calculated from this model is smaller than that
	calculated from the conventional \{DEA\} model. Few systems will
	have perfect efficiency score; consequently, a stronger discrimination
	power is gained. In addition to theoretical derivations, a case of
	the national forests of Taiwan is used as an example to illustrate
	the whole idea. },
	doi = {http://dx.doi.org/10.1016/j.ejor.2008.04.020},
	issn = {0377-2217},
	keywords = {Data envelopment analysis},
	url = {http://www.sciencedirect.com/science/article/pii/S0377221708003810}
}
@ARTICLE{Kao2008,
	author = {Chiang Kao and Shiuh-Nan Hwang},
	title = {Efficiency decomposition in two-stage data envelopment analysis:
	An application to non-life insurance companies in Taiwan },
	journal = {European Journal of Operational Research },
	year = {2008},
	volume = {185},
	pages = {418-429},
	number = {1},
	abstract = {The efficiency of decision processes which can be divided into two
	stages has been measured for the whole process as well as for each
	stage independently by using the conventional data envelopment analysis
	(DEA) methodology in order to identify the causes of inefficiency.
	This paper modifies the conventional \{DEA\} model by taking into
	account the series relationship of the two sub-processes within the
	whole process. Under this framework, the efficiency of the whole
	process can be decomposed into the product of the efficiencies of
	the two sub-processes. In addition to this sound mathematical property,
	the case of Taiwanese non-life insurance companies shows that some
	unusual results which have appeared in the independent model do not
	exist in the relational model. In other words, the relational model
	developed in this paper is more reliable in measuring the efficiencies
	and consequently is capable of identifying the causes of inefficiency
	more accurately. Based on the structure of the model, the idea of
	efficiency decomposition can be extended to systems composed of multiple
	stages connected in series. },
	doi = {http://dx.doi.org/10.1016/j.ejor.2006.11.041},
	issn = {0377-2217},
	keywords = {Data envelopment analysis},
	url = {http://www.sciencedirect.com/science/article/pii/S0377221707000112}
}
@ARTICLE{Kao2014,
	author = {Chiang Kao},
	title = {Network data envelopment analysis: A review },
	journal = {European Journal of Operational Research },
	year = {2014},
	volume = {239},
	pages = {1-16},
	number = {1},
	abstract = {Abstract Network data envelopment analysis (DEA) concerns using the
	\{DEA\} technique to measure the relative efficiency of a system,
	taking into account its internal structure. The results are more
	meaningful and informative than those obtained from the conventional
	black-box approach, where the operations of the component processes
	are ignored. This paper reviews studies on network \{DEA\} by examining
	the models used and the structures of the network system of the problem
	being studied. This review highlights some directions for future
	studies from the methodological point of view, and is inspirational
	for exploring new areas of application from the empirical point of
	view. },
	doi = {http://dx.doi.org/10.1016/j.ejor.2014.02.039},
	issn = {0377-2217},
	keywords = {Data envelopment analysis},
	url = {http://www.sciencedirect.com/science/article/pii/S037722171400174X}
}


@ARTICLE{Kao2009,
	author = {Chiang Kao},
	title = {Efficiency decomposition in network data envelopment analysis: A
	relational model },
	journal = {European Journal of Operational Research },
	year = {2009},
	volume = {192},
	pages = {949 - 962},
	number = {3},
	abstract = {Traditional studies in data envelopment analysis (DEA) view systems
	as a whole when measuring the efficiency, ignoring the operation
	of individual processes within a system. This paper builds a relational
	network \{DEA\} model, taking into account the interrelationship
	of the processes within the system, to measure the efficiency of
	the system and those of the processes at the same time. The system
	efficiency thus measured more properly represents the aggregate performance
	of the component processes. By introducing dummy processes, the original
	network system can be transformed into a series system where each
	stage in the series is of a parallel structure. Based on these series
	and parallel structures, the efficiency of the system is decomposed
	into the product of the efficiencies of the stages in the series
	and the inefficiency slack of each stage into the sum of the inefficiency
	slacks of its component processes connected in parallel. With efficiency
	decomposition, the process which causes the inefficient operation
	of the system can be identified for future improvement. An example
	of the non-life insurance industry in Taiwan illustrates the whole
	idea. },
	doi = {http://dx.doi.org/10.1016/j.ejor.2007.10.008},
	issn = {0377-2217},
	keywords = {Data envelopment analysis},
	url = {http://www.sciencedirect.com/science/article/pii/S0377221707010077}
}
@ARTICLE{Koop2011,
	author = {Gary Koop and Simon M. Potter},
	title = {Time varying \{VARs\} with inequality restrictions },
	journal = {Journal of Economic Dynamics and Control },
	year = {2011},
	volume = {35},
	pages = {1126 - 1138},
	number = {7},
	doi = {http://dx.doi.org/10.1016/j.jedc.2011.02.001},
	issn = {0165-1889},
	keywords = {Bayesian},
	url = {http://www.sciencedirect.com/science/article/pii/S0165188911000248}
}

@article{Kumar2002,
	jstor_articletype = {research-article},
	title = {Technological Change, Technological Catch-up, and Capital Deepening: Relative Contributions to Growth and Convergence},
	author = {Kumar, Subodh and Russell, R. Robert},
	journal = {The American Economic Review},
	jstor_issuetitle = {},
	volume = {92},
	number = {3},
	jstor_formatteddate = {Jun., 2002},
	pages = {527-548},
	url = {http://www.jstor.org/stable/3083353},
	ISSN = {00028282},
	
	year = {2002},
	publisher = {American Economic Association},
	copyright = {Copyright Â© 2002 American Economic Association},
}
@ARTICLE{Kumbhakar2005,
	author = {Subal C. Kumbhakar and Hung-Jen Wang},
	title = {Estimation of growth convergence using a stochastic production frontier
	approach },
	journal = {Economics Letters },
	year = {2005},
	volume = {88},
	pages = {300 - 305},
	number = {3},
	abstract = {In this paper we estimate stochastic world production frontiers econometrically
	taking country heterogeneity into account. From the estimated frontier
	we obtain estimates of technical change, technological catch-up (efficiency
	improvement/convergence), and scale related components of total factor
	productivity (TFP) growth. },
	doi = {http://dx.doi.org/10.1016/j.econlet.2005.01.023},
	issn = {0165-1765},
	keywords = {Convergence},
	url = {http://www.sciencedirect.com/science/article/pii/S0165176505001229}
}

@BOOK{Kumb2002,
	title = {Stochastic Frontier Analysis},
	publisher = {Cambridge University Press, Cambridge UK},
	year = {2002},
	author = {Kumbhakar,Subal C., and C.A.Knox Lovell},
	owner = {uqttrin2},
	timestamp = {2014.06.23}
}
@ARTICLE{Kneip2015,
	author = {Kneip,Alois and Simar,Léopold and Wilson,Paul W.},
	title = {WHEN BIAS KILLS THE VARIANCE: CENTRAL LIMIT THEOREMS FOR DEA AND
	FDH EFFICIENCY SCORES},
	journal = {Econometric Theory},
	year = {2015},
	volume = {31},
	pages = {394--422},
	doi = {10.1017/S0266466614000413},
	issn = {1469-4360},
	issue = {02},
	numpages = {29}
}


@article{Kneip1998,
	jstor_articletype = {research-article},
	title = {A Note on the Convergence of Nonparametric \text{DEA} Estimators for Production Efficiency Scores},
	author = {Kneip, Alois and Park, Byeong U. and Simar, L\'{e}opold},
	journal = {Econometric Theory},
	jstor_issuetitle = {},
	volume = {14},
	number = {6},
	jstor_formatteddate = {Dec., 1998},
	pages = {pp. 783-793},
	url = {http://www.jstor.org/stable/3533091},
	ISSN = {02664666},
	abstract = {Efficiency scores of production units are measured by their distance to an estimated production frontier. Nonparametric data envelopment analysis estimators are based on a finite sample of observed production units, and radial distances are considered. We investigate the consistency and the speed of convergence of these estimated efficiency scores (or of the radial distances) in the very general setup of a multi-output and multi-input case. It is shown that the speed of convergence relies on the smoothness of the unknown frontier and on the number of inputs and outputs. Furthermore, one has to distinguish between the output- and the input-oriented cases.},
	language = {English},
	year = {1998},
	publisher = {Cambridge University Press},
	copyright = {Copyright Â© 1998 Cambridge University Press},
}


@article{Kneip2008,
	jstor_articletype = {research-article},
	title = {Asymptotics and Consistent Bootstraps for DEA Estimators in Nonparametric Frontier Models},
	author = {Kneip, Alois and Simar, L\'{e}opold and Wilson, Paul W.},
	journal = {Econometric Theory},
	jstor_issuetitle = {},
	volume = {24},
	number = {6},
	jstor_formatteddate = {Dec., 2008},
	pages = {1663-1697},
	url = {http://www.jstor.org/stable/20142561},
	ISSN = {02664666},
	abstract = {Nonparametric data envelopment analysis (DEA) estimators based on linear programming methods have been widely applied in analyses of productive efficiency. The distributions of these estimators remain unknown except in the simple case of one input and one output, and previous bootstrap methods proposed for inference have not been proved consistent, making inference doubtful. This paper derives the asymptotic distribution of DEA estimators under variable returns to scale. This result is used to prove consistency of two different bootstrap procedures (one based on subsampling, the other based on smoothing). The smooth bootstrap requires smoothing the irregularly bounded density of inputs and outputs and smoothing the DEA frontier estimate. Both bootstrap procedures allow for dependence of the inefficiency process on output levels and the mix of inputs in the case of input-oriented measures, or on input levels and the mix of outputs in the case of output-oriented measures.},
	language = {English},
	year = {2008},
	publisher = {Cambridge University Press},
	copyright = {Copyright Â© 2008 Cambridge University Press},
}


@ARTICLE{Kneip2011,
	author = {Kneip, Alois and Simar,  L\'{e}opold and Wilson, PaulW.},
	title = {A Computationally Efficient, Consistent Bootstrap for Inference with
	Non-parametric DEA Estimators},
	journal = {Computational Economics},
	year = {2011},
	volume = {38},
	pages = {483-515},
	number = {4},
	doi = {10.1007/s10614-010-9217-z},
	issn = {0927-7099},
	keywords = {Bootstrap; Inference; Nonparametric; Data-envelopment-analysis; Efficiency;
	Productivity; C12; C14; C15},
	language = {English},
	publisher = {Springer US},
	url = {http://dx.doi.org/10.1007/s10614-010-9217-z}
}
@ARTICLE{Kumb1990,
	author = {Subal C. Kumbhakar},
	title = {Production frontiers, panel data, and time-varying technical inefficiency
	},
	journal = {Journal of Econometrics },
	year = {1990},
	volume = {46},
	pages = {201 - 211},
	number = {1â€“2},
	abstract = {This paper uses a panel-data framework and models firm-specific technical
	inefficiency which is allowed to vary over time. The specification
	is flexible enough to accommodate increasing, decreasing, and time-invariant
	behavior of technical inefficiency. Time-varying firm- and input-specific
	allocative inefficiency is also incorporated. The estimation method
	suggested uses a parametric production function and cost-minimization
	hypothesis. },
	doi = {http://dx.doi.org/10.1016/0304-4076(90)90055-X},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/030440769090055X}
}

@BOOK{Kumbhakar2000,
	title = {Stochastic Frontier Analysis},
	publisher = {Cambridge University Press},
	year = {2000},
	author = {Subal C. Kumbhakar, and C.A.Knox Lovell},
	owner = {uqttrin2},
	timestamp = {2014.12.13}}


@article{Kumb1991,
	jstor_articletype = {research-article},
	title = {A Generalized Production Frontier Approach for Estimating Determinants of Inefficiency in {U.S.} Dairy Farms},
	author = {Kumbhakar, Subal C. and Ghosh, Soumendra and McGuckin, J. Thomas},
	journal = {Journal of Business & Economic Statistics},
	jstor_issuetitle = {},
	volume = {9},
	number = {3},
	jstor_formatteddate = {Jul., 1991},
	pages = {279-286},
	url = {http://www.jstor.org/stable/1391292},
	ISSN = {07350015},
	abstract = {This article investigates farm-level efficiency of U.S. dairy farmers by estimating their technical and allocative efficiency. Technical inefficiency is assumed to be composed of a deterministic component that is a function of some farm-specific characteristics and a random component. By doing this we extend the stochastic frontier methodology in which determinants of technical inefficiency are explicitly introduced in the model. Given the inputs, variations in efficiency of farms are then explained by both deterministic and random components of technical inefficiency. The empirical results indicate that (a) levels of education of the farmer are important factors determining technical inefficiency and (b) large farms are more efficient (technically) than small and medium-sized farms. Both technical and allocative inefficiency are found to decrease with increase in the level of education of the farmer.},
	language = {English},
	year = {1991},
	publisher = {Taylor & Francis, Ltd. on behalf of American Statistical Association},
	copyright = {Copyright © 1991 American Statistical Association},
}


@ARTICLE{Koop1999,
	author = {Koop, Gary and Osiewalski, Jacek and Steel, Mark F. J.},
	title = {The Components of Output Growth: A Stochastic Frontier Analysis},
	journal = {Oxford Bulletin of Economics and Statistics},
	year = {1999},
	volume = {61},
	pages = {455--487},
	number = {4},
	doi = {10.1111/1468-0084.00139},
	issn = {1468-0084},
	publisher = {Blackwell Publishers Ltd},
	url = {http://dx.doi.org/10.1111/1468-0084.00139}
}
@ARTICLE{Kumbhakar2007,
	author = {Kumbhakar,Subal C. , and Byeong U. Park and Simar, L\'{e}opold,  and Efthymios
	G. Tsionas},
	title = {Nonparametric stochastic frontiers: A local maximum likelihood approach
	},
	journal = {Journal of Econometrics },
	year = {2007},
	volume = {137},
	pages = {1-27},
	number = {1},
	doi = {http://dx.doi.org/10.1016/j.jeconom.2006.03.006},
	issn = {0304-4076},
	keywords = {Stochastic cost frontier},
	url = {http://www.sciencedirect.com/science/article/pii/S0304407606000376}
}
@BOOK{Koopmans1951,
	title = {Activity analysis of production and allocation},
	publisher = {John Wiley & Sons, New York},
	year = {1951},
	author = {Koopmans, T.},
	owner = {uqttrin2},
	timestamp = {2014.07.18}
}
@article{Konus1926,
	
	title ={K probleme pokupatelnoi cili deneg},
	author ={Kon\"{u}s, A.A. and S.S. Byushgens },
	journal = {Voprosi Konyunktur},
	jstor_issuetitle = {},
	volume = {2},
	number = {},
	pages = {151-172},
	year = {1926},
	publisher = {American Economic Association},
	copyright = {Copyright Â© 2002 American Economic Association},
}
@article{Kopp1981,
	jstor_articletype = {research-article},
	title = {{The Measurement of Productive Efficiency: A Reconsideration}},
	author = {Kopp, Raymond J.},
	journal = {The Quarterly Journal of Economics},
	jstor_issuetitle = {},
	volume = {96},
	number = {3},
	jstor_formatteddate = {Aug., 1981},
	pages = {477-503},
	url = {http://www.jstor.org/stable/1882683},
	ISSN = {00335533},
	abstract = {The purpose of this paper is to generalize the Farrell indexes of productive efficiency to nonhomothetic production technologies, and at the same time maintain the cost interpretation of the Farrell measures. Since the generalized indexes rely heavily on recent developments in the estimation of frontier cost and production functions, several frontier models are reviewed. In addition to generalized indexes of technical, allocative, and overall productive efficiency, a variety of single-factor efficiency measures are discussed. The applicability of the proposed efficiency measures is illustrated with a numerical example of electric power generation.},
	language = {English},
	year = {1981},
	publisher = {Oxford University Press},
	copyright = {Copyright Â© 1981 Oxford University Press},
}
@ARTICLE{Kneller2006,
	author = {Kneller, Richard and Stevens, Philip Andrew},
	title = {Frontier Technology and Absorptive Capacity: Evidence from OECD Manufacturing
	Industries},
	journal = {Oxford Bulletin of Economics and Statistics},
	year = {2006},
	volume = {68},
	pages = {1--21},
	number = {1},
	doi = {10.1111/j.1468-0084.2006.00150.x},
	issn = {1468-0084},
	keywords = {O3, O4},
	publisher = {Blackwell Publishing Ltd},
	url = {http://dx.doi.org/10.1111/j.1468-0084.2006.00150.x}
}
@ARTICLE{Kumb1991,
	author = {Kumbhakar, Subal C. and Ghosh, Soumendra and McGuckin, J. Thomas},
	title = {A Generalized Production Frontier Approach for Estimating Determinants
	of Inefficiency in U.S. Dairy Farms},
	journal = {Journal of Business \& Economic Statistics},
	year = {1991},
	volume = {9},
	pages = {279-286},
	number = {3},
	abstract = {This article investigates farm-level efficiency of U.S. dairy farmers
	by estimating their technical and allocative efficiency. Technical
	inefficiency is assumed to be composed of a deterministic component
	that is a function of some farm-specific characteristics and a random
	component. By doing this we extend the stochastic frontier methodology
	in which determinants of technical inefficiency are explicitly introduced
	in the model. Given the inputs, variations in efficiency of farms
	are then explained by both deterministic and random components of
	technical inefficiency. The empirical results indicate that (a) levels
	of education of the farmer are important factors determining technical
	inefficiency and (b) large farms are more efficient (technically)
	than small and medium-sized farms. Both technical and allocative
	inefficiency are found to decrease with increase in the level of
	education of the farmer.},
	copyright = {Copyright Â© 1991 American Statistical Association},
	issn = {07350015},
	jstor_articletype = {research-article},  
	jstor_formatteddate = {Jul., 1991},
	language = {English},
	publisher = {Taylor \& Francis, Ltd. on behalf of American Statistical Association},
	url = {http://www.jstor.org/stable/1391292}
}
@ARTICLE{Kumbhakar1990,
	author = {Subal C. Kumbhakar},
	title = {Production frontiers, panel data, and time-varying technical inefficiency
	},
	journal = {Journal of Econometrics },
	year = {1990},
	volume = {46},
	pages = {201 - 211},
	number = {1Ã¢â‚¬â€œ2},
	abstract = {This paper uses a panel-data framework and models firm-specific technical
	inefficiency which is allowed to vary over time. The specification
	is flexible enough to accommodate increasing, decreasing, and time-invariant
	behavior of technical inefficiency. Time-varying firm- and input-specific
	allocative inefficiency is also incorporated. The estimation method
	suggested uses a parametric production function and cost-minimization
	hypothesis. },
	doi = {http://dx.doi.org/10.1016/0304-4076(90)90055-X},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/030440769090055X}
}
@article{Kastner2014,
	title = "Ancillarity-sufficiency interweaving strategy (ASIS) for boosting MCMC estimation of stochastic volatility models",
	journal = "Computational Statistics & Data Analysis",
	volume = "76",
	number = "Supplement C",
	pages = "408 - 423",
	year = "2014",
	note = "CFEnetwork: The Annals of Computational and Financial Econometrics",
	issn = "0167-9473",
	doi = "https://doi.org/10.1016/j.csda.2013.01.002",
	url = "http://www.sciencedirect.com/science/article/pii/S0167947313000030",
	author = "Gregor Kastner and Sylvia Frühwirth-Schnatter",
	keywords = "Markov chain Monte Carlo",
	keywords = "Non-centering",
	keywords = "Auxiliary mixture sampling",
	keywords = "Massively parallel computing",
	keywords = "State space model",
	keywords = "Exchange rate data"
}
@article{Kim1998,
	ISSN = {00346527, 1467937X},
	URL = {http://www.jstor.org/stable/2566931},
	abstract = {In this paper, Markov chain Monte Carlo sampling methods are exploited to provide a unified, practical likelihood-based framework for the analysis of stochastic volatility models. A highly effective method is developed that samples all the unobserved volatilities at once using an approximating offset mixture model, followed by an importance reweighting procedure. This approach is compared with several alternative methods using real data. The paper also develops simulation-based methods for filtering, likelihood evaluation and model failure diagnostics. The issue of model choice using non-nested likelihood ratios and Bayes factors is also investigated. These methods are used to compare the fit of stochastic volatility and GARCH models. All the procedures are illustrated in detail.},
	author = {Sangjoon Kim and Neil Shephard and Siddhartha Chib},
	journal = {The Review of Economic Studies},
	number = {3},
	pages = {361-393},
	publisher = {[Oxford University Press, Review of Economic Studies, Ltd.]},
	title = {Stochastic Volatility: Likelihood Inference and Comparison with ARCH Models},
	volume = {65},
	year = {1998}
}
@article{Kass1995,
	ISSN = {01621459},
	URL = {http://www.jstor.org/stable/2291091},
	abstract = {In a 1935 paper and in his book Theory of probability, Jeffresy developed a methodology for quantifying the evidence in favor of a scientific theory. The centerpies was a number, now called the Bayes factor, which is the posterior odds of the null hypothesis when the prior probability on the null is one-half. Although there has been much discussion of Bayesian hypothesis testing in the context of criticism of P-values, less attention has been given to the Bayes as a practical tool of applied statistics. In this article we review and discuss the uses of Bayes factors in the context of five scientific applications in genetics, sports, ecology, sociology, and psychology. We emphasize the following points: From Jeffrey's Bayesian viewpoint, the purpose of hypothesis testing is to evaluate the evidence in favor of a scientific theory. Bayes factors offer a way of evaluating evidence in favor of a null hypothesis. Bayes factors provide a way of incorporating external information into the evaluation of evidence about a hypothesis. Bayes factors are very general and do not require alternative models to be nested. Several techniques are available for computing Bayes factors, including asymptotic approximations that are easy to compute using the output from standard packages that maximize likelihoods. In "non-Bayesian significance tests. The Schwarz criterion (or BIC) gives a rough approximation to the logarithm of the Bayes factor, which is easy to use and does not require evaluation of prior distributions. When one is interested in estimation or prediction, Bayes factors may be converted to weights to be attached to various models so that a composite estimate or prediction may be obtained that takes account of structural or model uncertainty. Algorithms have been proposed that allow model uncertainty to be taken into account when the class of models initially considered is very large. Bayes factors are useful for guiding an evolutionary model-building process. It is important, and feasible, to assess the sensitivity of conclusions to the prior distributions used.},
	author = {Robert E. Kass and Adrian E. Raftery},
	journal = {Journal of the American Statistical Association},
	number = {430},
	pages = {773-795},
	publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
	title = {Bayes Factors},
	volume = {90},
	year = {1995}
}

@ARTICLE{Lichtenberg1998,
	author = {Frank R Lichtenberg and Bruno van Pottelsberghe de la Potterie},
	title = {{International R\&D spillovers: A comment}},
	journal = {European Economic Review },
	year = {1998},
	volume = {42},
	pages = {1483 - 1491},
	number = {8},
	doi = {http://dx.doi.org/10.1016/S0014-2921(97)00089-5},
	issn = {0014-2921},
	keywords = {International technology transfer},
	url = {http://www.sciencedirect.com/science/article/pii/S0014292197000895}
}
@Article{Van2001,
	author={van Pottelsberghe de la Potterie, Bruno and Frank Lichtenberg},
	title={{Does Foreign Direct Investment Transfer Technology Across Borders?}},
	journal={The Review of Economics and Statistics},
	year=2001,
	volume={83},
	number={3},
	pages={490-497},
	month={August},
	keywords={},
	abstract={ Previous studies have found that importing goods from R\&D-intensive countries raises a country's productivity. In this paper, we investigate econometrically whether foreign direct investment (FDI) also transfers technology across borders. The data indicates that FDI transfers technology, but only in one direction: a country's productivity is increased if it invests in R\&D-intensive foreign countries-particularly in recent years-but not if foreign R\&D-intensive countries invest in it. Other findings of the paper are that the ratio of foreign-R\&D benefits conveyed by outward FDI to foreign R\&D benefits conveyed by imports is higher for large countries than it is for small ones, that failure to account for international R\&D spillovers leads to upwardly biased estimates of the output elasticity of the domestic R\&D capital stock, and that there are much larger transfers of technology from the United States to Japan than there are from Japan to the United States. © 2001 by the President and Fellows of Harvard College and the Massachusetts Institute of Technology},
	url={http://ideas.repec.org/a/tpr/restat/v83y2001i3p490-497.html}
}

@article{Leibenstein1966,
	ISSN = {00028282},
	URL = {http://www.jstor.org/stable/1823775},
	author = {Harvey Leibenstein},
	journal = {The American Economic Review},
	number = {3},
	pages = {392-415},
	publisher = {American Economic Association},
	title = {Allocative Efficiency vs. "X-Efficiency"},
	volume = {56},
	year = {1966}
}



@ARTICLE{Li2003,
	author = {Qi Li and Jeff Racine},
	title = {Nonparametric estimation of distributions with categorical and continuous
	data },
	journal = {Journal of Multivariate Analysis },
	year = {2003},
	volume = {86},
	pages = {266 - 292},
	number = {2},
	doi = {http://dx.doi.org/10.1016/S0047-259X(02)00025-8},
	issn = {0047-259X},
	keywords = {Discrete and continuous variables},
	url = {http://www.sciencedirect.com/science/article/pii/S0047259X02000258}
}


@ARTICLE{Li2004,
	author = {Jeff Racine and Qi Li},
	title = {Nonparametric estimation of regression functions with both categorical
	and continuous data },
	journal = {Journal of Econometrics },
	year = {2004},
	volume = {119},
	pages = {99 - 130},
	number = {1},
	doi = {http://dx.doi.org/10.1016/S0304-4076(03)00157-X},
	issn = {0304-4076},
	keywords = {Discrete variables},
	url = {http://www.sciencedirect.com/science/article/pii/S030440760300157X}
}
@ARTICLE{Li1999,
	author = {Li, Qi},
	title = {Nonparametric testing the similarity of two unknown density functions:
	local power and bootstrap analysis},
	journal = {Journal of Nonparametric Statistics},
	year = {1999},
	volume = {11},
	pages = {189-213},
	number = {1-3},
	doi = {10.1080/10485259908832780},
	eprint = { http://dx.doi.org/10.1080/10485259908832780 },
	url = { http://dx.doi.org/10.1080/10485259908832780 
	}
}

@ARTICLE{Li1996,
	author = {Li, Qi},
	title = {Nonparametric testing of closeness between two unknown distribution
	functions},
	journal = {Econometric Reviews},
	year = {1996},
	volume = {15},
	pages = {261-274},
	number = {3},
	doi = {10.1080/07474939608800355},
	eprint = {http://www.tandfonline.com/doi/pdf/10.1080/07474939608800355},
	url = {http://www.tandfonline.com/doi/abs/10.1080/07474939608800355}
}
@article{Linton1995,
	jstor_articletype = {research-article},
	title = {A Kernel Method of Estimating Structured Nonparametric Regression Based on Marginal Integration},
	author = {Linton, Oliver and Nielsen, Jens Perch},
	journal = {Biometrika},
	jstor_issuetitle = {},
	volume = {82},
	number = {1},
	jstor_formatteddate = {Mar., 1995},
	pages = {93-100},
	url = {http://www.jstor.org/stable/2337630},
	ISSN = {00063444},
	abstract = {We define a simple kernel procedure based on marginal integration that estimates the relevant univariate quantity in both additive and multiplicative nonparametric regression.},
	language = {English},
	year = {1995},
	publisher = {Biometrika Trust},
	copyright = {Copyright © 1995 Biometrika Trust},
}
@BOOK{Li2007,
	title = {{Nonparametric econometrics: theory and practice}},
	publisher = {Princeton University Press, Princeton},
	year = {2007},
	author = {Li, Q, and Racine, J.S},
	owner = {uqttrin2},
	timestamp = {2014.04.09}}
{Lo1988,
	title = "Stock Market Prices Do Not Follow Random Walks:  Evidence From a Simple Specification Test",
	author = "Andrew W. Lo and A. Craig MacKinlay",
	institution = "National Bureau of Economic Research",
	type = "Working Paper",
	journal="Review of Financial Studies",
	number="1"
	pages = "21-44",
	year = "1988",
	
	doi = {10.3386/w2168},
	URL = "http://www.nber.org/papers/w2168",
	abstract = {In this paper, we test the random walk hypothesis for weekly stock market returns by comparing variance estimators derived from data sampled at different frequencies. The random walk model is strongly rejected for the entire sample period (1962-1985) and for all sub-periods for a variety of aggregate returns indexes and size-sorted portfolios. Although the rejections are largely due to the behavior of small stocks, they cannot be ascribed to either the effects of infrequent trading or time-varying volatilities. Moreover, the rejection of the random walk cannot be interpreted as supporting a mean-reverting stationary model of asset prices, but is more consistent with a specific nonstationary alternative hypothesis.},
}

@article{Lean2007,
	author = {Lean, Hooi Hooi and Smyth, Russell},
	title = {Do Asian Stock Markets Follow a Random Walk? Evidence from LM Unit Root Tests with One and Two Structural Breaks},
	journal = {Review of Pacific Basin Financial Markets and Policies},
	volume = {10},
	number = {01},
	pages = {15-31},
	year = {2007},
	doi = {10.1142/S0219091507000933},
	
	URL = {http://www.worldscientific.com/doi/abs/10.1142/S0219091507000933},
	eprint = {http://www.worldscientific.com/doi/pdf/10.1142/S0219091507000933}
}
%=============MMM================================
@Article{Meeusen1977,
	author={Meeusen, Wim and van den Broeck, Julien},
	title={{Efficiency Estimation from Cobb-Douglas Production Functions with Composed Error}},
	journal={International Economic Review},
	year=1977,
	volume={18},
	number={2},
	pages={435-44},
	month={June},
	keywords={},
	abstract={No abstract is available for this item.},
	url={http://ideas.repec.org/a/ier/iecrev/v18y1977i2p435-44.html}
}
@article{Maddison1987,
	jstor_articletype = {research-article},
	title = {Growth and Slowdown in Advanced Capitalist Economies: Techniques of Quantitative Assessment},
	author = {Maddison, Angus},
	journal = {Journal of Economic Literature},
	jstor_issuetitle = {},
	volume = {25},
	number = {2},
	jstor_formatteddate = {Jun., 1987},
	pages = {649-698},
	url = {http://www.jstor.org/stable/2726106},
	ISSN = {00220515},
	abstract = {},
	language = {English},
	year = {1987},
	publisher = {American Economic Association},
	copyright = {Copyright © 1987 American Economic Association},
}
@article{Marschak1944,
	jstor_articletype = {research-article},
	title = {Random Simultaneous Equations and the Theory of Production},
	author = {Marschak, Jacob and Andrews, William H., Jr.},
	journal = {Econometrica},
	jstor_issuetitle = {},
	volume = {12},
	number = {3/4},
	jstor_formatteddate = {Jul. - Oct., 1944},
	pages = {143-205},
	
	language = {English},
	year = {1944},
	publisher = {The Econometric Society},
	copyright = {Copyright Â© 1944 The Econometric Society},
}

@Article{McFadden1978,
	author={McFadden, Daniel},
	title={{Decentralization, Bureaucracy, and Government: Discussion}},
	journal={American Economic Review},
	year=1978,
	volume={68},
	number={2},
	pages={261-62},
	month={May},
	keywords={},
	abstract={No abstract is available for this item.},
	url={http://ideas.repec.org/a/aea/aecrev/v68y1978i2p261-62.html}
}
@ARTICLE{Mastromarco2009,
	author = {Camilla Mastromarco and Sucharita Ghosh},
	title = {Foreign Capital, Human Capital, and Efficiency: A Stochastic Frontier
	Analysis for Developing Countries },
	journal = {World Development },
	year = {2009},
	volume = {37},
	pages = {489 - 502},
	number = {2},
	doi = {http://dx.doi.org/10.1016/j.worlddev.2008.05.009},
	issn = {0305-750X},
	keywords = {technology diffusion},
	url = {http://www.sciencedirect.com/science/article/pii/S0305750X08001952}
}

@article{Mukerjee1988,
	jstor_articletype = {research-article},
	title = {Monotone Nonparametric Regression},
	author = {Mukerjee, Hari},
	journal = {The Annals of Statistics},
	jstor_issuetitle = {},
	volume = {16},
	number = {2},
	jstor_formatteddate = {Jun., 1988},
	pages = {pp. 741-750},
	url = {http://www.jstor.org/stable/2241753},
	ISSN = {00905364},
	abstract = {In monotone regression procedures one utilizes only the monotonicity of the regression function. In nonparametric regression one utilizes only the assumed smoothness. The analytic and asymptotic properties of the estimator are superior in the latter case; however, monotonicity is not guaranteed. We study a hybrid procedure that produces monotone estimators with properties similar to those of nonparametric regression estimators.},
	language = {English},
	year = {1988},
	publisher = {Institute of Mathematical Statistics},
	copyright = {Copyright © 1988 Institute of Mathematical Statistics},
}


@article{Mammen1991,
	jstor_articletype = {research-article},
	title = {Estimating a Smooth Monotone Regression Function},
	author = {Mammen, Enno},
	journal = {The Annals of Statistics},
	jstor_issuetitle = {},
	volume = {19},
	number = {2},
	jstor_formatteddate = {Jun., 1991},
	pages = {pp. 724-740},
	url = {http://www.jstor.org/stable/2242080},
	ISSN = {00905364},
	abstract = {The problem of estimating a smooth monotone regression function m will be studied. We will consider the estimator mSI consisting of a smoothing step (application of a kernel estimator based on a kernel K) and of a isotonisation step (application of the pool adjacent violator algorithm). The estimator mSI will be compared with the estimator mIS where these two steps are interchanged. A higher order stochastic expansion of these estimators will be given which show that mSI and mSI are asymptotically first order equivalent and that mIS has a smaller mean squared error than mSI if and only if the kernel function of the kernel estimator is not too smooth.},
	language = {English},
	year = {1991},
	publisher = {Institute of Mathematical Statistics},
	copyright = {Copyright © 1991 Institute of Mathematical Statistics},
}


@article{Matzkin1991,
	jstor_articletype = {research-article},
	title = {Semiparametric Estimation of Monotone and Concave Utility Functions for Polychotomous Choice Models},
	author = {Matzkin, Rosa L.},
	journal = {Econometrica},
	jstor_issuetitle = {},
	volume = {59},
	number = {5},
	jstor_formatteddate = {Sep., 1991},
	pages = {pp. 1315-1327},
	url = {http://www.jstor.org/stable/2938369},
	ISSN = {00129682},
	abstract = {This paper introduces a semiparametric estimation method for polychotomous choice models. The method does not require a parametric structure for the systematic subutility of observable exogenous variables. The distribution of the random terms is assumed to be known up to a finite-dimensional parameter vector. In contrast, previous semiparametric methods of estimating discrete choice models have concentrated on relaxing parametric assumptions on the distribution of the random terms while leaving the systematic subutility parametrically specified. The systematic subutility is assumed to possess properties, such as monotonicity and concavity, that are typically assumed in microeconomic theory. The estimator for the systematic subutility and the parameter vector of the distribution is shown to be strongly consistent. A computational technique to calculate the estimators is developed.},
	language = {English},
	year = {1991},
	publisher = {The Econometric Society},
	copyright = {Copyright © 1991 The Econometric Society},
}
@article{Matzkin1992,
	jstor_articletype = {research-article},
	title = {Nonparametric and Distribution-Free Estimation of the Binary Threshold Crossing and The Binary Choice Models},
	author = {Matzkin, Rosa L.},
	journal = {Econometrica},
	jstor_issuetitle = {},
	volume = {60},
	number = {2},
	jstor_formatteddate = {Mar., 1992},
	pages = {pp. 239-270},
	url = {http://www.jstor.org/stable/2951596},
	ISSN = {00129682},
	abstract = {In this paper, it is shown that it is possible to identify binary threshold crossing models and binary choice models without imposing any parametric structure either on the systematic function of observable exogenous variables or on the distribution of the random term. This identification result is employed to develop a fully nonparametric maximum likelihood estimator for both the function of observable exogenous variables and the distribution of the random term. The estimator is shown to be strongly consistent, and a two step procedure for its calculation is developed. The paper also includes examples of economic models that satisfy the conditions that are necessary to apply the results.},
	language = {English},
	year = {1992},
	publisher = {The Econometric Society},
	copyright = {Copyright © 1992 The Econometric Society},
}

@article{Mammen1999,
	jstor_articletype = {research-article},
	title = {The Existence and Asymptotic Properties of a Backfitting Projection Algorithm under Weak Conditions},
	author = {Mammen, E. and Linton, O. and Nielsen, J.},
	journal = {The Annals of Statistics},
	jstor_issuetitle = {},
	volume = {27},
	number = {5},
	jstor_formatteddate = {Oct., 1999},
	pages = {1443-1490},
	url = {http://www.jstor.org/stable/2674078},
	ISSN = {00905364},
	abstract = {We derive the asymptotic distribution of a new backfitting procedure for estimating the closest additive approximation to a nonparametric regression function. The procedure employs a recent projection interpretation of popular kernel estimators provided by Mammen, Marron, Turlach and Wand and the asymptotic theory of our estimators is derived using the theory of additive projections reviewed in Bickel, Klaassen, Ritov and Wellner. Our procedure achieves the same bias and variance as the oracle estimator based on knowing the other components, and in this sense improves on the method analyzed in Opsomer and Ruppert. We provide "high level" conditions independent of the sampling scheme. We then verify that these conditions are satisfied in a regression and a time series autoregression under weak conditions.},
	language = {English},
	year = {1999},
	publisher = {Institute of Mathematical Statistics},
	copyright = {Copyright © 1999 Institute of Mathematical Statistics},
}
@Article{Marron1988,
	Title= {Automatic smoothing parameter selection: A survey},
	Author                   = {Marron, J. S.},
	Journal                  = {Empirical Economics},
	Year                     = {1988},
	Number                   = {3},
	Pages                    = {187--208},
	Volume                   = {13},
	
	Abstract                 = {This is a survey of recent developments in smoothing parameter selection for curve estimation. The first goal of this paper is to provide an introduction to the methods available, with discussion at both a practical and also a nontechnical level, including comparison of methods. The second goal is to provide access to the literature, especially on smoothing parameter selection, but also on curve estimation in general. The two main settings considered here are nonparametric regression and probability density estimation, although the points made apply to other settings as well. These points also apply to many different estimators, although the focus is on kernel estimators, because they are the most easily understood and motivated, and have been at the heart of the development in the field.},
	Doi                      = {10.1007/BF01972448},
	ISSN                     = {1435-8921},
	Url                      = {http://dx.doi.org/10.1007/BF01972448}
}

@ARTICLE{Matzkin1993,
	author = {Rosa L. Matzkin},
	title = {Nonparametric identification and estimation of polychotomous choice
	models },
	journal = {Journal of Econometrics },
	year = {1993},
	volume = {58},
	pages = {137 - 168},
	number = {1â€“2},
	abstract = {In this paper we provide conditions guaranteeing the identification
	of nonparametric polychotomous choice models. In these models, neither
	the subutility function of observable attributes nor the distribution
	of the unobservable random terms is specified parametrically. Sets
	of nonparametric functions that possess properties that are often
	implied by economic theory and satisfy the restrictions required
	to identify the models are described. We use the identification results
	to develop nonparametric strongly-consistent estimators for the subutility
	function of observable attributes. The results concern models in
	which the distribution of the unobservable random terms both depend
	and do not depend on the observable characteristics. },
	doi = {http://dx.doi.org/10.1016/0304-4076(93)90116-M},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/030440769390116M}
}
@INCOLLECTION{Matzkin1994,
	author = {Rosa L. Matzkin},
	title = {Chapter 42 Restrictions of economic theory in nonparametric methods
	},
	publisher = {Elsevier},
	year = {1994},
	editor = {Robert F. Engle and Daniel L. McFadden},
	volume = {4},
	series = {Handbook of Econometrics },
	pages = {2523 - 2558},
	abstract = {This chapter describes several nonparametric estimation and testing
	methods for econometric models. Instead of using parametric assumptions
	on the functions and distributions in an economic model, the methods
	use the restrictions that can be derived from the model. Examples
	of such restrictions are the concavity and monotonicity of functions,
	equality conditions, and exclusion restrictions. The chapter shows,
	first, how economic restrictions can guarantee the identification
	of nonparametric functions in several structural models. It then
	describes how shape restrictions can be used to estimate nonparametric
	functions using popular methods for nonparametric estimation. Finally,
	the chapter describes how to test nonparametrically the hypothesis
	that an economic model is correct and the hypothesis that a nonparametric
	function satisfies some specified shape properties. },
	doi = {http://dx.doi.org/10.1016/S1573-4412(05)80011-X},
	issn = {1573-4412},
	url = {http://www.sciencedirect.com/science/article/pii/S157344120580011X}
}
%=============NNN=================================
@article{Narayan2007,
	title = "Mean reversion versus random walk in G7 stock prices evidence from multiple trend break unit root tests",
	journal = "Journal of International Financial Markets, Institutions and Money",
	volume = "17",
	number = "2",
	pages = "152 - 166",
	year = "2007",
	issn = "1042-4431",
	doi = "https://doi.org/10.1016/j.intfin.2005.10.002",
	url = "http://www.sciencedirect.com/science/article/pii/S1042443105000892",
	author = "Paresh Kumar Narayan and Russell Smyth",
	keywords = "Random walk",
	keywords = "Stock prices",
	keywords = "Structural break"
}
@Article{Noulas2001,
	author={Noulas, Athanasios G., and Hatzigayios, Thomas., and  Lazaridis, John., and Lyroudi, Katerina},
	title={{Non-parametric production frontier approach to the study of efficiency of non-life insurance companies in Greece}},
	journal={Journal of financial management and analysis : international review of finance},
	year=2001,
	volume={14},
	number={},
	pages={19-26},
	month={},
	
}
@article{Niclas2003,
	Abstract = {Focuses on the benefits of economic freedom. Components used in measuring economic freedom; Importance of economic freedom for economic growth; Importance of income equality; Neoclassical explanation of economic growth.},
	Author = {Berggren, Niclas},
	ISSN = {10861653},
	Journal = {Independent Review},
	Keywords = {CAPITALISM, ECONOMIC development, INCOME distribution, NEOCLASSICAL school of economics},
	Number = {2},
	Pages = {193},
	Title = {The Benefits of Economic Freedom.},
	Volume = {8},
	URL = {http://search.ebscohost.com.ezproxy.library.uq.edu.au/login.aspx?direct=true&db=bah&AN=10946547&site=ehost-live},
	Year = {2003},
}
@MISC{Nakamoto2009,
	author = {Satoshi Nakamoto},
	title = {Bitcoin: A peer-to-peer electronic cash system,” http://bitcoin.org/bitcoin.pdf},
	year = {2009}
}

  
@article{Nyman1989,
	jstor_articletype = {research-article},
	title = {Profit Incentives and Technical Efficiency in the Production of Nursing Home Care},
	author = {Nyman, John A. and Bricker, Dennis L.},
	journal = {The Review of Economics and Statistics},
	jstor_issuetitle = {},
	volume = {71},
	number = {4},
	jstor_formatteddate = {Nov., 1989},
	pages = { 586-594},
	url = {http://www.jstor.org/stable/1928100},
	ISSN = {00346535},
	abstract = {In recent years, nursing home care expenditures have approached 1% of GNP. Their growth is a major contributor to the escalating costs of health care. In this article, we analyze a sample of nursing homes from Wisconsin to determine the characteristics of the efficiently operated nursing homes. Data envelopment analysis is used to calculate efficiency scores for the various nursing homes in the sample. We then use regression analysis to investigate the determinants of efficiency, holding constant the characteristics of the output. We find that for-profit firms have significantly higher efficiency scores.},
	language = {English},
	year = {1989},
	publisher = {The MIT Press},
	copyright = {Copyright © 1989 The MIT Press},
}




@article{Nelson1966,
	jstor_articletype = {research-article},
	title = {Investment in Humans, Technological Diffusion, and Economic Growth},
	author = {Nelson, Richard R. and Phelps, Edmund S.},
	journal = {The American Economic Review},
	jstor_issuetitle = {},
	volume = {56},
	number = {1/2},
	jstor_formatteddate = {Mar. 1, 1966},
	pages = {pp. 69-75},
	url = {http://www.jstor.org/stable/1821269},
	ISSN = {00028282},
	abstract = {},
	language = {English},
	year = {1966},
	publisher = {American Economic Association},
	copyright = {Copyright © 1966 American Economic Association},
}

%=============OOO===============================


%============PPP===============================
@Article{Hall1984,
	author={Hall, Peter},
	title={{Central limit theorem for integrated square error of multivariate nonparametric density estimators}},
	journal={Journal of Multivariate Analysis},
	year=1984,
	volume={14},
	number={1},
	pages={1-16},
	month={February},
	keywords={ central limit theorem integrated square error Martingale nonparametric density estimator U-statisti},
	abstract={Martingale theory is used to obtain a central limit theorem for degenerate U-statistics with variable kernels, which is applied to derive central limit theorems for the integrated square error of multivariate nonparametric density estimators. Previous approaches to this problem have employed Komlós-Major-Tusnády type approximations to the empiric distribution function, and have required the following two restrictive assumptions which are not necessary using the present approach: (i) the data are in one or two dimensions, and (ii) the estimator is constructed suboptimally.},
	url={http://ideas.repec.org/a/eee/jmvana/v14y1984i1p1-16.html}
}

@ARTICLE{Park2014,
	author = {Byeong U. Park and  L\'{e}opold Simar and Valentin Zelenyuk},
	title = {Categorical data in local maximum likelihood: theory and applications
	to productivity analysis},
	journal = {Journal of Productivity Analysis},
	year = {2015},
	volume = {43},
	pages = {199-214},
	number = {2},
	doi = {10.1007/s11123-014-0394-y},
	issn = {0895-562X},
	keywords = {Stochastic frontier models; Truncated regression; Local maximum likelihood;
	Nonparametric smoothing; Categorical variables; C13; C14; C2},
	language = {English},
	publisher = {Springer US},
	url = {http://dx.doi.org/10.1007/s11123-014-0394-y}
} 
@ARTICLE{Politis2001,
	author = {Politis, Dimitris N, and Romano, Joseph P, and Wolf, Michael},
	title = {On the asymptotic theory of subsampling},
	journal = {Statistica Sinica},
	year = {2001},
	volume = {11},
	pages = {1105-1124},
	owner = {uqttrin2},
	timestamp = {2014.04.11}
}

@Article{Park2017,
	Title                    = {Nonparametric estimation of dynamic discrete choice models for time series data },
	Author                   = {Byeong U. Park and L\'{e}opold Simar and Valentin Zelenyuk},
	Journal                  = {Computational Statistics \& Data Analysis },
	Year                     = {2017},
	Pages                    = {97 - 120},
	Volume                   = {108},
	
	Doi                      = {https://doi.org/10.1016/j.csda.2016.10.024},
	ISSN                     = {0167-9473},
	Keywords                 = {Nonparametric quasi-likelihood},
	Url                      = {http://www.sciencedirect.com/science/article/pii/S0167947316302596}
}
@ARTICLE{Park2008,
	author = {Byeong U. Park and L\'{e}opold Simar and Valentin Zelenyuk},
	title = {Local likelihood estimation of truncated regression and its partial
	derivatives: Theory and application },
	journal = {Journal of Econometrics },
	year = {2008},
	volume = {146},
	pages = {185 - 198},
	number = {1},
	issn = {0304-4076},
	keywords = {Nonparametric truncated regression},
	url = {http://www.sciencedirect.com/science/article/pii/S030440760800095X}
}

@BOOK{Pagan1999,
	title = {Nonparametric Econometrics},
	publisher = {Cambridge University Press},
	year = {1999},
	author = {Adrian Pagan and Aman Ullah},
	note = {Cambridge Books Online},
	isbn = {9780511612503},
	url = {http://dx.doi.org/10.1017/CBO9780511612503}
}
@article{Powell1989,
	jstor_articletype = {research-article},
	title = {Semiparametric Estimation of Index Coefficients},
	author = {Powell, James L. and Stock, James H. and Stoker, Thomas M.},
	journal = {Econometrica},
	jstor_issuetitle = {},
	volume = {57},
	number = {6},
	jstor_formatteddate = {Nov., 1989},
	pages = {1403-1430},
	url = {http://www.jstor.org/stable/1913713},
	ISSN = {00129682},
	abstract = {This paper gives a solution to the problem of estimating coefficients of index models, through the estimation of the density-weighted average derivative of a general regression function. We show how a normalized version of the density-weighted average derivatives can be estimated by certain linear instrumental variables coefficients. Both of the estimators are computationally simple, root-N-consistent and asymptotically normal; their statistical properties do not rely on functional form assumptions on the regression function or the distribution of the data. The estimators, based on sample analogues of the product moment representation of the average derivative, are constructed using nonparametric kernel estimators of the density of the regressors. Asymptotic normality is established using extensions of classical U-statistic theorems, and asymptotic bias is reduced through use of a higher-order kernel. Consistent estimators of the asymptotic variance-covariance matrices of the estimators are given, and a limited Monte Carlo simulation is used to study the practical performance of the procedures.},
	language = {English},
	year = {1989},
	publisher = {The Econometric Society},
	copyright = {Copyright © 1989 The Econometric Society},
}

@TechReport{Pang2012,
	author={Pang Du and Christopher F. Parmeter and Jeffrey S. Racine},
	title={{Nonparametric Kernel Regression with Multiple Predictors and Multiple Shape Constraints}},
	year=2012,
	month=Aug,
	institution={McMaster University},
	type={Department of Economics Working Papers},
	url={http://ideas.repec.org/p/mcm/deptwp/2012-08.html},
	number={2012-08},
	abstract={Nonparametric smoothing under shape constraints has recently received much well-deserved attention. Powerful methods have been proposed for imposing a single shape constraint such as monotonicity and concavity on univariate functions. In this paper, we extend the monotone kernel regression method in Hall and Huang (2001) to the multivariate and multi-constraint setting. We impose equality and/or inequality constraints on a nonparametric kernel regression model and its derivatives. A bootstrap procedure is also proposed for testing the validity of the constraints. Consistency of our constrained kernel estimator is provided through an asymptotic analysis of its relationship with the unconstrained estimator. Theoretical underpinnings for the bootstrap procedure are also provided. Illustrative Monte Carlo results are presented and an application is considered.},
	keywords={shape restrictions; nonparametric regression; multivariate kernel estimation; hypothesis testing},
}
@ARTICLE{Parmeter2014,
	author = {Christopher F. Parmeter and Subal C. Kumbhakar},
	title = {Efficiency Analysis: A Primer on Recent Advances},
	journal = {Foundations and TrendsÂ® in Econometrics},
	year = {2014},
	volume = {7},
	pages = {191-385},
	number = {3â€“4},
	doi = {10.1561/0800000023},
	issn = {1551-3076},
	url = {http://dx.doi.org/10.1561/0800000023}
}
@Article{Park1990,
	Title                    = {Comparison of Data-Driven Bandwidth Selectors},
	Author                   = {Byeong U. Park and J. S. Marron},
	Journal                  = {Journal of the American Statistical Association},
	Year                     = {1990},
	Number                   = {409},
	Pages                    = {66-72},
	Volume                   = {85},
	
	Doi                      = {10.1080/01621459.1990.10475307},
	Eprint                   = { 
	http://amstat.tandfonline.com/doi/pdf/10.1080/01621459.1990.10475307
	
	},
}

%===========QQQ================================
@ARTICLE{Quah1996a,
	author = {Quah, DannyT.},
	title = {Convergence empirics across economies with (some) capital mobility},
	journal = {Journal of Economic Growth},
	year = {1996},
	volume = {1},
	pages = {95-124},
	number = {1},
	doi = {10.1007/BF00163344},
	issn = {1381-4338},
	keywords = {convergence club; distribution dynamics; polarization; stochastic
	kernel; twin peaks; C23; O40; O57},
	language = {English},
	publisher = {Kluwer Academic Publishers},
	url = {http://dx.doi.org/10.1007/BF00163344}
}

@article{Quah1996b,
	jstor_articletype = {research-article},
	title = {Twin Peaks: Growth and Convergence in Models of Distribution Dynamics},
	author = {Quah, Danny.},
	journal = {The Economic Journal},
	jstor_issuetitle = {},
	volume = {106},
	number = {437},
	jstor_formatteddate = {Jul., 1996},
	pages = { 1045-1055},
	url = {http://www.jstor.org/stable/2235377},
	ISSN = {00130133},
	abstract = {Convergence concerns poor economies catching up with rich ones. At issue is what happens to the cross sectional distribution of economies, not whether a single economy tends toward its own steady state. It is the latter, however, that has preoccupied the traditional approach to convergence analysis. This paper describes a body of research that overcomes this shortcoming in the traditional approach. The new findings - on persistence and stratification; on the formation of convergence clubs; and on the distribution polarising into twin peaks of which and poor - suggest the relevance of a class of theoretical ideas, different from the production-function accounting traditionally favored.},
	language = {English},
	year = {1996},
	publisher = {Wiley on behalf of the Royal Economic Society},
	copyright = {Copyright Â© 1996 Royal Economic Society},
}

@ARTICLE{Quah1997,
	author = {Quah, DannyT.},
	title = {Empirics for Growth and Distribution: Stratification, Polarization,
	and Convergence Clubs},
	journal = {Journal of Economic Growth},
	year = {1997},
	volume = {2},
	pages = {27-59},
	number = {1},
	doi = {10.1023/A:1009781613339},
	issn = {1381-4338},
	keywords = {conditional; convergence; distribution dynamics; income distribution;
	inequality; trade; twin peaks},
	language = {English},
	publisher = {Kluwer Academic Publishers},
	url = {http://dx.doi.org/10.1023/A%3A1009781613339}
}



%============RRR=============================
@article{Reif1991,
	jstor_articletype = {research-article},
	title = {Systematic Departures from the Frontier: A Framework for the Analysis of Firm Inefficiency},
	author = {Reifschneider, David and Stevenson, Rodney},
	journal = {International Economic Review},
	jstor_issuetitle = {},
	volume = {32},
	number = {3},
	jstor_formatteddate = {Aug., 1991},
	pages = {715-723},
	url = {http://www.jstor.org/stable/2527115},
	ISSN = {00206598},
	abstract = {Departures from a production or cost frontier may reflect the systematic effect of conditions that contribute to inefficiency. To test whether some portion of frontier function departures can be systematically explained, the frontier inefficiency error component is modeled as a function of various causal factors and a random component. The functional form of the inefficiency error term model is chosen to assure strict one-sidedness. Preliminary empirical results generated from electric utility data suggest the superiority of including both systematic and random components in the inefficiency error term.},
	language = {English},
	year = {1991},
	publisher = {Wiley for the Economics Department of the University of Pennsylvania and Institute of Social and Economic Research -- Osaka University},
	copyright = {Copyright © 1991 Economics Department of the University of Pennsylvania},
}

@article{Ray1991,
	jstor_articletype = {research-article},
	title = {Resource-Use Efficiency in Public Schools: A Study of Connecticut Data},
	author = {Ray, Subhash C.},
	journal = {Management Science},
	jstor_issuetitle = {},
	volume = {37},
	number = {12},
	jstor_formatteddate = {Dec., 1991},
	pages = {1620-1628},
	url = {http://www.jstor.org/stable/2632732},
	ISSN = {00251909},
	abstract = {This study combines Data Envelopment Analysis (DEA) with regression modelling to estimate relative efficiency in the public school districts of Connecticut. Factors affecting achievements are classified as school inputs and other socio-economic factors. DEA is performed with the school inputs only. Efficiency measures obtained from DEA are subsequently related to the socio-economic factors in a regression model with a one-sided disturbance term. The findings suggest that while productivity of school inputs varies considerably across districts this can be ascribed to a large extent to differences in the socio-economic background of the communities served. Variation in managerial efficiency is much less than what is implied by the DEA results.},
	language = {English},
	year = {1991},
	publisher = {INFORMS},
	copyright = {Copyright © 1991 INFORMS},
}
@article{Ramsay1988,
	jstor_articletype = {research-article},
	title = {Monotone Regression Splines in Action},
	author = {Ramsay, J. O.},
	journal = {Statistical Science},
	jstor_issuetitle = {},
	volume = {3},
	number = {4},
	jstor_formatteddate = {Nov., 1988},
	pages = {pp. 425-441},
	url = {http://www.jstor.org/stable/2245395},
	ISSN = {08834237},
	abstract = {Piecewise polynomials or splines extend the advantages of polynomials to include greater flexibility, local effects of parameter changes and the possibility of imposing useful constraints on estimated functions. Among these constraints is monotonicity, which can be an important property in many curve estimation problems. This paper shows the virtues of monotone splines through a number of statistical applications, including response variable transformation in nonlinear regression, transformation of variables in multiple regression, principal components and canonical correlation, and the use of monotone splines to model a dose-response function and to perform item analysis. Computational and inferential issues are discussed and illustrated.},
	language = {English},
	year = {1988},
	publisher = {Institute of Mathematical Statistics},
	copyright = {Copyright © 1988 Institute of Mathematical Statistics},
}

@article{Robinson1991,
	jstor_articletype = {research-article},
	title = {Consistent Nonparametric Entropy-Based Testing},
	author = {Robinson, P. M.},
	journal = {The Review of Economic Studies},
	jstor_issuetitle = {Special Issue: The Econometrics of Financial Markets},
	volume = {58},
	number = {3},
	jstor_formatteddate = {May, 1991},
	pages = {437-453},
	url = {http://www.jstor.org/stable/2298005},
	ISSN = {00346527},
	abstract = {The Kullback-Leibler information criterion is used as a basis for one-sided testing of nested hypotheses. No distributional form is assumed, so nonparametric density estimation is used to form the test statistic. In order to obtain a normal null limiting distribution, a form of weighting is employed. The test is also shown to be consistent against a class of alternatives. The exposition focusses on testing for serial independence in time series, with a small application to testing the random walk hypothesis for exchange rate series, and tests of some other hypotheses of econometric interest are briefly described.},
	language = {English},
	year = {1991},
	publisher = {Oxford University Press},
	copyright = {Copyright © 1991 The Review of Economic Studies, Ltd.},
}
@article{Robinson1988,
	jstor_articletype = {research-article},
	title = {Root-N-Consistent Semiparametric Regression},
	author = {Robinson, P. M.},
	journal = {Econometrica},
	jstor_issuetitle = {},
	volume = {56},
	number = {4},
	jstor_formatteddate = {Jul., 1988},
	pages = {931-954},
	url = {http://www.jstor.org/stable/1912705},
	ISSN = {00129682},
	abstract = {One type of semiparametric regression on an <tex-math>$\scr{R}^{p}\times \scr{R}^{q}\text{-valued}$</tex-math> random variable (X, Z) is β′X + θ(Z), where β and θ(Z) are an unknown slope coefficient vector and function, and X is neither wholly dependent on Z nor necessarily independent of it. Estimators of β based on incorrect parameterization of θ are generally inconsistent, whereas consistent nonparametric estimators deviate from β by a larger probability order than N<sup>-1/2</sup>, where N is sample size. An estimator generalizing the ordinary least squares estimator of β is constructed by inserting nonparametric regression estimators in the nonlinear orthogonal projection on Z. Under regularity conditions β̂ is shown to be <tex-math>$N^{1/2}\text{-consistent}$</tex-math> for β and asymptotically normal, and a consistent estimator of its limiting covariance matrix is given, affording statistical inference that is not only asymptotically valid but has nonzero asymptotic first-order efficiency relative to estimators based on a correctly parameterized θ. We discuss the identification problem and β̂'s efficiency, and report results of a Monte Carlo study of finite-sample performance. While the paper focuses on the simplest interesting setting of multiple regression with independent observations, extensions to other econometric models are described, in particular seemingly unrelated and nonlinear regressions, simultaneous equations, distributed lags, and sample selectivity models.},
	language = {English},
	year = {1988},
	publisher = {The Econometric Society},
	copyright = {Copyright © 1988 The Econometric Society},
}

@article{Robinson1995,
	jstor_articletype = {research-article},
	title = {The Normal Approximation for Semiparametric Averaged Derivatives},
	author = {Robinson, P. M.},
	journal = {Econometrica},
	jstor_issuetitle = {},
	volume = {63},
	number = {3},
	jstor_formatteddate = {May, 1995},
	pages = {pp. 667-680},
	url = {http://www.jstor.org/stable/2171912},
	ISSN = {00129682},
	abstract = {With the same normalization as that for standard parametric statistics, and centered at a parameter of interest, many semiparametric estimates based on n observations have been shown to be root-n-consistent and asymptotically normal. In the context of semiparametric averaged derivative estimates, we go further by showing that the rate of convergence of the finite-sample distribution to the normal limit distribution can equal that of standard parametric statistics.},
	language = {English},
	year = {1995},
	publisher = {The Econometric Society},
	copyright = {Copyright © 1995 The Econometric Society},
}

@ARTICLE{Racine2004,
	author = {Jeff Racine and Qi Li},
	title = {Nonparametric estimation of regression functions with both categorical
	and continuous data },
	journal = {Journal of Econometrics },
	year = {2004},
	volume = {119},
	pages = {99 - 130},
	number = {1},
	doi = {http://dx.doi.org/10.1016/S0304-4076(03)00157-X},
	issn = {0304-4076},
	keywords = {Discrete variables},
	url = {http://www.sciencedirect.com/science/article/pii/S030440760300157X}
}

@article{Racine1997,
	jstor_articletype = {research-article},
	title = {Consistent Significance Testing for Nonparametric Regression},
	author = {Racine, Jeff},
	journal = {Journal of Business & Economic Statistics},
	jstor_issuetitle = {},
	volume = {15},
	number = {3},
	jstor_formatteddate = {Jul., 1997},
	pages = {369-378},
	url = {http://www.jstor.org/stable/1392340},
	ISSN = {07350015},
	abstract = {This article presents a framework for individual and joint tests of significance employing nonparametric estimation procedures. The proposed test is based on nonparametric estimates of partial derivatives, is robust to functional misspecification for general classes of models, and employs nested pivotal bootstrapping procedures. Two simulations and one application are considered to examine size and power relative to misspecified parametric models, and to test for the linear unpredictability of exchange-rate movements for G7 currencies.},
	language = {English},
	year = {1997},
	publisher = {Taylor & Francis, Ltd. on behalf of American Statistical Association},
	copyright = {Copyright © 1997 American Statistical Association},
}

@article{Rilstone1991,
	jstor_articletype = {research-article},
	title = {Nonparametric Hypothesis Testing with Parametric Rates of Convergence},
	author = {Rilstone, Paul},
	journal = {International Economic Review},
	jstor_issuetitle = {},
	volume = {32},
	number = {1},
	jstor_formatteddate = {Feb., 1991},
	pages = {209-227},
	url = {http://www.jstor.org/stable/2526941},
	ISSN = {00206598},
	abstract = {Nonparametric estimators are frequently criticized for their poor performance in small samples. In this paper we consider using kernel methods for the estimation of the expected derivatives of a regression function. The proposed estimators are shown to be asymptotically normal and <tex-math>$\sqrt{n}$</tex-math>-consistent. As a consequence their standard errors are comparable to parametric estimates. An empirical example demonstrates the facility of the approach.},
	language = {English},
	year = {1991},
	publisher = {Wiley for the Economics Department of the University of Pennsylvania and Institute of Social and Economic Research -- Osaka University},
	copyright = {Copyright © 1991 Economics Department of the University of Pennsylvania},
}

@article{Ray1997,
	jstor_articletype = {research-article},
	title = {Productivity Growth, Technical Progress, and Efficiency Change in Industrialized Countries: Comment},
	author = {Ray, Subhash C. and Desli, Evangelia},
	journal = {The American Economic Review},
	jstor_issuetitle = {},
	volume = {87},
	number = {5},
	jstor_formatteddate = {Dec., 1997},
	pages = {1033-1039},
	url = {http://www.jstor.org/stable/2951340},
	ISSN = {00028282},
	abstract = {},
	language = {English},
	year = {1997},
	publisher = {American Economic Association},
	copyright = {Copyright © 1997 American Economic Association},
}

@ARTICLE{Ritter1997,
	author = {Ritter, Christian and Simar, L\'{e}opold},
	title = {Pitfalls of Normal-Gamma Stochastic Frontier Models},
	journal = {Journal of Productivity Analysis},
	year = {1997},
	volume = {8},
	pages = {167-182},
	number = {2},
	doi = {10.1023/A:1007751524050},
	issn = {0895-562X},
	keywords = {Identifiability; least squares; likelihood; profile; simulation},
	language = {English},
	publisher = {Kluwer Academic Publishers},
	url = {http://dx.doi.org/10.1023/A%3A1007751524050}
}

%============SSS=============================
@ARTICLE{Simar2002,
	author = {Simar, L\'{e}opold  and Paul W Wilson},
	title = {Non-parametric tests of returns to scale },
	journal = {European Journal of Operational Research },
	year = {2002},
	volume = {139},
	pages = {115 - 132},
	number = {1},
	abstract = {This paper discusses various statistics for testing hypotheses regarding
	returns to scale in the context of non-parametric models of technical
	efficiency. In addition, the paper presents bootstrap estimation
	procedures which yield appropriate critical values for the test statistics.
	Evidence on the true sizes and power of the various proposed tests
	is obtained from Monte-Carlo experiments. This paper is an extension
	of earlier work in [Manage. Sci. 44 (1998) 49; J. Appl. Statist.
	27 (2000b) 779]. },
	doi = {http://dx.doi.org/10.1016/S0377-2217(01)00167-9},
	issn = {0377-2217},
	keywords = {Returns to scale},
	url = {http://www.sciencedirect.com/science/article/pii/S0377221701001679}
}
@ARTICLE{Sim1992,
	author = {Christopher A. Sims},
	title = {Interpreting the macroeconomic time series facts},
	journal = {European Economic Review},
	year = {1992},
	volume = {36},
	pages = {975 - 1000},
	number = {5},
	doi = {http://dx.doi.org/10.1016/0014-2921(92)90041-T},
	issn = {0014-2921},
	url = {http://www.sciencedirect.com/science/article/pii/001429219290041T}
}


@ARTICLE{Simar1992,
	author = {Simar, L\'{e}opold },
	title = {Estimating efficiencies from frontier models with panel data: A comparison
	of parametric, non-parametric and semi-parametric methods with bootstrapping},
	journal = {Journal of Productivity Analysis},
	year = {1992},
	volume = {3},
	pages = {171-203},
	number = {1-2},
	doi = {10.1007/BF00158775},
	issn = {0895-562X},
	language = {English},
	publisher = {Kluwer Academic Publishers},
	url = {http://dx.doi.org/10.1007/BF00158775}
}
@Article{Solow1956,
	Title                    = {A Contribution to the Theory of Economic Growth},
	Author                   = {Solow, Robert M.},
	Journal                  = {The Quarterly Journal of Economics},
	Year                     = {1956},
	Number                   = {1},
	Pages                    = {65},
	Volume                   = {70},
	
	Doi                      = {10.2307/1884513},
	Eprint                   = {/oup/backfile/Content_public/Journal/qje/70/1/10.2307/1884513/2/70-1-65.pdf},
	Url                      = { + http://dx.doi.org/10.2307/1884513}
}

@article{Sala1996,
	ISSN = {00130133, 14680297},
	URL = {http://www.jstor.org/stable/2235375},
	author = {Sala-i-Martin, Xavier X.},
	journal = {The Economic Journal},
	number = {437},
	pages = {1019-1036},
	publisher = {[Royal Economic Society, Wiley]},
	title = {The Classical Approach to Convergence Analysis},
	volume = {106},
	year = {1996}
}


@ARTICLE{Simar2007,
	author = {Simar, L\'{e}opold },
	title = {{How to improve the performances of DEA/FDH estimators in the presence
	of noise?}},
	journal = {Journal of Productivity Analysis},
	year = {2007},
	volume = {28},
	pages = {183-201},
	number = {3},
	doi = {10.1007/s11123-007-0057-3},
	issn = {0895-562X},
	keywords = {Frontier; Nonparametric estimation; Stochastic DEA/FDH; Robustness
	to outliers; C13; C14; D20},
	language = {English},
	publisher = {Springer US},
	url = {http://dx.doi.org/10.1007/s11123-007-0057-3}
}
@book{Sickles2015,
	Title                    = {Panel Data and Productivity Measurement},
	author                   = {Robin C. Sickles and Jiaqi Hao and Chenjun Shang},
	Year                   = {2015},
	ISBN                     = {9780199940042},
	Publisher                = {In The Oxford Handbook of Panel Data: Oxford University Press},
	Url                      = {//www.oxfordhandbooks.com/10.1093/oxfordhb/9780199940042.001.0001/oxfordhb-9780199940042-e-17}
}

@article{Sheather1991,
	ISSN = {00359246},
	URL = {http://www.jstor.org/stable/2345597},
	abstract = {We present a new method for data-based selection of the bandwidth in kernel density estimation which has excellent properties. It improves on a recent procedure of Park and Marron (which itself is a good method) in various ways. First, the new method has superior theoretical performance; second, it also has a computational advantage; third, the new method has reliably good performance for smooth densities in simulations, performance that is second to none in the existing literature. These methods are based on choosing the bandwidth to (approximately) minimize good quality estimates of the mean integrated squared error. The key to the success of the current procedure is the reintroduction of a non-stochastic term which was previously omitted together with use of the bandwidth to reduce bias in estimation without inflating variance.},
	author = {S. J. Sheather and M. C. Jones},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	number = {3},
	pages = {683-690},
	publisher = {[Royal Statistical Society, Wiley]},
	title = {A Reliable Data-Based Bandwidth Selection Method for Kernel Density Estimation},
	volume = {53},
	year = {1991}
}



@article{Sylvia2008,
	title = "Marginal likelihoods for non-Gaussian models using auxiliary mixture sampling ",
	journal = "Computational Statistics & Data Analysis ",
	volume = "52",
	number = "10",
	pages = "4608 - 4624",
	year = "2008",  
	note = "",
	issn = "0167-9473",
	doi = "http://dx.doi.org/10.1016/j.csda.2008.03.028",
	url = "http://www.sciencedirect.com/science/article/pii/S016794730800176X",
	author = {Sylvia Fr\"{u}hwirth-Schnatter and Helga Wagner}}
@article{Simar1998,
	jstor_articletype = {research-article},
	title = {{Sensitivity Analysis of Efficiency Scores: How to Bootstrap in Nonparametric Frontier Models}},
	author = {Simar, L\'{e}opold  and Wilson, Paul W.},
	journal = {Management Science},
	volume = {44},
	number = {1},
	jstor_formatteddate = {Jan., 1998},
	pages = {49-61},
	ISSN = {00251909},
	year = {1998},
	publisher = {INFORMS},
	copyright = {Copyright Â© 1998 INFORMS},
}
@BOOK{Simar2008,
	title = {{Statistical inference in nonparametric frontier models: recent developments and perspectives}},
	publisher = {In: Fried H, Lovell CAK, Schmidt S (eds) The measurement of productive efficiency, 2nd edn. Oxford University Press, Oxford},
	year = {2008},
	author = {Simar, L\'{e}opold  and Wilson, Paul W.},
	owner = {uqttrin2},
	timestamp = {2014.04.09}
}



@ARTICLE{Mastromarco2014,
	author = {Mastromarco, Camilla and Simar, L\'{e}opold},
	title = {Effect of {FDI} and Time on Catching Up: New Insights from a Conditional
	Nonparametric Frontier Analysis},
	journal = {Journal of Applied Econometrics},
	year = {2015},
	volume = {30},
	pages = {826--847},
	number = {5},
	doi = {10.1002/jae.2382},
	issn = {1099-1255},
	url = {http://dx.doi.org/10.1002/jae.2382}
}

@ARTICLE{Simar2000a,
	author = {Simar, L\'{e}opold and Wilson, Paul W.},
	title = {Statistical Inference in Nonparametric Frontier Models: The State
	of the Art},
	journal = {Journal of Productivity Analysis},
	year = {2000},
	volume = {13},
	pages = {49-78},
	number = {1},
	doi = {10.1023/A:1007864806704},
	issn = {0895-562X},
	keywords = {DEA; FDH; Nonparametric estimation; Efficiency; Frontier models; Bootstrapping},
	language = {English},
	publisher = {Kluwer Academic Publishers},
	url = {http://dx.doi.org/10.1023/A%3A1007864806704}
}
@article{Sim1980,
	ISSN = {00129682, 14680262},
	URL = {http://www.jstor.org/stable/1912017},
	abstract = {Existing strategies for econometric analysis related to macroeconomics are subject to a number of serious objections, some recently formulated, some old. These objections are summarized in this paper, and it is argued that taken together they make it unlikely that macroeconomic models are in fact over identified, as the existing statistical theory usually assumes. The implications of this conclusion are explored, and an example of econometric work in a non-standard style, taking account of the objections to the standard style, is presented.},
	author = {Christopher A. Sims},
	journal = {Econometrica},
	number = {1},
	pages = {1-48},
	publisher = {[Wiley, Econometric Society]},
	title = {Macroeconomics and Reality},
	volume = {48},
	year = {1980}
}



@ARTICLE{Simar2000,
	author = {Simar, L\'{e}opold and Wilson, Paul W.},
	title = {A general methodology for bootstrapping in non-parametric frontier
	models},
	journal = {Journal of Applied Statistics},
	year = {2000},
	volume = {27},
	pages = {779-802},
	number = {6},
	doi = {10.1080/02664760050081951},
	eprint = {http://www.tandfonline.com/doi/pdf/10.1080/02664760050081951},
	url = {http://www.tandfonline.com/doi/abs/10.1080/02664760050081951}
}
@ARTICLE{Simar2011,
	author = {Simar, L\'{e}opold  and Zelenyuk, Valentin},
	title = {{Stochastic FDH/DEA estimators for frontier analysis}},
	journal = {Journal of Productivity Analysis},
	year = {2011},
	volume = {36},
	pages = {1-20},
	number = {1},
	doi = {10.1007/s11123-010-0170-6},
	issn = {0895-562X},
	keywords = {Stochastic frontier; Nonparametric frontier; Local maximum likelihood;
	Stochastic DEA; C13; C14; C2},
	language = {English},
	publisher = {Springer US},
	url = {http://dx.doi.org/10.1007/s11123-010-0170-6}
}


@ARTICLE{Simar2007a,
	author = {Simar, L\'{e}opold  and Wilson, Paul W.},
	title = {Estimation and inference in two-stage, semi-parametric models of
	production processes },
	journal = {Journal of Econometrics },
	year = {2007},
	volume = {136},
	pages = {31-64},
	number = {1},
	abstract = {Many papers have regressed non-parametric estimates of productive
	efficiency on environmental variables in two-stage procedures to
	account for exogenous factors that might affect firmsâ€™ performance.
	None of these have described a coherent data-generating process (DGP).
	Moreover, conventional approaches to inference employed in these
	papers are invalid due to complicated, unknown serial correlation
	among the estimated efficiencies. We first describe a sensible \{DGP\}
	for such models. We propose single and double bootstrap procedures;
	both permit valid inference, and the double bootstrap procedure improves
	statistical efficiency in the second-stage regression. We examine
	the statistical performance of our estimators using Monte Carlo experiments.
	},
	doi = {http://dx.doi.org/10.1016/j.jeconom.2005.07.009},
	issn = {0304-4076},
	keywords = {Data envelopment analysis},
	url = {http://www.sciencedirect.com/science/article/pii/S0304407605001594}
}

@ARTICLE{Simar2011c,
	author = {Simar, L\'{e}opold and Wilson, Paul W.},
	title = {Two-stage {DEA}: caveat emptor},
	journal = {Journal of Productivity Analysis},
	year = {2011},
	volume = {36},
	pages = {205-218},
	number = {2},
	doi = {10.1007/s11123-011-0230-6},
	issn = {0895-562X},
	keywords = {Technical efficiency; Two-stage estimation; Bootstrap; Data envelopment
	analysis (DEA); C12; C14; C61; D24},
	language = {English},
	publisher = {Springer US},
	url = {http://dx.doi.org/10.1007/s11123-011-0230-6}
}


@ARTICLE{Simar2011b,
	author = {Simar, L\'{e}opold  and Wilson, Paul W.},
	title = {Inference by the m out of n bootstrap in nonparametric frontier models},
	journal = {Journal of Productivity Analysis},
	year = {2011},
	volume = {36},
	pages = {33-53},
	number = {1},
	doi = {10.1007/s11123-010-0200-4},
	issn = {0895-562X},
	keywords = {Nonparametric frontier; Efficiency; Bootstrap; Nonparametric testing;
	Testing convexity; Testing returns to scale; C14; C12; C18},
	language = {English},
	publisher = {Springer US},
	url = {http://dx.doi.org/10.1007/s11123-010-0200-4}
}

@TECHREPORT{Simar1994,
	author = {Simar,  L\'{e}opold, and Lovel, A.K, and Vanden Eeckaut,P.},
	title = {Stochastic Frontiers Incorporating Exogenous Influences on Efficiency},
	institution = {Discussion Paper No 9403, Institut de Statistique, Universite Catholique
	de Louvain, Lovain-la-Neuve, Belgium},
	year = {1994},
	owner = {uqttrin2},
	timestamp = {2014.11.20}
}
@ARTICLE{Solow1957,
	author = {Solow,R.W.},
	title = {Technical Change and the Aggregate Production Function},
	journal = {Review of Economics and Statistics},
	year = {1957},
	volume = {39},
	pages = {312-320},
	number = {},
	doi = {10.1007/s11123-010-0170-6},
	
}
@article{Stone1984,
	author = "Stone, Charles J.",
	doi = "10.1214/aos/1176346792",
	fjournal = "The Annals of Statistics",
	journal = "Ann. Statist.",
	month = "12",
	number = "4",
	pages = "1285--1297",
	publisher = "The Institute of Mathematical Statistics",
	title = "An Asymptotically Optimal Window Selection Rule for Kernel Density Estimates",
	url = "http://dx.doi.org/10.1214/aos/1176346792",
	volume = "12",
	year = "1984"
}

@BOOK{Shephard1970,
	title = {Theory of cost and production functions},
	publisher = {Princeton, NJ:Princeton University Press},
	year = {1970},
	author = {Shephard, Ronald W.},
	owner = {uqttrin2},
	timestamp = {2014.04.09}
}

@article{Silverman1981,
	jstor_articletype = {research-article},
	title = {Using Kernel Density Estimates to Investigate Multimodality},
	author = {Silverman, B. W.},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	jstor_issuetitle = {},
	volume = {43},
	number = {1},
	jstor_formatteddate = {1981},
	pages = { 97-99},
	url = {http://www.jstor.org/stable/2985156},
	ISSN = {00359246},
	abstract = {A technique for using kernel density estimates to investigate the number of modes in a population is described and discussed. The amount of smoothing is chosen automatically in a natural way.},
	language = {English},
	year = {1981},
	publisher = {Wiley for the Royal Statistical Society},
	copyright = {Copyright Â© 1981 Royal Statistical Society},
}
@BOOK{Silverman1986,
	title = {Density Estimation for Statistics and Data Analysis},
	publisher = {London:Chapman and Hall},
	year = {1986},
	author = {Silverman, B. W.},
	owner = {uqttrin2},
	timestamp = {2014.04.09}
}

@article{Sheater1991,
	jstor_articletype = {research-article},
	title = {A Reliable Data-Based Bandwidth Selection Method for Kernel Density Estimation},
	author = {Sheather, S. J. and Jones, M. C.},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	jstor_issuetitle = {},
	volume = {53},
	number = {3},
	jstor_formatteddate = {1991},
	pages = {683-690},
	url = {http://www.jstor.org/stable/2345597},
	ISSN = {00359246},
	abstract = {We present a new method for data-based selection of the bandwidth in kernel density estimation which has excellent properties. It improves on a recent procedure of Park and Marron (which itself is a good method) in various ways. First, the new method has superior theoretical performance; second, it also has a computational advantage; third, the new method has reliably good performance for smooth densities in simulations, performance that is second to none in the existing literature. These methods are based on choosing the bandwidth to (approximately) minimize good quality estimates of the mean integrated squared error. The key to the success of the current procedure is the reintroduction of a non-stochastic term which was previously omitted together with use of the bandwidth to reduce bias in estimation without inflating variance.},
	language = {English},
	year = {1991},
	publisher = {Wiley for the Royal Statistical Society},
	copyright = {Copyright Â© 1991 Royal Statistical Society},
}

%============TTT============================
@article{Tibs1987,
	jstor_articletype = {research-article},
	title = {Local Likelihood Estimation},
	author = {Tibshirani, Robert and Hastie, Trevor},
	journal = {Journal of the American Statistical Association},
	
	volume = {82},
	number = {398},
	jstor_formatteddate = {Jun., 1987},
	pages = {559-567},
	url = {http://www.jstor.org/stable/2289465},
	ISSN = {01621459},
	year = {1987},
	publisher = {American Statistical Association},
	
}
@ARTICLE{Tsionas2007,
	author = {Tsionas, Efthymios G.},
	title = {Efficiency Measurement with the Weibull Stochastic Frontier*},
	journal = {Oxford Bulletin of Economics and Statistics},
	year = {2007},
	volume = {69},
	pages = {693--706},
	number = {5},
	doi = {10.1111/j.1468-0084.2007.00475.x},
	issn = {1468-0084},
	keywords = {C13, D24},
	publisher = {Blackwell Publishing Ltd},
	url = {http://dx.doi.org/10.1111/j.1468-0084.2007.00475.x}
}

@article{Timmer1971,
	jstor_articletype = {research-article},
	title = {Using a Probabilistic Frontier Production Function to Measure Technical Efficiency},
	author = {Timmer, C. P.},
	journal = {Journal of Political Economy},
	jstor_issuetitle = {},
	volume = {79},
	number = {4},
	jstor_formatteddate = {Jul. - Aug., 1971},
	pages = {776-794},
	url = {http://www.jstor.org/stable/1830828},
	ISSN = {00223808},
	abstract = {This article uses linear programming techniques to "estimate" a frontier Cobb-Douglas production function for U.S. agriculture from 1960 to 1967, using the "average farm" in each state in each year as an observation. Both deterministic and probabilistic frontiers are generated and the results compared with ordinary least-squares and analysis of covariance estimates of the production function. Technical inefficiency is defined relative to the probabilistic frontier function and the extent of any inefficiency calculated for each state. Little technical inefficiency exists across states when the production function includes intermediate inputs as well as land, labor, and capital.},
	language = {English},
	year = {1971},
	publisher = {The University of Chicago Press},
	copyright = {Copyright Â© 1971 The University of Chicago Press},
}

@Article{Tsionas2006,
	author={Efthymios G. Tsionas},
	title={{Inference in dynamic stochastic frontier models}},
	journal={Journal of Applied Econometrics},
	year=2006,
	volume={21},
	number={5},
	pages={669-676},
	month={},
	keywords={},
	abstract={ An important issue in models of technical efficiency measurement concerns the temporal behaviour of inefficiency. Consideration of dynamic models is necessary but inference in such models is complicated. In this paper we propose a stochastic frontier model that allows for technical inefficiency effects and dynamic technical inefficiency, and use Bayesian inference procedures organized around data augmentation techniques to provide inferences. Also provided are firm-specific efficiency measures. The new methods are applied to a panel of large US commercial banks over the period 1989-2000. Copyright © 2006 John Wiley \& Sons, Ltd.},
	url={http://ideas.repec.org/a/jae/japmet/v21y2006i5p669-676.html}
}
@ARTICLE{Tran2009,
	author = {Kien C. Tran and Efthymios G. Tsionas},
	title = {Estimation of nonparametric inefficiency effects stochastic frontier
	models with an application to British manufacturing },
	journal = {Economic Modelling },
	year = {2009},
	volume = {26},
	pages = {904 - 909},
	number = {5},
	doi = {http://dx.doi.org/10.1016/j.econmod.2009.02.011},
	issn = {0264-9993},
	keywords = {Two-step semiparametric estimator},
	url = {http://www.sciencedirect.com/science/article/pii/S026499930900042X}
}
@Article{Tone2014,
	Title                    = {Dynamic {DEA} with network structure: A slacks-based measure approach },
	Author                   = {Kaoru Tone and Miki Tsutsui},
	Journal                  = {Omega },
	Year                     = {2014},
	Number                   = {1},
	Pages                    = {124 - 131},
	Volume                   = {42},
	
	Abstract                 = {Abstract We propose a dynamic \{DEA\} model involving network structure in each period within the framework of a slacks-based measure approach. We have previously published the network \{SBM\} (NSBM) and the dynamic \{SBM\} (DSBM) models separately. Hence, this article is a composite of these two models. Vertically, we deal with multiple divisions connected by links of network structure within each period and, horizontally, we combine the network structure by means of carry-over activities between two succeeding periods. This model can evaluate (1) the overall efficiency over the entire observed period, (2) dynamic change of period efficiency and (3) dynamic change of divisional efficiency. The model can be implemented in input-, output- or non-(both) oriented forms under the \{CRS\} or \{VRS\} assumptions on the production possibility set. Finally, we applied this model to a dataset of \{US\} electric utilities and compared the result with that of DSBM. },
	Doi                      = {http://dx.doi.org/10.1016/j.omega.2013.04.002},
	ISSN                     = {0305-0483},
	Keywords                 = {Dynamic \{DEA\}}
}
@Article{Tone2010,
	Title                    = {Dynamic {DEA}: A slacks-based measure approach },
	Author                   = {Kaoru Tone and Miki Tsutsui},
	Journal                  = {Omega },
	Year                     = {2010},
	Number                   = {3-4},
	Pages                    = {145-156},
	Volume                   = {38},
	
	Abstract                 = {In data envelopment analysis, there are several methods for measuring efficiency changes over time, e.g. the window analysis and the Malmquist index. However, they usually neglect carry-over activities between two consecutive terms and only focus on the separate time period independently aiming local optimization in a single period, even if these models can take into account the time change effect. In the actual business world, a long time planning and investment is a subject of great concern. For these cases, single period optimization model is not suitable for performance evaluation. To cope with long time point of view, the dynamic \{DEA\} model incorporates carry-over activities into the model and enables us to measure period specific efficiency based on the long time optimization during the whole period. Dynamic \{DEA\} model proposed by FÃ¤re and Grosskopf is the first innovative contribution for such purpose. In this paper we develop their model in the slacks-based measure (SBM) framework, called dynamic \{SBM\} (DSBM). The \{SBM\} model is non-radial and can deal with inputs/outputs individually, contrary to the radial approaches that assume proportional changes in inputs/outputs. Furthermore, according to the characteristics of carry-overs, we classify them into four categories, i.e. desirable, undesirable, free and fixed. Desirable carry-overs correspond, for example, to profit carried forward and net earned surplus carried to the next term, while undesirable carry-overs include, for example, loss carried forward, bad debt and dead stock. Free and fixed carry-overs indicate, respectively, discretionary and non-discretionary ones. We develop dynamic \{SBM\} models that can evaluate the overall efficiency of decision making units for the whole terms as well as the term efficiencies. },
	Doi                      = {http://dx.doi.org/10.1016/j.omega.2009.07.003},
	ISSN                     = {0305-0483},
	Keywords                 = {\{DEA\}}
}
@Article{Tone2009,
	Title                    = {Network {DEA}: A slacks-based measure approach },
	Author                   = {Kaoru Tone and Miki Tsutsui},
	Journal                  = {European Journal of Operational Research },
	Year                     = {2009},
	Number                   = {1},
	Pages                    = {243-252},
	Volume                   = {197},
	
	Abstract                 = {Traditional \{DEA\} models deal with measurements of relative efficiency of \{DMUs\} regarding multiple-inputs vs. multiple-outputs. One of the drawbacks of these models is the neglect of intermediate products or linking activities. After pointing out needs for inclusion of them to \{DEA\} models, we propose a slacks-based network \{DEA\} model, called Network SBM, that can deal with intermediate products formally. Using this model we can evaluate divisional efficiencies along with the overall efficiency of decision making units (DMUs). },
	Doi                      = {http://dx.doi.org/10.1016/j.ejor.2008.05.027},
	ISSN                     = {0377-2217},
	Keywords                 = {\{DEA\}}
}


%============UUU===========================

%=============VVV===========================
@ARTICLE{Valentin2006,
	author = {Simar, L\'{e}opold  and Zelenyuk, Valentin},
	title = {On Testing Equality of Distributions of Technical Efficiency Scores},
	journal = {Econometric Reviews},
	year = {2006},
	volume = {25},
	pages = {497-522},
	number = {4},
	doi = {10.1080/07474930600972582},
	eprint = {http://www.tandfonline.com/doi/pdf/10.1080/07474930600972582},
	url = {http://www.tandfonline.com/doi/abs/10.1080/07474930600972582}
}

@Article{Valentin2007,
	author={L\'{e}opold Simar and Valentin Zelenyuk},
	title={{Statistical inference for aggregates of Farrell-type efficiencies}},
	journal={Journal of Applied Econometrics},
	year=2007,
	volume={22},
	number={7},
	pages={1367-1394},
	month={},
	keywords={},
	abstract={ In this study, we merge results of two recent directions in efficiency analysis research-aggregation and bootstrap-applied, as an example, to one of the most popular point estimators of individual efficiency: the data envelopment analysis (DEA) estimator. A natural context of the methodology developed here is a study of efficiency of a particular economic system (e.g., an industry) as a whole, or a comparison of efficiencies of distinct groups within such a system (e.g., regulated vs. non-regulated firms or private vs. public firms). Our methodology is justified by the (neoclassical) economic theory and is supported by carefully adapted statistical methods. Copyright Â© 2007 John Wiley \& Sons, Ltd.},
	url={http://ideas.repec.org/a/jae/japmet/v22y2007i7p1367-1394.html}
}

@ARTICLE{Valentin2013,
	author = {Byeong U. Park and  L\'{e}opold Simar and Valentin Zelenyuk},
	title = {Categorical data in local maximum likelihood: theory and applications
	to productivity analysis},
	journal = {Journal of Productivity Analysis},
	year = {2014},
	pages = {1-16},
	doi = {10.1007/s11123-014-0394-y},
	issn = {0895-562X},
	keywords = {Stochastic frontier models; Truncated regression; Local maximum likelihood;
	Nonparametric smoothing; Categorical variables; C13; C14; C2},
	language = {English},
	publisher = {Springer US},
	url = {http://dx.doi.org/10.1007/s11123-014-0394-y}
}
@Article{Valentin2013a,
	author={Zelenyuk, Valentin},
	title={{A scale elasticity measure for directional distance function and its dual: Theory and DEA estimation}},
	journal={European Journal of Operational Research},
	year=2013,
	volume={228},
	number={3},
	pages={592-600},
	month={},
	keywords={Scale elasticity; Production theory; Distance functions; Duality theory},
	abstract={In this paper we focus on scale elasticity measure based on directional distance function for multi-outputâ€“multi-input technologies, explore its fundamental properties and show its equivalence with the input oriented and output oriented scale elasticity measures. We also establish duality relationship between the scale elasticity measure based on the directional distance function with scale elasticity measure based on the profit function. Finally, we discuss the estimation issues of the scale elasticity based on the directional distance function via the DEA estimator.},
	url={http://ideas.repec.org/a/eee/ejores/v228y2013i3p592-600.html}
}


}

@Article{Valentin2013b,
	author={Valentin Zelenyuk},
	title={{A Note on Equivalences in Measuring Returns to Scale}},
	journal={International Journal of Business and Economics},
	year=2013,
	volume={12},
	number={1},
	pages={85-89},
	month={June},
	keywords={scale elasticity; economies of scale; distance functions},
	abstract={No abstract is available for this item.},
	url={http://ideas.repec.org/a/ijb/journl/v12y2013i1p85-89.html}
}

@TechReport{Valentin2002,
	author={F\"{a}re, Rolf and Grosskopf, Shawna and Zelenyuk, Valentin},
	title={{Finding Common Ground: Efficiency Indices}},
	year=2002,
	month=Jan,
	institution={University Library of Munich, Germany},
	type={MPRA Paper},
	url={http://ideas.repec.org/p/pra/mprapa/28004.html},
	number={28004},
	abstract={The last two decades have witnessed a revival in interest in the measurement of productive efficiency pioneered by (1957) and (1951). 1978 was a watershed year in this revival with the christening of DEA by (1978) and the critique of Farrell technical efficiency in terms of axiomatic production and index number theory in Fare and (1978). These papers have inspired many others to apply these methods and to add to the debate on how best to define technical efficiency.},
	keywords={Efficiency Measurement; Russell Efficiency; Farrell Efficiency},
}

@BOOK{Varian1992,
	title = {Microeconomic analysis},
	publisher = {W.W.Norton, New York},
	year = {1992},
	author = {Varian,H.R},
	
}
@Article{Valentin2002a,
	author={F\"{a}re, Rolf  and Valentin Zelenyuk},
	title={Input aggregation and technical efficiency},
	journal={Applied Economics Letters},
	year=2002,
	volume={9},
	number={10},
	pages={635-636},
	month={},
	keywords={},
	abstract={ This paper defines the notion of unbiased aggregation of inputs and provides a necessary and sufficient condition for this to apply.},
	url={http://ideas.repec.org/a/taf/apeclt/v9y2002i10p635-636.html}
} 
@Article{Valentin2007a,
	author={F\"{a}re, Rolf  and Zelenyuk, Valentin},
	title={{Extending F\"{a}re and Zelenyuk (2003)}},
	journal={European Journal of Operational Research},
	year=2007,
	volume={179},
	number={2},
	pages={594-595},
	month={June},
	keywords={},
	abstract={No abstract is available for this item.},
	url={http://ideas.repec.org/a/eee/ejores/v179y2007i2p594-595.html}
}

%==============WWW=============================
@ARTICLE{Whang1993,
	author = {Yoon-Jae Whang and Donald W.K. Andrews},
	title = {Tests of specification for parametric and semiparametric models },
	journal = {Journal of Econometrics },
	year = {1993},
	volume = {57},
	pages = {277 - 318},
	number = {1â€“3},
	abstract = {This paper provides a general framework for constructing specification
	tests for parametric and semiparametric models. The paper develops
	new specification tests using the general framework. In particular,
	specification tests for semiparametric partially linear regression
	and sample selection models are introduced. The results apply in
	time series and cross-sectional contexts. The method of proof exploits
	results concerning the stochastic equicontinuity or weak convergence
	of normalized sums of stochastic processes. },
	doi = {http://dx.doi.org/10.1016/0304-4076(93)90068-G},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/030440769390068G}
}

@ARTICLE{Weinstein1964,
	title = {The Sum of Values from a Normal and a Truncated Normal Distribution},
	author= {Weinstein,M.A},
	journal = {Technometrics},
	year = {1964},
	volume = {6},
	pages = {469-471},
	number = {4},
	doi = {10.1080/00401706.1964.10490210},
	eprint = {http://www.tandfonline.com/doi/pdf/10.1080/00401706.1964.10490210},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00401706.1964.10490210}
}


@ARTICLE{Wang2002,
	author = {Wang, Hung-jen and Schmidt, Peter},
	title = {One-Step and Two-Step Estimation of the Effects of Exogenous Variables
	on Technical Efficiency Levels},
	journal = {Journal of Productivity Analysis},
	year = {2002},
	volume = {18},
	pages = {129-144},
	number = {2},
	doi = {10.1023/A:1016565719882},
	issn = {0895-562X},
	keywords = {technical efficiency; stochastic frontiers},
	language = {English},
	publisher = {Kluwer Academic Publishers},
	url = {http://dx.doi.org/10.1023/A%3A1016565719882}
}
@article{Winston1974,
	ISSN = {00220515},
	URL = {http://www.jstor.org/stable/2722381},
	author = {Gordon C. Winston},
	journal = {Journal of Economic Literature},
	number = {4},
	pages = {1301-1320},
	publisher = {American Economic Association},
	title = {The Theory of Capital Utilization and Idleness},
	volume = {12},
	year = {1974}
}
@ARTICLE{Wang2012,
	author = {Miao Wang and M. C. Sunny Wong},
	title = {International {R\&D} Transfer and Technical Efficiency: Evidence
	from Panel Study Using Stochastic Frontier Analysis },
	journal = {World Development },
	year = {2012},
	volume = {40},
	pages = {1982-1998},
	number = {10},
	doi = {http://dx.doi.org/10.1016/j.worlddev.2012.05.001},
	issn = {0305-750X},
	keywords = {R&amp;D},
	url = {http://www.sciencedirect.com/science/article/pii/S0305750X12001118}
}
@article {Werner2000,
	author = {Smolny, Werner},
	title = {Post-War Growth, Productivity Convergence and Reconstruction},
	journal = {Oxford Bulletin of Economics and Statistics},
	volume = {62},
	number = {5},
	publisher = {Blackwell Publishers Ltd},
	issn = {1468-0084},
	url = {http://dx.doi.org/10.1111/1468-0084.00191},
	doi = {10.1111/1468-0084.00191},
	pages = {589--606},
	year = {2000},
}


%==============XXX=========================

%=============YYY==========================
@article{Young1995,
	author = {Young, Alwyn}, 
	title = {The Tyranny of Numbers: Confronting the Statistical Realities of the {East Asian} Growth Experience},
	volume = {110}, 
	number = {3}, 
	pages = {641-680}, 
	year = {1995}, 
	doi = {10.2307/2946695}, 
	abstract ={This paper documents the fundamental role played by factor accumulation in explaining the extraordinary postwar growth of Hong Kong, Singapore, South Korea, and Taiwan. Participation rates, educational levels, and (excepting Hong Kong) investment rates have risen rapidly in all four economies. In addition, in most cases there has been a large intersectoral transfer of labor into manufacturing, which has helped fuel growth in that sector. Once one accounts for the dramatic rise in factor inputs, one arrives at estimated total factor productivity growth rates that are closely approximated by the historical performance of many of the OECD and Latin American economies. While the growth of output and manufacturing exports in the newly industrializing countries of East Asia is virtually unprecedented, the growth of total factor productivity in these economies is not.}, 
	URL = {http://qje.oxfordjournals.org/content/110/3/641.abstract}, 
	eprint = {http://qje.oxfordjournals.org/content/110/3/641.full.pdf+html}, 
	journal = {The Quarterly Journal of Economics} 
}
@article{Yatchew1992,
	jstor_articletype = {research-article},
	title = {Nonparametric Regression Tests Based on Least Squares},
	author = {Yatchew, Adonis John},
	journal = {Econometric Theory},
	jstor_issuetitle = {},
	volume = {8},
	number = {4},
	jstor_formatteddate = {Dec., 1992},
	pages = {435-451},
	url = {http://www.jstor.org/stable/3532124},
	ISSN = {02664666},
	abstract = {This paper proposes tests on semiparametric models based on the sum of squared residuals from a least-squares procedure. Smoothness conditions are imposed on the nonparametric portion of the model to obtain asymptotic normality of the sum of squared residuals. The approach yields tests of specification, significance, smoothness and concavity and allows for heteroskedastic residuals.},
	language = {English},
	year = {1992},
	publisher = {Cambridge University Press},
	copyright = {Copyright © 1992 Cambridge University Press},
}

%=============ZZZ===========================

@article{Zellner1966,
	jstor_articletype = {research-article},
	title = {Specification and Estimation of Cobb-Douglas Production Function Models},
	author = {Zellner, A. and Kmenta, J. and Dr\'{e}ze, J.},
	journal = {Econometrica},
	jstor_issuetitle = {},
	volume = {34},
	number = {4},
	jstor_formatteddate = {Oct., 1966},
	pages = {784-795},
	url = {http://www.jstor.org/stable/1910099},
	ISSN = {00129682},
	
	year = {1966},
	publisher = {The Econometric Society},
	copyright = {Copyright Â© 1966 The Econometric Society},
}
@ARTICLE{Zieschang1984,
	author = {Kimberly D Zieschang},
	title = {{An extended 
	l technical efficiency measure} },
	journal = {Journal of Economic Theory },
	year = {1984},
	volume = {33}, 
	pages = {387 - 396},
	number = {2},
	doi = {http://dx.doi.org/10.1016/0022-0531(84)90101-7},
	issn = {0022-0531},
	url = {http://www.sciencedirect.com/science/article/pii/0022053184901017}
}
@ARTICLE{Yang2006,
	author = {Zijiang Yang},
	title = {A two-stage \{DEA\} model to evaluate the overall performance of
	Canadian life and health insurance companies },
	journal = {Mathematical and Computer Modelling },
	year = {2006},
	volume = {43},
	pages = {910 - 919},
	
	abstract = {A two-stage data envelopment analysis (DEA) model is created to provide
	valuable managerial insights when assessing the dual impacts of operating
	and business strategies for the Canadian life and health (L&amp;H)
	insurance industry. This new model allows integration of the production
	performance and investment performance for the insurance companies
	and provides management overall performance evaluation and how to
	achieve efficiency systematically for the insurers involved. The
	results also show that the Canadian L&amp;H insurance industry operated
	fairly efficiently during the period examined (the year 1998). In
	addition, the scale efficiency in the Canadian L&amp;H insurance
	industry is found in this study. },
	doi = {http://dx.doi.org/10.1016/j.mcm.2005.12.011},
	issn = {0895-7177},
	keywords = {Data envelopment analysis},
	url = {http://www.sciencedirect.com/science/article/pii/S0895717705005388}
}


%*******************************************************************************

% GAUSSIAN

%==========================AAAAAAAAA===================================
@book{Anderson1979,
	title={Optimal filtering},
	author={Anderson, Brian D. O., and Moore, John B.},
	publisher={Prentice-Hall},
	year={1979}
}


%==========================CCC======================================
@ARTICLE{Carter1994,
	author = {Carter, C. K. and Kohn, R.},
	title = {On {Gibbs} Sampling for State Space Models},
	journal = {Biometrika},
	year = {1994},
	volume = {81},
	pages = {pp. 541-553},
	number = {3},
	abstract = {We show how to use the Gibbs sampler to carry out Bayesian inference
	on a linear state space model with errors that are a mixture of normals
	and coefficients that can switch over time. Our approach simultaneously
	generates the whole of the state vector given the mixture and coefficient
	indicator variables and simultaneously generates all the indicator
	variables conditional on the state vectors. The states are generated
	efficiently using the Kalman filter. We illustrate our approach by
	several examples and empirically compare its performance to another
	Gibbs sampler where the states are generated one at a time. The empirical
	results suggest that our approach is both practical to implement
	and dominates the Gibbs sampler that generates the states one at
	a time.},
	copyright = {Copyright Â© 1994 Biometrika Trust},
	issn = {00063444},
	jstor_articletype = {research-article},
	jstor_formatteddate = {Aug., 1994},
	language = {English},
	owner = {s4191549},
	publisher = {Biometrika Trust},
	timestamp = {2012.03.23},
	url = {http://www.jstor.org/stable/2337125}
}
@article{Christie1982,
	issn = "0304-405X",
	abstract = "This paper examines the relation between the variance of equity returns and several explanatory variables. It is found that equity variances have a strong positive association with both financial leverage and, contrary to the predictions of the options literature, interest rates. To a substantial degree, the negative elasticity of variance with respect to value of equity that is part of market folklore is found to be attributable to financial leverage. A maximum likehood estimator is developed for this elasticity that is substantially more efficient than extant estimation procedures.",
	journal = "Journal of Financial Economics",
	pages = "407--432",
	volume = "10",
	publisher = "Elsevier B.V.",
	number = "4",
	year = "1982",
	title = "The stochastic behavior of common stock variances: Value, leverage and interest rate effects",
	language = "eng",
	author = "Christie, Andrew A.",
	keywords = "Variances ; Variability ; Studies ; Stock Prices ; Securities Markets ; Mathematical Models ; Interest Rates ; Financial Leverage ; Economic Theory ; Common Stock ; Experimental/Theoretical Treatment ; Investment Analysis ; Economic Theory;",
}

%==========================DDD======================================

@ARTICLE{Durbin2002,
	author = {Durbin, J. and Koopman, S. J.},
	title = {A Simple and Efficient Simulation Smoother for State Space Time Series
	Analysis},
	journal = {Biometrika},
	year = {2002},
	volume = {89},
	pages = {pp. 603-615},
	number = {3},
	abstract = {A simulation smoother in state space time series analysis is a procedure
	for drawing samples from the conditional distribution of state or
	disturbance vectors given the observations. We present a new technique
	for this which is both simple and computationally efficient. The
	treatment includes models with diffuse initial conditions and regression
	effects. Computational comparisons are made with the previous standard
	method. Two applications are provided to illustrate the use of the
	simulation smoother for Gibbs sampling for Bayesian inference and
	importance sampling for classical inference.},
	copyright = {Copyright Â© 2002 Biometrika Trust},
	issn = {00063444},
	jstor_articletype = {research-article},
	jstor_formatteddate = {Sep., 2002},
	language = {English},
	owner = {s4191549},
	publisher = {Biometrika Trust},
	timestamp = {2012.02.15},
	url = {http://www.jstor.org/stable/4140605}
}
%==========================FFF======================================
@ARTICLE{Fruhwirth1994,
	author = {Fr\"{u}hwirth-Schnatter, Sylvia},
	title = {Bayesian Model Discrimination and Bayes Factors for Linear Gaussian
	State Space Models},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	year = {1995},
	volume = {57},
	pages = {pp. 237-246},
	number = {1},
	abstract = {It is shown how to discriminate between different linear Gaussian
	state space models for a given time series by means of a Bayesian
	approach which chooses the model that minimizes the expected loss.
	A practical implementation of this procedure requires a fully Bayesian
	analysis for both the state vector and the unknown hyperparameters
	and is carried out by Markov chain Monte Carlo methods. An application
	to some non-standard situations such as testing hypothesis on the
	boundary of the parameter space, discriminating non-nested models
	and discrimination of more than two models is discussed in detail.},
	copyright = {Copyright © 1995 Royal Statistical Society},
	issn = {00359246},
	jstor_formatteddate = {1995},
	language = {English},
	publisher = {Blackwell Publishing for the Royal Statistical Society},
	url = {http://www.jstor.org/stable/2346097}
}

%==========================GGG======================================
@article{Glosten1993,
	issn = "0022-1082",
	abstract = "We find support for a negative relation between conditional expected monthly return and conditional variance of monthly return, using a GARCH‐M model modified by allowing (1) seasonal patterns in volatility, (2) positive and negative innovations to returns having different impacts on conditional volatility, and (3) nominal interest rates to predict conditional variance. Using the modified GARCH‐M model, we also show that monthly conditional volatility may not be as persistent as was thought. Positive unanticipated returns appear to result in a downward revision of the conditional volatility whereas negative unanticipated returns result in an upward revision of conditional volatility.",
	journal = "Journal of Finance",
	pages = "1779--1801",
	volume = "48",
	publisher = "Blackwell Publishing Ltd",
	number = "5",
	year = "1993",
	title = "On the Relation between the Expected Value and the Volatility of the Nominal Excess Return on Stocks",
	address = "Oxford, UK",
	author = "Glosten, Lawrence R. and Jagannathan, Ravi and Runkle, David E.",
	keywords = "Us ; Volatility ; Time Series ; Studies ; Stocks ; Rates of Return ; Investment Policy ; Economic Models ; Econometrics ; Us ; Experimental/Theoretical Treatment ; Investment Analysis ; Economic Theory;",
	month = "December",
}
%==========================III=======================================
@ARTICLE{Ito2000, 
	author={Ito, K. and Kaiqi Xiong}, 
	journal={Automatic Control, IEEE Transactions on}, 
	title={Gaussian filters for nonlinear filtering problems}, 
	year={2000}, 
	month={May}, 
	volume={45}, 
	number={5}, 
	pages={910-927}, 
	keywords={Gaussian distribution;Kalman filters;filtering theory;Gaussian distributions;Gaussian filters;Kalman filter;nonlinear filtering;probability density;Bayesian methods;Cost function;Filtering;Filters;Gaussian distribution;Gaussian processes;Helium;Indium tin oxide;Sonar navigation;Testing}, 
	doi={10.1109/9.855552}, 
	ISSN={0018-9286},}

@article {Young2008,
	author = {Young, Andrew T. and Higgins, Matthew J. and Levy, Daniel},
	title = {{Sigma Convergence versus Beta Convergence: Evidence from U.S. county-Level Data}},
	journal = {Journal of Money, Credit and Banking},
	volume = {40},
	number = {5},
	publisher = {Blackwell Publishing Inc},
	issn = {1538-4616},
	url = {http://dx.doi.org/10.1111/j.1538-4616.2008.00148.x},
	doi = {10.1111/j.1538-4616.2008.00148.x},
	pages = {1083--1093},
	keywords = {O11, O18, O40, R11, σ-convergence, β-convergence, Solow growth model, speed of convergence, balanced growth, U.S. county-level data, income distribution, Gini coefficient, income equality},
	year = {2008},
}


%==========================JJJJJJJJJJ==================================
@ARTICLE{Joshua2009,
	author = {Chan,Joshua C. C. and Jeliazkov,Ivan},
	title = {Efficient simulation and integrated likelihood estimation in state
	space models},
	journal = {International Journal of Mathematical Modelling and Numerical Optimisation},
	year = {2009},
	volume = {1},
	pages = {101-120},
	number = {1-2},
	isbn = {2040-3607},
	keywords = {State smoothing; Banded matrix; Time-varying parameter model; Kalman
	filter; Dynamic factor model; Bayesian estimation; Collapsed sampler;
	Markov chain Monte Carlo; MCMC},
	language = {English},
	url = {www.summon.com}
}
@ARTICLE{deJong1995,
	author = {Piet de Jong and Neil Shephard},
	title = {The Simulation Smoother for Time Series Models},
	journal = {Biometrika},
	year = {1995},
	volume = {Vol. 82, No. 2},
	pages = {pp. 339-350},
	owner = {s4191549},
	timestamp = {2012.02.15},
	url = {http://www.jstor.org/stable/2337412}
}


@TechReport{Joshua2014,
	author={Chan, Joshua and Strachan, Rodney},
	title={{The zero lower bound: Implications for modelling the interest rate}},
	year=2014,
	institution={Research School of Economics, Australian National University},
	type={MPRA Paper},
	url={http://ideas.repec.org/p/pra/mprapa/39360.html},
	number={39360},
	abstract={In recent years state space models, particularly the linear Gaussian version, have become the standard framework for analyzing macro-economic and financial data. However, many theoretically motivated models imply non-linear or non-Gaussian specifications or both. Existing methods for estimating such models are computationally intensive, and often cannot be applied to models with more than a few states. Building upon recent developments in precision-based algorithms, we propose a general approach to estimating high-dimensional non-linear non-Gaussian state space models. The baseline algorithm approximates the conditional distribution of the states by a multivariate Gaussian or t density, which is then used for posterior simulation. We further develop this baseline algorithm to construct more sophisticated samplers with attractive properties: one based on the accept-reject Metropolis-Hastings (ARMH) algorithm, and another adaptive collapsed sampler inspired by the cross-entropy method. To illustrate the proposed approach, we investigate the effect of the zero lower bound of interest rate on monetary transmission mechanism.},
	keywords={integrated likelihood; accept-reject Metropolis-Hastings; cross-entropy; liquidity trap; zero lower },
}

%=================================== KKKK=====================================

@BOOK{Kroese2011,
	title = {Handbook of Monte Carlo},
	publisher = {John Wiley \& Sons Ltd},
	year = {2011},
	author = {Dirk P. Kroese, and Thomas Taimre, and Zdravko I. Botev},
	owner = {uqttrin2},
	timestamp = {2015.03.29}
}


@article{Koop2000,
	jstor_articletype = {research-article},
	title = {Modeling the Sources of Output Growth in a Panel of Countries},
	author = {Koop, Gary and Osiewalski, Jacek and Steel, Mark F. J.},
	journal = {Journal of Business & Economic Statistics},
	jstor_issuetitle = {},
	volume = {18},
	number = {3},
	jstor_formatteddate = {Jul., 2000},
	pages = {284-299},
	url = {http://www.jstor.org/stable/1392262},
	ISSN = {07350015},
	abstract = {This article seeks to improve understanding of cross-country patterns of economic growth. It adopts a stochastic production-frontier model that allows for the decomposition of output change into input, efficiency, and technical change. The production frontier is assumed to depend on effective inputs rather than measured inputs. We develop a model in which effective inputs depend on observed factor use and a correction term that depends on variables such as education. A further extension over related work is our use of a production frontier that varies over regional country groups. Empirical results indicate that both these extensions are very important.},
	language = {English},
	year = {2000},
	publisher = {Taylor & Francis, Ltd. on behalf of American Statistical Association},
	copyright = {Copyright © 2000 American Statistical Association},
}
%======LLLL




%======================== MMMMM======================================

@article{Matheron1973,
	jstor_articletype = {research-article},
	title = {The Intrinsic Random Functions and Their Applications},
	author = {Matheron, G.},
	journal = {Advances in Applied Probability},
	jstor_issuetitle = {},
	volume = {5},
	number = {3},
	jstor_formatteddate = {Dec., 1973},
	pages = {439-468},
	url = {http://www.jstor.org/stable/1425829},
	ISSN = {00018678},
	abstract = {The intrinsic random functions (IRF) are a particular case of the Guelfand generalized processes with stationary increments. They constitute a much wider class than the stationary RF, and are used in practical applications for representing non-stationary phenomena. The most important topics are: existence of a generalized covariance (GC) for which statistical inference is possible from a unique realization; theory of the best linear intrinsic estimator (BLIE) used for contouring and estimating problems; the turning bands method for simulating IRF; and the models with polynomial GC, for which statistical inference may be performed by automatic procedures.},
	language = {English},
	year = {1973},
	publisher = {Applied Probability Trust},
	copyright = {Copyright © 1973 Applied Probability Trust},
}

%==========================NNN==============================
@ARTICLE{Norgaard2000,
	author = {Magnus Norgaard and Niels K. Poulsen and Ole Ravn},
	title = {New developments in state estimation for nonlinear systems },
	journal = {Automatica },
	year = {2000},
	volume = {36},
	pages = {1627 - 1638},
	number = {11},
	doi = {http://dx.doi.org/10.1016/S0005-1098(00)00089-3},
	issn = {0005-1098},
	keywords = {State estimation},
	url = {http://www.sciencedirect.com/science/article/pii/S0005109800000893}
}
@article{Nelson1991,
	issn = "00129682",
	journal = "Econometrica : journal of the Econometric Society, an internat. society for the advancement of economic theory in its relation to statistics and mathematics",
	pages = "347--370",
	volume = "59",
	number = "2",
	year = "1991",
	title = "Conditional heteroskedasticity in asset returns a new approach",
	language = "eng",
	author = "Nelson, Daniel B",
	keywords = "Capm ; Schätztheorie ; Zeitreihenanalyse ; Arch-Modell ; Theorie ; Usa ; 1962-1987",
}

%==========================OOOOOOO=====================
%==============================PPP===============================
@BOOK{Poirier1995,
	title = {{Intermediate Statistics and Econometrics: A comparative approach}},
	publisher = {Cambridge: The MIT Press},
	year = {1995},
	author = {Poirier,D.},
	owner = {uqttrin2},
	timestamp = {2014.04.09}
}


%===================QQQQQQQQQQQQQQQ=======================

%===========================RRR==============================

@INPROCEEDINGS{Rasmussen2006,
	author = {Carl Edward Rasmussen},
	title = {Gaussian processes for machine learning},
	booktitle = {},
	year = {2006},
	publisher = {MIT Press}
}


@ARTICLE{Raftery1995,
	author = {Raftery, Adrian E.},
	title = {{Bayesian Model Selection in Social Research}},
	journal = {Sociological Methodology},
	year = {1995},
	volume = {25},
	pages = {111--163},
	
	doi = {10.2307/271063},
	issn = {00811750},
	keywords = {bayesian, bayesian-model-averaging, bic, model-selection},
	posted-at = {2010-12-01 18:49:12},
	priority = {0},
	publisher = {American Sociological Association},
	url = {http://dx.doi.org/10.2307/271063}
}

%==============================SSS==================================

@BOOK{Stein1999,
	title = {Interpolation of Spatial Data},
	publisher = {Springer-Verlag, New York},
	year = {1999},
	author = {Stein, M.L},
	owner = {uqttrin2},
	timestamp = {2014.12.13}}
@ARTICLE{Sorenson1974,
	author = {H.W. Sorenson},
	title = {On the development of practical nonlinear filters },
	journal = {Information Sciences },
	year = {1974},
	volume = {7},
	pages = {253 - 270},
	number = {0},
	doi = {http://dx.doi.org/10.1016/0020-0255(74)90017-6},
	issn = {0020-0255},
	url = {http://www.sciencedirect.com/science/article/pii/0020025574900176}
}

@ARTICLE{Sorenson1971,
	author = {H.W. Sorenson and D.L. Alspach},
	title = {Recursive bayesian estimation using gaussian sums },
	journal = {Automatica },
	year = {1971},
	volume = {7},
	pages = {465 - 479},
	number = {4},
	doi = {http://dx.doi.org/10.1016/0005-1098(71)90097-5},
	issn = {0005-1098},
	url = {http://www.sciencedirect.com/science/article/pii/0005109871900975}
}
@ARTICLE{Sorenson1972,
	author = {D. L. Alspach and H. W. Sorenson},
	title = {{Nonlinear Bayesian estimation using Gaussian sum approximations}},
	journal = {IEEE Transactions on Automatic Control},
	year = {1972},
	volume = {17},
	pages = {439--448},
	doi = {10.1109/TAC.1972.1100034},
	issue = {4},
	masid = {1580759}
}
@ARTICLE{Sylvia2010,
	author = {Sylvia Fr\"{u}hwirth-Schnatter and Helga Wagner},
	title = {Stochastic model specification search for Gaussian and partial non-Gaussian
	state space models },
	journal = {Journal of Econometrics },
	year = {2010},
	volume = {154},
	pages = {85 - 100},
	number = {1},
}

%=====================TTTTTTTTTTTTTTTTTT=================
@ARTICLE{Tsutsui2009,
	author = {Miki Tsutsui and Mika Goto},
	title = {{A multi-division efficiency evaluation of U.S. electric power companies
	using a weighted slacks-based measure }},
	journal = {Socio-Economic Planning Sciences },
	year = {2009},
	volume = {43},
	pages = {201-208},
	number = {3},
	doi = {http://dx.doi.org/10.1016/j.seps.2008.05.002},
	issn = {0038-0121},
	keywords = {Weighted slacks-based measure}
}


%=====================UUUUUUUUUUUUUUUUUUUU==================


%=====================VVVVVV===================================
@ARTICLE{Verdinelli1995,
	author = {Verdinelli, Isabella and Wasserman, Larry},
	title = {Computing Bayes Factors Using a Generalization of the Savage-Dickey
	Density Ratio},
	journal = {Journal of the American Statistical Association},
	year = {1995},
	volume = {90},
	pages = { 614-618},
	number = {430},
	abstract = {We present a simple method for computing Bayes factors. The method
	derives from observing that in general, a Bayes factor can be written
	as the product of a quantity called the Savage-Dickey density ratio
	and a correction factor; both terms are easily estimated from posterior
	simulation. In some cases it is possible to do these computations
	without ever evaluating the likelihood.},
	copyright = {Copyright © 1995 American Statistical Association},
	issn = {01621459},
	jstor_articletype = {research-article},
	jstor_formatteddate = {Jun., 1995},
	language = {English},
	publisher = {Taylor \& Francis, Ltd. on behalf of the American Statistical Association},
	url = {http://www.jstor.org/stable/2291073}
}


%=========================WWW===================================== 
@INPROCEEDINGS{Williams1996,
	author = {Christopher K. I. Williams and Carl Edward Rasmussen},
	title = {Gaussian Processes for Regression},
	booktitle = {Advances in Neural Information Processing Systems 8},
	year = {1996},
	pages = {514--520},
	publisher = {MIT press}
}

@book{West1997,
	author = {West, Mike and Harrison, Jeff},
	title = {Bayesian Forecasting and Dynamic Models (2Nd Ed.)},
	year = {1997},
	isbn = {0-387-94725-6},
	publisher = {Springer-Verlag New York, Inc.},
	address = {New York, NY, USA},
} 



%=========YYYYYYYYYYYYYYYYYYYYYY=====================


%=============ZZZZZZZZZZZZZZZZZZZZ======================


%******************************THIRD PAPER REFERENCES***************************

%===========AAA=========================================
@article{Anderson1951,
	author = "Anderson, T. W.",
	doi = "10.1214/aoms/1177729580",
	fjournal = "The Annals of Mathematical Statistics",
	journal = "Ann. Math. Statist.",
	month = "09",
	number = "3",
	pages = "327--351",
	publisher = "The Institute of Mathematical Statistics",
	title = "Estimating Linear Restrictions on Regression Coefficients for Multivariate Normal Distributions",
	url = "http://dx.doi.org/10.1214/aoms/1177729580",
	volume = "22",
	year = "1951"
}
@ARTICLE{Ardia2012,
	author = {David Ardia and Nalan Ba\v{s}t\"{u}rk and Lennart Hoogerheide and Herman
	K. van Dijk},
	title = {{A comparative study of Monte Carlo methods for efficient evaluation
	of marginal likelihood} },
	journal = {Computational Statistics \& Data Analysis },
	year = {2012},
	volume = {56},
	pages = {3398 - 3414},
	number = {11},
	note = {1st issue of the Annals of Computational and Financial EconometricsSixth
	Special Issue on Computational Econometrics },
	doi = {http://dx.doi.org/10.1016/j.csda.2010.09.001},
	issn = {0167-9473},
	keywords = {Marginal likelihood},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947310003440}
}

@ARTICLE{Adolfson2007,
	author = {Malin Adolfson and Jesper Lind\'{e} and Mattias Villani},
	title = {Forecasting Performance of an Open Economy DSGE Model},
	journal = {Econometric Reviews},
	year = {2007},
	volume = {26},
	pages = {289-328},
	number = {2-4},
	doi = {10.1080/07474930701220543},
	eprint = { http://dx.doi.org/10.1080/07474930701220543 },
	url = { http://dx.doi.org/10.1080/07474930701220543 
	}
}

%============BBB============================
@ARTICLE{Banbura2010,
	author = {Banbura, Marta and Giannone, Domenico and Reichlin, Lucrezia},
	title = {{Large Bayesian vector auto regressions}},
	journal = {Journal of Applied Econometrics},
	year = {2010},
	volume = {25},
	pages = {71--92},
	number = {1},
	doi = {10.1002/jae.1137},
	issn = {1099-1255},
	publisher = {John Wiley \& Sons, Ltd.},
	url = {http://dx.doi.org/10.1002/jae.1137}
}
@article{Bernanke2005,
	ISSN = {00335533, 15314650},
	URL = {http://www.jstor.org/stable/25098739},
	abstract = {Structural vector autoregressions (VARs) are widely used to trace out the effect of monetary policy innovations on the economy. However, the sparse information sets typically used in these empirical models lead to at least three potential problems with the results. First, to the extent that central banks and the private sector have information not reflected in the VAR, the measurement of policy innovations is likely to be contaminated. Second, the choice of a specific data series to represent a general economic concept such as "real activity" is often arbitrary to some degree. Third, impulse responses can be observed only for the included variables, which generally constitute only a small subset of the variables that the researcher and policy-maker care about. In this paper we investigate one potential solution to this limited information problem, which combines the standard structural VAR analysis with recent developments in factor analysis for large data sets. We find that the information that our factor-augmented VAR (FAVAR) methodology exploits is indeed important to properly identify the monetary transmission mechanism. Overall, our results provide a comprehensive and coherent picture of the effect of monetary policy on the economy.},
	author = {Ben S. Bernanke and Jean Boivin and Piotr Eliasz},
	journal = {The Quarterly Journal of Economics},
	number = {1},
	pages = {387-422},
	publisher = {Oxford University Press},
	title = {Measuring the Effects of Monetary Policy: A Factor-Augmented Vector Autoregressive (FAVAR) Approach},
	volume = {120},
	year = {2005}
}

@article{Bartlett1957,
	author = {Bartlett, M. S.}, 
	title = {A comment on D. V. Lindley's statistical paradox},
	volume = {44}, 
	number = {3-4}, 
	pages = {533-534}, 
	year = {1957}, 
	doi = {10.1093/biomet/44.3-4.533}, 
	URL = {http://biomet.oxfordjournals.org/content/44/3-4/533.short}, 
	eprint = {http://biomet.oxfordjournals.org/content/44/3-4/533.full.pdf+html}, 
	journal = {Biometrika} 
}

@article{Breslow1995,
	author = {Breslow, Norman E. and Lin, Xihong}, 
	title = {Bias correction in generalised linear mixed models with a single component of dispersion},
	volume = {82}, 
	number = {1}, 
	pages = {81-91}, 
	year = {1995}, 
	doi = {10.1093/biomet/82.1.81}, 
	abstract ={SUMMARY General expressions are derived for the asymptotic biases in three approximate estimators of regression coefficients and variance component, for small values of the variance component, in generalised linear mixed models with canonical link function and a single source of extraneous variation. The estimators involve first and second order Laplace expansions of the integrated likelihood and a related procedure known as penalised quasi-likelihood. Numerical studies of a series of matched pairs of binary outcomes show that the first order estimators of the variance component are seriously biased. Easily computed correction factors produce satisfactory estimators of small variance components, comparable to those obtained with a second order Laplace expansion, and markedly improve the asymptotic performance for larger values. For a series of matched pairs of binomial observations, the variance correction factors rapidly approach one as the binomial denominators increase. These results greatly extend the range of parameter values for which the approximate estimation procedures have satisfactory asymptotic properties.}, 
	URL = {http://biomet.oxfordjournals.org/content/82/1/81.abstract}, 
	eprint = {http://biomet.oxfordjournals.org/content/82/1/81.full.pdf+html}, 
	journal = {Biometrika} 
}
@BOOK{Bauwens2000,
	author = {Bauwens, Luc, Michel Lubrano, and Jean-Fran\c{c}ois Richard},
	title = {Bayesian Inference in Dynamic Econometric Models},
	year = {2000},
	publisher = {Oxford: Oxford University Press},
} 
@ARTICLE{Bai2002,
	author = {Bai, Jushan and Ng, Serena},
	title = {Determining the Number of Factors in Approximate Factor Models},
	journal = {Econometrica},
	year = {2002},
	volume = {70},
	pages = {191--221},
	number = {1},
	doi = {10.1111/1468-0262.00273},
	issn = {1468-0262},
	keywords = {factor analysis, asset pricing, principal components, model selection},
	publisher = {Blackwell Publishers Ltd},
	url = {http://dx.doi.org/10.1111/1468-0262.00273}
}
@ARTICLE{Bernanke2003,
	author = {Ben S. Bernanke and Jean Boivin},
	title = {Monetary policy in a data-rich environment },
	journal = {Journal of Monetary Economics },
	year = {2003},
	volume = {50},
	pages = {525 - 546},
	number = {3},
	url = {http://www.sciencedirect.com/science/article/pii/S0304393203000242}
}
@ARTICLE{Bauwens1985,
	author = {Luc Bauwens and Jean-Fran\c{c}ois Richard},
	title = {A 1-1 poly-t random variable generator with application to Monte
	Carlo integration},
	journal = {Journal of Econometrics},
	year = {1985},
	volume = {29},
	pages = {19 - 46},
	number = {1},
	doi = {http://dx.doi.org/10.1016/0304-4076(85)90031-4},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/0304407685900314}
}
@TechReport{Bauwens1994, 
	author={Bauwens, Luc and Lubrano , Michel},
	title={{Identification Restrictions and Posterior Densities in Cointegrated Gaussian VAR Systems}},
	year=1994,
	month=4,
	institution={Université catholique de Louvain, Center for Operations Research and Econometrics (CORE)},
	type={CORE Discussion Papers},
	url={https://ideas.repec.org/p/cor/louvco/1994018.html},
	number={1994018},
	abstract={We derive the postenor density of the cointegrating coetficients in a Gaussian VAR system. The density does not belong in general to a family of densities with known properties. If there is one cointegrating vector, the density belongs to the class of poly-t densities. It is integrable if the coefficients are identified and it has finite moments to the order of overidentification. The identifying restrictions we consider are linear restrictions on the cointegrating vectors. The structure or the posterior density is exploited to implement Monte Carlo integTi\tion nwthods that are needed when there is more than one cointegrating veetor. The paper contains two empirical illustrations.},
	keywords={},
	doi={},
}
@ARTICLE{Bauwens1997,
	author = {L Bauwens and P. Giot},
	title = {A Gibbs Sampling Approach To Cointegration},
	journal = {Computational Statistics},
	year = {1997},
	volume = {13},
	pages = {339--368}
}

@article {Bai2002,
	author = {Bai, Jushan and Ng, Serena},
	title = {Determining the Number of Factors in Approximate Factor Models},
	journal = {Econometrica},
	volume = {70},
	number = {1},
	publisher = {Blackwell Publishers Ltd},
	issn = {1468-0262},
	url = {http://dx.doi.org/10.1111/1468-0262.00273},
	doi = {10.1111/1468-0262.00273},
	pages = {191--221},
	keywords = {factor analysis, asset pricing, principal components, model selection},
	year = {2002},
}

%============CCC===============================
@BOOK{Chikuse2003,
	title = {{Statistics on special manifolds}},
	publisher = {Springer},
	year = {2003},
	author = {Chikuse, Yasuko},
	edition = {2003},
	isbn = {0387001603},
	posted-at = {2013-02-14 01:50:07},
	priority = {2},
	url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0387001603}
}
@inproceedings{Chickering1996,
	author = {Chickering, David Maxwell and Heckerman, David},
	title = {Efficient Approximations for the Marginal Likelihood of Incomplete Data Given a {Bayesian} Network},
	booktitle = {Proceedings of the Twelfth International Conference on Uncertainty in Artificial Intelligence},
	series = {UAI'96},
	year = {1996},
	isbn = {1-55860-412-X},
	location = {Portland, OR},
	pages = {158-168},
	numpages = {11},
	url = {http://dl.acm.org/citation.cfm?id=2074284.2074303},
	acmid = {2074303},
	publisher = {Morgan Kaufmann Publishers Inc.},
	address = {San Francisco, CA, USA},
} 
@Article{Cragg1997,
	Title                    = {Inferring the rank of a matrix },
	Author                   = {John G. Cragg and Stephen G. Donald},
	Journal                  = {Journal of Econometrics },
	Year                     = {1997},
	Number                   = {1â€“2},
	Pages                    = {223 - 250},
	Volume                   = {76},
	
	Doi                      = {http://dx.doi.org/10.1016/0304-4076(95)01790-9},
	ISSN                     = {0304-4076},
	Keywords                 = {Rank of matrix tests},
	Url                      = {http://www.sciencedirect.com/science/article/pii/0304407695017909}
}

@article{Camba2003,
	ISSN = {07350015},
	URL = {http://www.jstor.org/stable/1392359},
	abstract = {There has recently been renewed research interest in the development of tests of the rank of a matrix. This article evaluates the performance of some asymptotic tests of rank determination in reduced rank regression models together with bootstrapped versions through simulation experiments. The bootstrapped procedures significantly improve on the performance of the corresponding asymptotic tests. The article also presents a Monte Carlo exercise comparing the forecasting performance of reduced rank and unrestricted vector autoregressive (VAR) models in which the former appear superior. The tests of rank considered here are then applied to construct reduced rank VAR models for leading indicators of U.K. economic activity. These more parsimonious multivariate representations display an improvement in forecasting performance over that of unrestricted VAR models.},
	author = { Camba-Mendez,Gonzalo, and Kapetanios,George, and Smith, Richard J. , and   Weale, Martin R.},
	journal = {Journal of Business & Economic Statistics},
	number = {1},
	pages = {145-155},
	publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
	title = {Tests of Rank in Reduced Rank Regression Models},
	volume = {21},
	year = {2003}
}


@ARTICLE{Carriero2011,
	author = {Carriero, Andrea and Kapetanios, George and Marcellino, Massimiliano},
	title = {{Forecasting large datasets with Bayesian reduced rank multivariate
	models}},
	journal = {Journal of Applied Econometrics},
	year = {2011},
	volume = {26},
	pages = {735--761},
	number = {5},
	doi = {10.1002/jae.1150},
	issn = {1099-1255},
	publisher = {John Wiley \& Sons, Ltd.},
	url = {http://dx.doi.org/10.1002/jae.1150}
}
@ARTICLE{Carrierio2016a,
	author = {Andrea Carriero and Todd E. Clark and Massimiliano Marcellino},
	title = {{Common drifting volatility in large Bayesian VARs}},
	journal = {Journal of Business \& Economic Statistics},
	year = {2016},
	volume = {34},
	pages = {375-390},
	number = {3},
	abstract = { The general pattern of estimated volatilities of macroeconomic and
	financial variables is often broadly similar. We propose two models
	in which conditional volatilities feature comovement and study them
	using U.S. macroeconomic data. The first model specifies the conditional
	volatilities as driven by a single common unobserved factor, plus
	an idiosyncratic component. We label this model BVAR with general
	factor stochastic volatility (BVAR-GFSV) and we show that the loss
	in terms of marginal likelihood from assuming a common factor for
	volatility is moderate. The second model, which we label BVAR with
	common stochastic volatility (BVAR-CSV), is a special case of the
	BVAR-GFSV in which the idiosyncratic component is eliminated and
	the loadings to the factor are set to 1 for all the conditional volatilities.
	Such restrictions permit a convenient Kronecker structure for the
	posterior variance of the VAR coefficients, which in turn permits
	estimating the model even with large datasets. While perhaps misspecified,
	the BVAR-CSV model is strongly supported by the data when compared
	against standard homoscedastic BVARs, and it can produce relatively
	good point and density forecasts by taking advantage of the information
	contained in large datasets. },
	doi = {10.1080/07350015.2015.1040116},
	eprint = { http://dx.doi.org/10.1080/07350015.2015.1040116 },
	url = { http://dx.doi.org/10.1080/07350015.2015.1040116 
	}
}

@ARTICLE{Carriero2016,
	author = {Andrea Carriero and George Kapetanios and Massimiliano Marcellino},
	title = {{Structural analysis with Multivariate Autoregressive Index models}
	},
	journal = {Journal of Econometrics },
	year = {2016},
	volume = {192},
	pages = {332 - 348},
	number = {2},
	doi = {http://dx.doi.org/10.1016/j.jeconom.2016.02.002},
	issn = {0304-4076},
	keywords = {Large datasets},
	url = {http://www.sciencedirect.com/science/article/pii/S0304407616300057}
}

@ARTICLE{Chib1995,
	author = {Siddhartha Chib},
	title = {Marginal Likelihood from the Gibbs Output},
	journal = {Journal of the American Statistical Association},
	year = {1995},
	volume = {90},
	pages = {1313-1321},
	number = {432},
	doi = {10.1080/01621459.1995.10476635},
	eprint = { http://www.tandfonline.com/doi/pdf/10.1080/01621459.1995.10476635
	},
	url = { http://www.tandfonline.com/doi/abs/10.1080/01621459.1995.10476635 
	}
}
@article{Chib2001,
	ISSN = {01621459},
	URL = {http://www.jstor.org/stable/2670365},
	abstract = {This article provides a framework for estimating the marginal likelihood for the purpose of Bayesian model comparisons. The approach extends and completes the method presented in Chib (1995) by overcoming the problems associated with the presence of intractable full conditional densities. The proposed method is developed in the context of MCMC chains produced by the Metropolis-Hastings algorithm, whose building blocks are used both for sampling and marginal likelihood estimation, thus economizing on prerun tuning effort and programming. Experiments involving the logit model for binary data, hierarchical random effects model for clustered Gaussian data, Poisson regression model for clustered count data, and the multivariate probit model for correlated binary data, are used to illustrate the performance and implementation of the method. These examples demonstrate that the method is practical and widely applicable.},
	author = {Siddhartha Chib, Ivan Jeliazkov},
	journal = {Journal of the American Statistical Association},
	number = {453},
	pages = {270-281},
	publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
	title = {Marginal Likelihood from the Metropolis-Hastings Output},
	volume = {96},
	year = {2001}
}


%============DDD===============================
@ARTICLE{Doan1984,
	author = {Thomas Doan and Robert Litterman and Christopher Sims},
	title = {Forecasting and conditional projection using realistic prior distributions},
	journal = {Econometric Reviews},
	year = {1984},
	volume = {3},
	pages = {1-100},
	number = {1},
	doi = {10.1080/07474938408800053},
	eprint = { http://dx.doi.org/10.1080/07474938408800053 },
	url = { http://dx.doi.org/10.1080/07474938408800053 
	}
}

@ARTICLE{DeMol2008,
	author = {Christine, De Mol and Domenico Giannone and Lucrezia Reichlin},
	title = {Forecasting using a large number of predictors: Is Bayesian shrinkage
	a valid alternative to principal components? },
	journal = {Journal of Econometrics },
	year = {2008},
	volume = {146},
	pages = {318 - 328},
	number = {2},
	issn = {0304-4076},
	keywords = {Bayesian shrinkage},
	url = {http://www.sciencedirect.com/science/article/pii/S0304407608001103}
}

@ARTICLE{Diebold1995,
	author = {Francis X. Diebold and Roberto S. Mariano},
	title = {Comparing Predictive Accuracy},
	journal = {Journal of Business \& Economic Statistics},
	year = {1995},
	volume = {13},
	pages = {253-263},
	number = {3},
	doi = {10.1080/07350015.1995.10524599},
	eprint = { http://amstat.tandfonline.com/doi/pdf/10.1080/07350015.1995.10524599
	},
	url = { http://amstat.tandfonline.com/doi/abs/10.1080/07350015.1995.10524599 
	}
}

%===========EEE===================================

@article{Engle1993,
	ISSN = {07350015},
	URL = {http://www.jstor.org/stable/1391623},
	abstract = {This article introduces a class of statistical tests for the hypothesis that some feature that is present in each of several variables is common to them. Features are data properties such as serial correlation, trends, seasonality, heteroscedasticity, autoregressive conditional heteroscedasticity, and excess kurtosis. A feature is detected by a hypothesis test taking no feature as the null, and a common feature is detected by a test that finds linear combinations of variables with no feature. Often, an exact asymptotic critical value can be obtained that is simply a test of overidentifying restrictions in an instrumental variable regression. This article tests for a common international business cycle.},
	author = { Engle,Robert F., and  Kozicki, Sharon},
	journal = {Journal of Business & Economic Statistics},
	number = {4},
	pages = {369-380},
	publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
	title = {Testing for Common Features},
	volume = {11},
	year = {1993}
}




%============FFF=================================
@article{James1954,
	ISSN = {00034851},
	URL = {http://www.jstor.org/stable/2236512},
	abstract = {New methods are introduced for deriving the sampling distributions of statistics obtained from a normal multivariate population. Exterior differential forms are used to represent the invariant measures on the orthogonal group and the Grassmann and Stiefel manifolds. The first part is devoted to a mathematical exposition of these. In the second part, the theory is applied; first, to the derivation of the distribution of the canonical correlation coefficients when the corresponding population parameters are zero; and secondly, to split the distribution of a normal multivariate sample into three independent distributions, (a) essentially the Wishart distribution, (b) the invariant distribution of a random plane which is given by the invariant measure on the Grassmann manifold, (c) the invariant distribution of a random orthogonal matrix. This decomposition provides derivations of the Wishart distribution and of the distribution of the latent roots of the sample variance covariance matrix when the population roots are equal.},
	author = {A. T. James},
	journal = {The Annals of Mathematical Statistics},
	number = {1},
	pages = {40-75},
	publisher = {Institute of Mathematical Statistics},
	title = {Normal Multivariate Analysis and the Orthogonal Group},
	volume = {25},
	year = {1954}
}

@article{Forni2000,
	ISSN = {00346535, 15309142},
	URL = {http://www.jstor.org/stable/2646650},
	abstract = {This paper proposes a factor model with infinite dynamics and nonorthogonal idiosyncratic components. The model, which we call the generalized dynamic-factor model, is novel to the literature and generalizes the static approximate factor model of Chamberlain and Rothschild (1983), as well as the exact factor model à la Sargent and Sims (1977). We provide identification conditions, propose an estimator of the common components, prove convergence as both time and cross-sectional size go to infinity at appropriate rates, and present simulation results. We use our model to construct a coincident index for the European Union. Such index is defined as the common component of real GDP within a model including several macroeconomic variables for each European country.},
	author = {Mario Forni and Marc Hallin and Marco Lippi and Lucrezia Reichlin},
	journal = {The Review of Economics and Statistics},
	number = {4},
	pages = {540-554},
	publisher = {The MIT Press},
	title = {The Generalized Dynamic-Factor Model: Identification and Estimation},
	volume = {82},
	year = {2000}
}


@ARTICLE{Forni2003,
	author = {Mario Forni and Marc Hallin and Marco Lippi and Lucrezia Reichlin},
	title = {Do financial variables help forecasting inflation and real activity
	in the {Euro} area? },
	journal = {Journal of Monetary Economics },
	year = {2003},
	volume = {50},
	pages = {1243 - 1255},
	number = {6},
	doi = {http://dx.doi.org/10.1016/S0304-3932(03)00079-5},
	issn = {0304-3932},
	keywords = {Dynamic factor models},
	url = {http://www.sciencedirect.com/science/article/pii/S0304393203000795}
}

@article{Friel2008,
	ISSN = {13697412, 14679868},
	URL = {http://www.jstor.org/stable/20203843},
	abstract = {Model choice plays an increasingly important role in statistics. From a Bayesian perspective a crucial goal is to compute the marginal likelihood of the data for a given model. However, this is typically a difficult task since it amounts to integrating over all model parameters. The aim of the paper is to illustrate how this may be achieved by using ideas from thermodynamic integration or path sampling. We show how the marginal likelihood can be computed via Markov chain Monte Carlo methods on modified posterior distributions for each model. This then allows Bayes factors or posterior model probabilities to be calculated. We show that this approach requires very little tuning and is straightforward to implement. The new method is illustrated in a variety of challenging statistical settings.},
	author = {N. Friel and A. N. Pettitt},
	journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
	number = {3},
	pages = {589-607},
	publisher = {[Royal Statistical Society, Wiley]},
	title = {Marginal Likelihood Estimation via Power Posteriors},
	volume = {70},
	year = {2008}
}


@ARTICLE{Fu1998,
	author = {Wenjiang J. Fu},
	title = {Penalized Regressions: The Bridge versus the Lasso},
	journal = {Journal of Computational and Graphical Statistics},
	year = {1998},
	volume = {7},
	pages = {397-416},
	number = {3},
	doi = {10.1080/10618600.1998.10474784},
	eprint = { http://www.tandfonline.com/doi/pdf/10.1080/10618600.1998.10474784
	},
	url = { http://www.tandfonline.com/doi/abs/10.1080/10618600.1998.10474784 
	}
}

%============GGG==================================
@ARTICLE{Geweke1996,
	author = {John Geweke},
	title = {Bayesian reduced rank regression in econometrics },
	journal = {Journal of Econometrics },
	year = {1996},
	volume = {75},
	pages = {121 - 146},
	number = {1},
	doi = {http://dx.doi.org/10.1016/0304-4076(95)01773-9},
	issn = {0304-4076},
	keywords = {Factor model},
	url = {http://www.sciencedirect.com/science/article/pii/0304407695017739}
}

@TechReport{Geweke1994,
	author={John F. Geweke},
	title={{Bayesian comparison of econometric models}},
	year=1994,
	institution={Federal Reserve Bank of Minneapolis},
	type={Working Papers},
	url={https://ideas.repec.org/p/fip/fedmwp/532.html},
	number={532},
	abstract={No abstract is available for this item.},
	keywords={Econometric models},
	doi={},
}
@ARTICLE{Geweke2001,
	author = {John Geweke},
	title = {Bayesian econometrics and forecasting },
	journal = {Journal of Econometrics },
	year = {2001},
	volume = {100},
	pages = {11-15},
	number = {1},
	doi = {http://dx.doi.org/10.1016/S0304-4076(00)00046-4},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/S0304407600000464}
}


@ARTICLE{Geweke1989,
	author = {John Geweke},
	title = {Exact predictive densities for linear models with {ARCH} disturbances},
	journal = {Journal of Econometrics},
	year = {1989},
	volume = {40},
	pages = {63 - 86},
	number = {1},
	doi = {http://dx.doi.org/10.1016/0304-4076(89)90030-4},
	issn = {0304-4076},
}

@ARTICLE{Gelfand1990,
	author = {Alan E. Gelfand and Adrian F. M. Smith},
	title = {Sampling-Based Approaches to Calculating Marginal Densities},
	journal = {Journal of the American Statistical Association},
	year = {1990},
	volume = {85},
	pages = {398-409},
	number = {410},
	doi = {10.1080/01621459.1990.10476213},
	eprint = { http://www.tandfonline.com/doi/pdf/10.1080/01621459.1990.10476213
	},
	url = { http://www.tandfonline.com/doi/abs/10.1080/01621459.1990.10476213 
	}
}


@ARTICLE{Guhaniyogi2015,
	author = {Rajarshi Guhaniyogi and David B. Dunson},
	title = {Bayesian Compressed Regression},
	journal = {Journal of the American Statistical Association},
	year = {2015},
	volume = {110},
	pages = {1500-1514},
	number = {512},
	doi = {10.1080/01621459.2014.969425},
	eprint = { http://dx.doi.org/10.1080/01621459.2014.969425 },
	url = { http://dx.doi.org/10.1080/01621459.2014.969425 
	}
}

@article{Gelfand1994,
	ISSN = {00359246},
	URL = {http://www.jstor.org/stable/2346123},
	abstract = {Model determination is a fundamental data analytic task. Here we consider the problem of choosing among a finite (without loss of generality we assume two) set of models. After briefly reviewing classical and Bayesian model choice strategies we present a general predictive density which includes all proposed Bayesian approaches that we are aware of. Using Laplace approximations we can conveniently assess and compare the asymptotic behaviour of these approaches. Concern regarding the accuracy of these approximations for small to moderate sample sizes encourages the use of Monte Carlo techniques to carry out exact calculations. A data set fitted with nested non-linear models enables comparisons between proposals and between exact and asymptotic values.},
	author = {A. E. Gelfand and D. K. Dey},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	number = {3},
	pages = {501-514},
	publisher = {[Royal Statistical Society, Wiley]},
	title = {Bayesian Model Choice: Asymptotics and Exact Calculations},
	volume = {56},
	year = {1994}
}


@BOOK{Gupta2000,
	title = {{Matrix variate distribution}},
	publisher = {Chapman \& Hall},
	year = {2000},
	author = {Gupta, A.K., and Nagar, D.K.}
}

@ARTICLE{Geweke2010,
	author = {John Geweke and Gianni Amisano},
	title = {Comparing and evaluating Bayesian predictive distributions of asset
	returns },
	journal = {International Journal of Forecasting },
	year = {2010},
	volume = {26},
	pages = {216 - 230},
	number = {2},
	doi = {http://dx.doi.org/10.1016/j.ijforecast.2009.10.007},
	issn = {0169-2070},
	keywords = {Forecasting},
	url = {http://www.sciencedirect.com/science/article/pii/S0169207009001757}
}
@article{Giannone2015,
	author={Domenico Giannone and Michele Lenza and Giorgio E. Primiceri},
	title={{Prior Selection for Vector Autoregressions}},
	journal = {Review of Economics and Statistics},
	year = {2015},
	volume = {97},
	pages = {436-451},
	number = {2},
}
@article{Gill1992,
	ISSN = {01621459},
	URL = {http://www.jstor.org/stable/2290214},
	author = {Gill,Len , and Lewbel, Arthur },
	journal = {Journal of the American Statistical Association},
	number = {419},
	pages = {766-776},
	publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
	title = {Testing the Rank and Definiteness of Estimated Matrices With Applications to Factor, State-Space and {ARMA} Models},
	volume = {87},
	year = {1992}
}



@ARTICLE{George2008,
	author = {Edward I. George and Dongchu Sun and Shawn Ni},
	title = {Bayesian stochastic search for {VAR} model restrictions },
	journal = {Journal of Econometrics },
	year = {2008},
	volume = {142},
	pages = {553 - 580},
	number = {1},
	doi = {http://dx.doi.org/10.1016/j.jeconom.2007.08.017},
	issn = {0304-4076},
	keywords = {Bayesian VAR},
	url = {http://www.sciencedirect.com/science/article/pii/S0304407607001753}
}
@ARTICLE{Ghosh2009,
	author = {Joyee Ghosh and David B. Dunson},
	title = {Default Prior Distributions and Efficient Posterior Computation in
	Bayesian Factor Analysis},
	journal = {Journal of Computational and Graphical Statistics},
	year = {2009},
	volume = {18},
	pages = {306-320},
	number = {2},
	doi = {10.1198/jcgs.2009.07145},
	eprint = { http://dx.doi.org/10.1198/jcgs.2009.07145 },
	url = { http://dx.doi.org/10.1198/jcgs.2009.07145 
	}
}

%==============HHH===========================================
@ARTICLE{Harvey1997,
	author = {David Harvey and Stephen Leybourne and Paul Newbold},
	title = {Testing the equality of prediction mean squared errors},
	journal = {International Journal of Forecasting},
	year = {1997},
	volume = {13},
	pages = {281 - 291},
	number = {2},
	doi = {http://dx.doi.org/10.1016/S0169-2070(96)00719-4},
	issn = {0169-2070},
	keywords = {Comparing forecasts},
	url = {http://www.sciencedirect.com/science/article/pii/S0169207096007194}
}

@BOOK{Harville1997,
	title = {Matrix Algebra From a Statistician's Perspective},
	publisher = {Springer},
	year = {1997},
	author = {Harville, David},
	isbn = {0387001603},
	posted-at = {2013-02-14 01:50:07},
	priority = {2},
	url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0387001603}
}

@article{Hobert1996,
	ISSN = {01621459},
	URL = {http://www.jstor.org/stable/2291572},
	abstract = {Often, either from a lack of prior information or simply for convenience, variance components are modeled with improper priors in hierarchical linear mixed models. Although the posterior distributions for these models are rarely available in closed form, the usual conjugate structure of the prior specification allows for painless calculation of the Gibbs conditionals. Thus the Gibbs sampler may be used to explore the posterior distribution without ever having established propriety of the posterior. An example is given showing that the output from a Gibbs chain corresponding to an improper posterior may appear perfectly reasonable. Thus one cannot expect the Gibbs output to provide a "red flag," informing the user that the posterior is improper. The user must demonstrate propriety before a Markov chain Monte Carlo technique is used. A theorem is given that classifies improper priors according to the propriety of the resulting posteriors. Applications concerning Bayesian analysis of animal breeding data and the location of maxima of unwieldy (restricted) likelihood functions are discussed. Gibbs sampling with improper posteriors is then considered in more generality. The concept of functional compatibility of conditional densities is introduced and is used to construct an invariant measure for a class of Markov chains. These results are used to show that Gibbs chains corresponding to improper posteriors are, in theory, quite ill-behaved.},
	author = {James P. Hobert, and George Casella},
	journal = {Journal of the American Statistical Association},
	number = {436},
	pages = {1461-1473},
	publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
	title = {The Effect of Improper Priors on Gibbs Sampling in Hierarchical Linear Mixed Models},
	volume = {91},
	year = {1996}
}

@ARTICLE{Hoerl1970,
	author = {Arthur E. Hoerl and Robert W. Kennard},
	title = {Ridge Regression: Biased Estimation for Nonorthogonal Problems},
	journal = {Technometrics},
	year = {1970},
	volume = {12},
	pages = {55-67},
	number = {1},
	doi = {10.1080/00401706.1970.10488634},
	eprint = { http://www.tandfonline.com/doi/pdf/10.1080/00401706.1970.10488634
	},
	url = { http://www.tandfonline.com/doi/abs/10.1080/00401706.1970.10488634 
	}
}
@article{Hall2004,
	ISSN = {01621459},
	URL = {http://www.jstor.org/stable/27590481},
	author = {Peter Hall and Jeff Racine and Qi Li},
	journal = {Journal of the American Statistical Association},
	number = {468},
	pages = {1015-1026},
	publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
	title = {Cross-Validation and the Estimation of Conditional Probability Densities},
	volume = {99},
	year = {2004}
}


@article{Hall1991,
	ISSN = {00359246},
	URL = {http://www.jstor.org/stable/2345739},
	abstract = {The method of least squares cross-validation for choosing the bandwidth of a kernel density estimator has been the object of considerable research, through both theoretical analysis and simulation studies. The method involves the minimization of a certain function of the bandwidth. One of the less attractive features of this method, which has been observed in simulation studies but has not previously been understood theoretically, is that rather often the cross-validation function has multiple local minima. The theoretical results of this paper provide an explanation and quantification of this empirical observation, through modelling the cross-validation function as a Gaussian stochastic process. Asymptotic analysis reveals that the degree of wiggliness of the cross-validation function depends on the underlying density through a fairly simple functional, but dependence on the kernel function is much more complicated. A simulation study explores the extent to which the asymptotic analysis describes the actual situation. Our techniques may also be used to obtain other related results--e.g. to show that spurious local minima of the cross-validation function are more likely to occur at too small values of the bandwidth, rather than at too large values.},
	author = {Peter Hall and J. S. Marron},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	number = {1},
	pages = {245-252},
	publisher = {[Royal Statistical Society, Wiley]},
	title = {Local Minima in Cross-Validation Functions},
	volume = {53},
	year = {1991}
}



@ARTICLE{Hoogerheide2007,
	author = {Lennart F. Hoogerheide and Johan F. Kaashoek and Herman K. van Dijk},
	title = {On the shape of posterior densities and credible sets in instrumental
	variable regression models with reduced rank: An application of flexible
	sampling methods using neural networks },
	journal = {Journal of Econometrics },
	year = {2007},
	volume = {139},
	pages = {154 - 180},
	number = {1},
	note = {Endogeneity, instruments and identification },
	abstract = {Likelihoods and posteriors of instrumental variable (IV) regression
	models with strong endogeneity and/or weak instruments may exhibit
	rather non-elliptical contours in the parameter space. This may seriously
	affect inference based on Bayesian credible sets. When approximating
	posterior probabilities and marginal densities using Monte Carlo
	integration methods like importance sampling or Markov chain Monte
	Carlo procedures the speed of the algorithm and the quality of the
	results greatly depend on the choice of the importance or candidate
	density. Such a density has to be â€˜closeâ€™ to the target density
	in order to yield accurate results with numerically efficient sampling.
	For this purpose we introduce neural networks which seem to be natural
	importance or candidate densities, as they have a universal approximation
	property and are easy to sample from. A key step in the proposed
	class of methods is the construction of a neural network that approximates
	the target density. The methods are tested on a set of illustrative
	\{IV\} regression models. The results indicate the possible usefulness
	of the neural network approach. },
	doi = {http://dx.doi.org/10.1016/j.jeconom.2006.06.009},
	issn = {0304-4076},
	keywords = {Instrumental variables},
	url = {http://www.sciencedirect.com/science/article/pii/S0304407606001072}
}

%================KKK==========================================
@article{Kloek1978,
	ISSN = {00129682, 14680262},
	URL = {http://www.jstor.org/stable/1913641},
	abstract = {Monte Carlo (MC) is used to draw parameter values from a distribution defined on the structural parameter space of an equation system. Making use of the prior density, the likelihood, and Bayes' Theorem it is possible to estimate posterior moments of both structural and reduced form parameters. The MC method allows a rather liberal choice of prior distributions. The number of elementary operations to be preformed need not be an explosive function of the number of parameters involved. The method overcomes some existing difficulties of applying Bayesian methods to medium size models. The method is applied to a small scale macro model. The prior information used stems from considerations regarding short and long run behavior of the model and form extraneous observations on empirical long term ratios of economic variables. Likelihood contours for several parameter combinations are plotted, and some marginal posterior densities are assessed by MC.},
	author = {T. Kloek, and H. K. van Dijk},
	journal = {Econometrica},
	number = {1},
	pages = {1-19},
	publisher = {[Wiley, Econometric Society]},
	title = {Bayesian Estimates of Equation System Parameters: An Application of Integration by Monte Carlo},
	volume = {46},
	year = {1978}
}


@TechReport{Kaufman2013,
	author={Sylvia Kaufmann and Christian Schumacher},
	title={{Bayesian estimation of sparse dynamic factor models with order-independent identification}},
	year=2013,
	month=4,
	institution={Swiss National Bank, Study Center Gerzensee},
	type={Working Papers},
	url={https://ideas.repec.org/p/szg/worpap/1304.html},
	number={13.04},
	abstract={The analysis of large panel data sets (with N variables) involves methods of dimension reduction and optimal information extraction. Dimension reduction is usually achieved by extracting the common variation in the data into few factors (k, where k},
	keywords={},
	doi={},
}

@ARTICLE{Kleibergen2002,
	author = {Frank Kleibergen and Richard Paap},
	title = {Priors, posteriors and bayes factors for a Bayesian analysis of cointegration
	},
	journal = {Journal of Econometrics },
	year = {2002},
	volume = {111},
	pages = {223 - 249},
	number = {2},
	note = {Finite Sample and Asymptotic Methods in Econometrics },
	doi = {http://dx.doi.org/10.1016/S0304-4076(02)00105-7},
	issn = {0304-4076},
	keywords = {Cointegration},
	url = {http://www.sciencedirect.com/science/article/pii/S0304407602001057}
}
@article{Kleibergen1998,
	ISSN = {02664666, 14694360},
	URL = {http://www.jstor.org/stable/3533088},
	abstract = {Diffuse priors lead to pathological posterior behavior when used in Bayesian analyses of simultaneous equation models (SEM's). This results from the local nonidentification of certain parameters in SEM's. When this a priori known feature is not captured appropriately, it results in an a posteriori favoring of certain specific parameter values that is not the consequence of strong data information but of local nonidentification. We show that a proper consistent Bayesian analysis of a SEM explicitly has to consider the reduced form of the SEM as a standard linear model on which nonlinear (reduced rank) restrictions are imposed, which result from a singular value decomposition. The priors/posteriors of the parameters of the SEM are therefore proportional to the priors/posteriors of the parameters of the linear model under the condition that the restrictions hold. This leads to a framework for constructing priors and posteriors for the parameters of SEM's. The framework is used to construct priors and posteriors for one, two, and three structural equation SEM's. These examples together with a theorem, showing that the reduced forms of SEM's accord with sets of reduced rank restrictions on standard linear models, show how Bayesian analyses of generally specified SEM's can be conducted.},
	author = {Frank Kleibergen, and Herman K. van Dijk},
	journal = {Econometric Theory},
	number = {6},
	pages = {701-743},
	publisher = {Cambridge University Press},
	title = {Bayesian Simultaneous Equations Analysis Using Reduced Rank Structures},
	volume = {14},
	year = {1998}
}

@ARTICLE{Koop2004,
	author = {Koop, Gary and Potter, Simon},
	title = {Forecasting in dynamic factor models using Bayesian model averaging},
	journal = {Econometrics Journal},
	year = {2004},
	volume = {7},
	pages = {550--565},
	number = {2},
	doi = {10.1111/j.1368-423X.2004.00143.x},
	issn = {1368-423X},
	keywords = {Bayesian model averaging, Diffusion index, Markov chain Monte Carlo
	Model Composition, Reference prior},
	publisher = {Blackwell Publishing Ltd.},
	url = {http://dx.doi.org/10.1111/j.1368-423X.2004.00143.x}
}

@ARTICLE{Koop2013,
	author = {Koop, Gary M.},
	title = {Forecasting with Medium and Large {Bayesian VARS}},
	journal = {Journal of Applied Econometrics},  
	year = {2013},
	volume = {28},
	pages = {177--203},
	number = {2},
	doi = {10.1002/jae.1270},
	issn = {1099-1255},
	url = {http://dx.doi.org/10.1002/jae.1270}
}

@ARTICLE{Koop2010,
	author = {Gary Koop and Roberto Roberto Leon-Gonzalez and Rodney W. Strachan},
	title = {Efficient Posterior Simulation for Cointegrated Models with Priors
	on the Cointegration Space},
	journal = {Econometric Reviews},
	year = {2010},
	volume = {29},
	pages = {224-242},
	number = {2},
	doi = {10.1080/07474930903382208},
	eprint = { http://dx.doi.org/10.1080/07474930903382208 },
	url = { http://dx.doi.org/10.1080/07474930903382208 
	}
}
@article{Koopman2002,
	issn = "0883-7252",
	journal = "Journal of Applied Econometrics",
	pages = "667--689",  
	volume = "17",
	publisher = "John Wiley & Sons, Ltd.",
	number = "6",
	year = "2002",
	title = "The stochastic volatility in mean model: empirical evidence from international stock markets",
	language = "eng",
	address = "Chichester, UK",
	author = "Koopman, Siem Jan and  Eugenie, Hol Uspensky",
	keywords = "Economics;",
	month = "December",
}
@ARTICLE{Korobilis2013,
	author = {Dimitris Korobilis},
	title = {Hierarchical shrinkage priors for dynamic regressions with many predictors 
	},
	journal = {International Journal of Forecasting },
	year = {2013},
	volume = {29},
	pages = {43 - 59},
	number = {1},
	doi = {http://dx.doi.org/10.1016/j.ijforecast.2012.05.006},
	issn = {0169-2070},
	keywords = {Forecasting},
	url = {http://www.sciencedirect.com/science/article/pii/S0169207012000817}
}


@TechReport{Kaufmann2013,
	author={Sylvia Kaufmann and Christian Schumacher},
	title={{Bayesian estimation of sparse dynamic factor models with order-independent identification}},
	year=2013,
	month=4,
	institution={Swiss National Bank, Study Center Gerzensee},
	type={Working Papers},
	url={https://ideas.repec.org/p/szg/worpap/1304.html},
	number={13.04},
	abstract={The analysis of large panel data sets (with N variables) involves methods of dimension reduction and optimal information extraction. Dimension reduction is usually achieved by extracting the common variation in the data into few factors (k, where k},
	keywords={},
	doi={},
}

%==============JJJJ========================================
@article{Joshua2017,
	author={Joshua C.C. Chan and Roberto Leon-Gonzalez and Rodney W. Strachan},
	title={{Invariant Inference and Efficient Computation in the Static Factor Model}},
	year=2017,
	journal = {Journal of the American Statistical Association, forthcoming},
	abstract={Factor models are used in a wide range of areas. Two issues with Bayesian versions of these models are a lack of invariance to ordering of the variables and computational inefficiency. This paper develops invariant and efficient Bayesian methods for estimating static factor models. This approach leads to inference on the number of factors that does not depend upon the ordering of the variables, and we provide arguments to explain this invariance. Beginning from identified parameters which have nonstandard forms, we use parameter expansions to obtain a specification with standard conditional posteriors. We show significant gains in computational efficiency. Identifying restrictions that are commonly employed result in interpretable factors or loadings and, using our approach, these can be imposed ex-post. This allows us to investigate several alternative identifying schemes without the need to respecify and resample the model. We apply our methods to a simple example using a macroeconomic dataset.},
	keywords={},
	doi={},
}
@ARTICLE{Joshua2015,
	author = {Joshua C. C. Chan and Eric Eisenstat},
	title = {Marginal Likelihood Estimation with the Cross-Entropy Method},
	journal = {Econometric Reviews},
	year = {2015},
	volume = {34},
	pages = {256-285},
	number = {3}, 
	doi = {10.1080/07474938.2014.944474},
	eprint = { http://dx.doi.org/10.1080/07474938.2014.944474 },
	url = { http://dx.doi.org/10.1080/07474938.2014.944474 
	}
}
@ARTICLE{Joshua2009,
	author = {Joshua C. C. Chan and  Ivan Jeliazkov},
	title = {Efficient Simulation and Integrated Likelihood Estimation in State Space Models },
	journal = {International Journal of Mathematical Modelling and Numerical Optimisation},
	year = {2009},
	volume = {1},
	pages = {101-120},
	number = {},
}
@article{Jacquier2004,
	title = "Bayesian analysis of stochastic volatility models with fat-tails and correlated errors",
	journal = "Journal of Econometrics",
	volume = "122",
	number = "1",
	pages = "185 - 212",
	year = "2004",
	issn = "0304-4076",
	doi = "https://doi.org/10.1016/j.jeconom.2003.09.001",
	url = "http://www.sciencedirect.com/science/article/pii/S0304407603002732",
	author = "Eric Jacquier and Nicholas G. Polson and Peter E. Rossi",
	keywords = "Stochastic volatility, MCMC, Bayes factor, Fat-tails, Leverage effect, Gibbs, Metropolis, GARCH"
}
%================LLL========================================
@BOOK{Lutkepohl2007,
	author = {L\"{u}tkepohl, Helmut},
	title = {New Introduction to Multiple Time Series Analysis},
	year = {2007},
	isbn = {3540262393, 9783540262398},
	publisher = {Springer Publishing Company, Incorporated},
} 
@article{Litterman1986,
	ISSN = {07350015},
	URL = {http://www.jstor.org/stable/1391384},
	abstract = {The results obtained in five years of forecasting with Bayesian vector autoregressions (BVAR's) demonstrate that this inexpensive, reproducible statistical technique is as accurate, on average, as those used by the best known commercial forecasting services. This article considers the problem of economic forecasting, the justification for the Bayesian approach, its implementation, and the performance of one small BVAR model over the past five years.},
	author = {Robert B. Litterman},
	journal = {Journal of Business & Economic Statistics},
	number = {1},
	pages = {25-38},
	publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
	title = {Forecasting with Bayesian Vector Autoregressions: Five Years of Experience},
	volume = {4},
	year = {1986}
}


%================MMM=========================================

@BOOK{Magnus2007,
	title = {{Matrix Differential Calculus with Applications in Statistics and Econometrics}},
	publisher = {John Wiley \& Sons},
	year = {2007},
	author = {Magnus, Jan, and Neudecker, Heinz},
	edition = {2007},
	priority = {2},
	url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0387001603}
}

@BOOK{Muirhead2005,
	title = {{Aspects of Multivariate Statistical Theory}},
	publisher = {John Wiley \& Sons},
	year = {2005},
	author = {Muirhead, Robb}
}
@TechReport{McCracken2015, 
	author={McCracken,Michael W.  and  Ng, Serena},
	title={{FRED-MD: A Monthly Database for Macroeconomic Research}},
	year=2015,
	month=12,
	institution={Federal Reserve Bank of St. Louis, working paper 2015-012A},
	
}
%===============NNN======================================

@ARTICLE{Neal2001,
	author = {Neal, Radford M.},
	title = {Annealed importance sampling},  
	journal = {Statistics and Computing},
	year = {2001},
	volume = {11},
	pages = {125--139},
	number = {2},
	abstract = {Simulated annealing---moving from a tractable distribution to a distribution
	of interest via a sequence of intermediate distributions---has traditionally
	been used as an inexact method of handling isolated modes in Markov
	chain samplers. Here, it is shown how one can use the Markov chain
	transitions for such an annealing sequence to define an importance
	sampler. The Markov chain aspect allows this method to perform acceptably
	even for high-dimensional problems, where finding good importance
	sampling distributions would otherwise be very difficult, while the
	use of importance weights ensures that the estimates found converge
	to the correct values as the number of annealing runs increases.
	This annealed importance sampling procedure resembles the second
	half of the previously-studied tempered transitions, and can be seen
	as a generalization of a recently-proposed variant of sequential
	importance sampling. It is also related to thermodynamic integration
	methods for estimating ratios of normalizing constants. Annealed
	importance sampling is most attractive when isolated modes are present,
	or when estimates of normalizing constants are required, but it may
	also be more generally useful, since its independent sampling allows
	one to bypass some of the problems of assessing convergence and autocorrelation
	in Markov chain samplers.},
	doi = {10.1023/A:1008923215028},
	issn = {1573-1375},
	url = {http://dx.doi.org/10.1023/A:1008923215028}
}

%**************OOO**********************************
@article{Ohagan1995,
	ISSN = {00359246},
	URL = {http://www.jstor.org/stable/2346088},
	abstract = {Bayesian comparison of models is achieved simply by calculation of posterior probabilities of the models themselves. However, there are difficulties with this approach when prior information about the parameters of the various models is weak. Partial Bayes factors offer a resolution of the problem by setting aside part of the data as a training sample. The training sample is used to obtain an initial informative posterior distribution of the parameters in each model. Model comparison is then based on a Bayes factor calculated from the remaining data. Properties of partial Bayes factors are discussed, particularly in the context of weak prior information, and they are found to have advantages over other proposed methods of model comparison. A new variant of the partial Bayes factor, the fractional Bayes factor, is advocated on grounds of consistency, simplicity, robustness and coherence.},
	author = {Anthony O'Hagan},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	number = {1},
	pages = {99-138},
	publisher = {[Royal Statistical Society, Wiley]},
	title = {Fractional Bayes Factors for Model Comparison},
	volume = {57},
	year = {1995}
}
@article{Omori2007,
	issn = "0304-4076",
	abstract = "This paper is concerned with the Bayesian analysis of stochastic volatility (SV) models with leverage. Specifically, the paper shows how the often used Kim et al. [1998. Stochastic volatility: likelihood inference and comparison with ARCH models. Review of Economic Studies 65, 361–393] method that was developed for SV models without leverage can be extended to models with leverage. The approach relies on the novel idea of approximating the joint distribution of the outcome and volatility innovations by a suitably constructed ten-component mixture of bivariate normal distributions. The resulting posterior distribution is summarized by MCMC methods and the small approximation error in working with the mixture approximation is corrected by a reweighting procedure. The overall procedure is fast and highly efficient. We illustrate the ideas on daily returns of the Tokyo Stock Price Index. Finally, extensions of the method are described for superposition models (where the log-volatility is made up of a linear combination of heterogenous and independent autoregressions) and heavy-tailed error distributions (student and log-normal).",
	journal = "Journal of Econometrics",
	pages = "425--449",
	volume = "140",
	publisher = "Elsevier B.V.",
	number = "2",
	year = "2007",
	title = "Stochastic volatility with leverage: Fast and efficient likelihood inference",
	language = "eng",
	author = "Omori, Yasuhiro and Chib, Siddhartha and Shephard, Neil and Nakajima, Jouchi",
	keywords = "Leverage Effect ; Markov Chain Monte Carlo ; Mixture Sampler ; Stochastic Volatility ; Stock Returns",
}
%==============PPP==================================
@TechReport{Pettenuzzo2016,
	author={Davide Pettenuzzo and Gary Koop and Dimitris Korobilis},
	title={{Bayesian Compressed Vector Autoregressions}},
	year=2016,
	month=03,
	institution={Brandeis University, Department of Economics and International Businesss School},
	type={Working Papers},
	url={https://ideas.repec.org/p/brd/wpaper/103.html},
	number={103},
	abstract={Macroeconomists are increasingly working with large Vector Autoregressions (VARs) where the number of parameters vastly exceeds the number of observations. Existing approaches either involve prior shrinkage or the use of factor methods. In this paper, we develop an alternative based on ideas from the compressed regression literature. It involves randomly compressing the explanatory variables prior to analysis. A huge dimensional problem is thus turned into a much smaller, more computationally tractable one. Bayesian model averaging can be done over various compressions, attaching greater weight to compressions which forecast well. In a macroeconomic application involving up to 129 variables, we find compressed VAR methods to forecast better than either factor methods or large VAR methods involving prior shrinkage.},
	keywords={multivariate time series; random projection; forecasting},
	doi={},
}


@ARTICLE{Park2008,
	author = {Trevor Park and George Casella},
	title = {The Bayesian Lasso},
	journal = {Journal of the American Statistical Association},
	year = {2008},
	volume = {103},
	pages = {681-686},
	number = {482},
	doi = {10.1198/016214508000000337},
	eprint = { http://dx.doi.org/10.1198/016214508000000337 },
	url = { http://dx.doi.org/10.1198/016214508000000337 
	}
}

%=============RRRRR================================
@Article{Robin2000,
	Title                    = {Tests of rank},
	Author                   = {Robin,Jean-Marc and Smith,Richard J.},
	Journal                  = {Econometric Theory},
	year                     = {2000},
	Pages                    = {151-175},
	Volume                   = {16},
	Doi                      = {null},
	ISSN                     = {1469-4360},
	Issue                    = {02},
	Numpages                 = {25},
	Url                      = {http://journals.cambridge.org/article_S0266466600162012}
}

@BOOK{Rubinstein2004,
	title = {The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization,
	Monte-Carlo Simulation, and Machine Learning},
	publisher = {New York: Springer-Verlag},
	year = {2004},
	author = {Reuven Y. Rubinstein, and D.P Kroese},
	owner = {uqttrin2},
	timestamp = {2016.07.07}
}
@BOOK{Reinsel1998,
	title = {{Multivariate Reduced-Rank Regression: Theory and Applications}},
	publisher = {Springer},
	year = {1988},
	author = {Reinsel, Gregory C., and Velu, Raja },
	owner = {uqttrin2},
	timestamp = {2014.07.18}
}

@article{Reinsel1983,
	ISSN = {00063444},
	URL = {http://www.jstor.org/stable/2335952},
	abstract = {We discuss methods for modelling multivariate autoregressive time series in terms of a smaller number of index series which are chosen to provide as complete a summary as possible of the past information contained in the original series necessary for prediction purposes. The maximum likelihood method of estimation and asymptotic properties of estimators of the coefficients which determine the index variables, as well as the corresponding autoregressive coefficients, are discussed. A numerical example is presented to illustrate the use of the autoregressive index models.},
	author = {Gregory Reinsel},
	journal = {Biometrika},
	number = {1},
	pages = {145-156},
	publisher = {[Oxford University Press, Biometrika Trust]},
	title = {Some Results on Multivariate Autoregressive Index Models},
	volume = {70},
	year = {1983}
}
@ARTICLE{Rubinstein1997,
	author = {Reuven Y. Rubinstein},
	title = {Optimization of computer simulation models with rare events },
	journal = {European Journal of Operational Research },
	year = {1997},
	volume = {99},
	pages = {89 - 112},
	number = {1},
	doi = {http://dx.doi.org/10.1016/S0377-2217(96)00385-2},
	issn = {0377-2217},
	keywords = {Inventory},
	url = {http://www.sciencedirect.com/science/article/pii/S0377221796003852}
}
@ARTICLE{Rao1979,
	author = {C.Radhakrishna Rao},
	title = {Separation theorems for singular values of matrices and their applications
	in multivariate analysis},
	journal = {Journal of Multivariate Analysis},
	year = {1979},
	volume = {9},
	pages = {362 - 377},
	number = {3},
	doi = {http://dx.doi.org/10.1016/0047-259X(79)90094-0},
	issn = {0047-259X},
	keywords = {Matrix approximations},
	url = {http://www.sciencedirect.com/science/article/pii/0047259X79900940}
}

@ARTICLE{Rasti2014, 
	author={B. Rasti and J. R. Sveinsson and M. O. Ulfarsson}, 
	journal={IEEE Transactions on Geoscience and Remote Sensing}, 
	title={Wavelet-Based Sparse Reduced-Rank Regression for Hyperspectral Image Restoration}, 
	year={2014}, 
	volume={52}, 
	number={10}, 
	pages={6688-6698}, 
	keywords={estimation theory;geophysical image processing;hyperspectral imaging;image classification;image denoising;image restoration;regression analysis;support vector machines;wavelet transforms;Stein unbiased risk estimation;WSRRR;cyclic descent-type algorithm;hyperspectral image restoration;image denoising;minimization problem;noisy data set simulation;random forest;signal-to-noise ratio;sparse regularization problem;spectral angle distance;support vector machine;wavelet-based sparse reduced-rank regression;Hyperspectral imaging;Image restoration;Multiresolution analysis;Noise;Noise reduction;Wavelet transforms;Classification;Stein's unbiased risk estimation (SURE);denoising;hyperspectral image restoration;sparse component analysis (SCA);sparse reduced-rank regression (SRRR);sparse regularization;wavelets}, 
	doi={10.1109/TGRS.2014.2301415}, 
	ISSN={0196-2892}, 
	month={Oct},}

@article{Robinson1974,
	ISSN = {00206598, 14682354},
	URL = {http://www.jstor.org/stable/2525734},
	author = {P. M. Robinson},
	journal = {International Economic Review},
	number = {3},
	pages = {680-692},
	publisher = {[Economics Department of the University of Pennsylvania, Wiley, Institute of Social and Economic Research, Osaka University]},
	title = {Identification, Estimation and Large-Sample Theory for Regressions Containing Unobservable Variables},
	volume = {15},
	year = {1974}
}
@InCollection{Romer1989,
	author={Christina D. Romer and David H. Romer},
	title={{Does Monetary Policy Matter? A New Test in the Spirit of Friedman and Schwartz}},
	booktitle={{NBER Macroeconomics Annual 1989, Volume 4}},
	publisher={National Bureau of Economic Research, Inc},
	year=1989,
	month={April},
	volume={},
	number={},
	series={NBER Chapters},
	edition={},
	chapter={},
	pages={121-184},
	doi={},
	keywords={},
	abstract={},
	url={https://ideas.repec.org/h/nbr/nberch/10964.html}
}



%=============SSS==================================

@BOOK{Schott2005,
	title = {{Matrix Analysis for Statistics}},
	publisher = {Wiley, New York},
	year = {2005},
	author = {Schott, J.R.},
	owner = {uqttrin2},
	timestamp = {2014.07.18}
}

@article{Strachan2003,
	ISSN = {07350015},
	URL = {http://www.jstor.org/stable/1392363},
	abstract = {Two methods of identifying cointegrating vectors are commonly used: linear restrictions and the nonlinear method of Johansen's maximum likelihood procedure. That the linear method can produce invalid estimates while the Johansen approach always produces valid estimates has been recognized in several recent articles. Because all Bayesian studies to date have used linear restrictions, this article presents a Bayesian method for obtaining estimates of cointegrating vectors that will always be valid. In addition, it also presents an approach for evaluating the validity of linear restrictions.},
	author = {Rodney W. Strachan},
	journal = {Journal of Business & Economic Statistics},
	number = {1},
	pages = {185-195},
	publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
	title = {Valid Bayesian Estimation of the Cointegrating Error Correction Model},
	volume = {21},
	year = {2003}
}
@ARTICLE{Strachan2004,
	author = {Rodney W. Strachan and Brett Inder},
	title = {Bayesian analysis of the error correction model },
	journal = {Journal of Econometrics },
	year = {2004},
	volume = {123},
	pages = {307 - 325},
	number = {2},
	doi = {http://dx.doi.org/10.1016/j.jeconom.2003.12.004},
	issn = {0304-4076},
	keywords = {Cointegration},
	url = {http://www.sciencedirect.com/science/article/pii/S0304407603002951}
}

@ARTICLE{Stock2002,
	author = {James H Stock and Mark W Watson},
	title = {Macroeconomic Forecasting Using Diffusion Indexes},
	journal = {Journal of Business \& Economic Statistics},
	year = {2002},
	volume = {20},
	pages = {147-162},
	number = {2},
	doi = {10.1198/073500102317351921},
	eprint = { http://dx.doi.org/10.1198/073500102317351921 },
	url = { http://dx.doi.org/10.1198/073500102317351921 
	}
}
@article{Sim1980,
	ISSN = {00129682, 14680262},
	URL = {http://www.jstor.org/stable/1912017},
	abstract = {Existing strategies for econometric analysis related to macroeconomics are subject to a number of serious objections, some recently formulated, some old. These objections are summarized in this paper, and it is argued that taken together they make it unlikely that macroeconomic models are in fact over identified, as the existing statistical theory usually assumes. The implications of this conclusion are explored, and an example of econometric work in a non-standard style, taking account of the objections to the standard style, is presented.},
	author = {Christopher A. Sims},
	journal = {Econometrica},
	number = {1},
	pages = {1-48},
	publisher = {[Wiley, Econometric Society]},
	title = {Macroeconomics and Reality},
	volume = {48},
	year = {1980}
}
@article{Smith1993,
	ISSN = {00359246},
	URL = {http://www.jstor.org/stable/2346063},
	abstract = {The use of the Gibbs sampler for Bayesian computation is reviewed and illustrated in the context of some canonical examples. Other Markov chain Monte Carlo simulation methods are also briefly described, and comments are made on the advantages of sample-based approaches for Bayesian inference summaries.},
	author = {A. F. M. Smith, G. O. Roberts},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	number = {1},
	pages = {3-23},
	publisher = {[Royal Statistical Society, Wiley]},
	title = {Bayesian Computation Via the Gibbs Sampler and Related Markov Chain Monte Carlo Methods},
	volume = {55},
	year = {1993}
}




%================TTT===================================='
@article{Tierney1986,
	ISSN = {01621459},
	URL = {http://www.jstor.org/stable/2287970},
	abstract = {This article describes approximations to the posterior means and variances of positive functions of a real or vector-valued parameter, and to the marginal posterior densities of arbitrary (i.e., not necessarily positive) parameters. These approximations can also be used to compute approximate predictive densities. To apply the proposed method, one only needs to be able to maximize slightly modified likelihood functions and to evaluate the observed information at the maxima. Nevertheless, the resulting approximations are generally as accurate and in some cases more accurate than approximations based on third-order expansions of the likelihood and requiring the evaluation of third derivatives. The approximate marginal posterior densities behave very much like saddle-point approximations for sampling distributions. The principal regularity condition required is that the likelihood times prior be unimodal.},
	author = {Luke Tierney and Joseph B. Kadane},
	journal = {Journal of the American Statistical Association},
	number = {393},
	pages = {82-86},
	publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
	title = {Accurate Approximations for Posterior Moments and Marginal Densities},
	volume = {81},
	year = {1986}
}

@article{Tibshirani1996,
	ISSN = {00359246},
	URL = {http://www.jstor.org/stable/2346178},
	abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	author = {Robert Tibshirani},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	number = {1},
	pages = {267-288},
	publisher = {[Royal Statistical Society, Wiley]},
	title = {Regression Shrinkage and Selection via the Lasso},
	volume = {58},
	year = {1996}
}

@misc{Taylor1982,
	title = {Financial returns modelled by the product of two stochastic processes: a study of daily sugar prices, 1961-79},
	journal = {Time series analysis: theory and practice},
	volume = {1},
	pages = {203--226},
	series= {Time series analysis : theory and practice. - Amsterdam [u.a.] : North-Holland, ZDB-ID 7214716. - Vol. 1.1981, p. 203-226},
	author = {Taylor, Stephen J.},
	address = {Amsterdam [u.a.]},
	publisher = {North-Holland},
	year = {1982},
	keywords = {Zuckermarkt},
	crossref = {https://www.econbiz.de/Record/financial-returns-modelled-by-the-product-of-two-stochastic-processes-a-study-of-daily-sugar-prices-1961-79-taylor-stephen/10002902593}
}


@ARTICLE{Tibshirani2005,
	author = {Tibshirani, Robert and Saunders, Michael and Rosset, Saharon and
	Zhu, Ji and Knight, Keith},
	title = {Sparsity and smoothness via the fused lasso},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	year = {2005},
	volume = {67},
	pages = {91--108},
	number = {1},
	doi = {10.1111/j.1467-9868.2005.00490.x},
	issn = {1467-9868},
	keywords = {Fused lasso, Gene expression, Lasso, Least squares regression, Protein
	mass spectroscopy, Sparse solutions, Support vector classifier},
	publisher = {Blackwell Publishing Ltd},
	url = {http://dx.doi.org/10.1111/j.1467-9868.2005.00490.x}
}


@ARTICLE{Ter1990,
	author = {Ter Braak, Cajo J. F.},
	title = {Interpreting canonical correlation analysis through biplots of structure
	correlations and weights},
	journal = {Psychometrika},
	year = {1990},
	volume = {55},
	pages = {519--531},
	number = {3},
	doi = {10.1007/BF02294765},
	issn = {1860-0980}}

%================WWW==================================
@ARTICLE{Warne2016,
	author = {Warne, Anders and Coenen, G\"{u}nter and Christoffel, Kai},
	title = {Marginalized Predictive Likelihood Comparisons of Linear Gaussian
	State-Space Models with application to {DSGE, DSGE-VAR and VAR} models},
	journal = {Journal of Applied Econometrics},
	Year                     = {2017},
	Number                   = {1},
	Pages                    = {103-119},
	Volume                   = {32},
}
@article{Wright2000,
	ISSN = {07350015},
	URL = {http://www.jstor.org/stable/1392131},
	abstract = {This article proposes using variance-ratio tests based on the ranks and signs of a time series to test the null that the series is a martingale difference sequence. Unlike conventional variance-ratio tests, these tests can be exact. In Monte Carlo simulations, I find that they can also be more powerful than conventional variance-ratio tests. I apply the proposed tests to five exchange-rate series and find that they are capable of detecting violations of the martingale hypothesis for all five series, whereas conventional variance-ratio tests yield ambiguous results.},
	author = {Jonathan H. Wright},
	journal = {Journal of Business & Economic Statistics},
	number = {1},
	pages = {1-9},
	publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
	title = {Alternative Variance-Ratio Tests Using Ranks and Signs},
	volume = {18},
	year = {2000}
}


@article{Wu2001,
	issn = "0893-9454",
	abstract = "Volatility in equity markets is asymmetric: contemporaneous return and conditional return volatility are negatively correlated. In this article I develop an asymmetric volatility model where dividend growth and dividend volatility are the two state variables of the economy. The model allows both the leverage effect and the volatility feedback effect, the two popular explanations of asymmetry. The model is estimated by the simulated method of moments. I find that both the leverage effect and volatility feedback are important determinants of asymmetric volatility, and volatility feedback is significant both statistically and economically.",
	journal = "The Review of Financial Studies",
	pages = "837--859",
	volume = "14",
	publisher = "Oxford University Press",
	number = "3",
	year = "2001",
	title = "The Determinants of Asymmetric Volatility",
	author = "Wu, Guojun",
	keywords = "United States ; Us ; Volatility ; Securities Markets ; Rates of Return ; Correlation Analysis ; Mathematical Models ; Dividends ; United States ; Investment Analysis & Personal Finance ; Experimental/Theoretical;",
	month = "July",
}
%================VVV====================================

@article{Velu1986,
	ISSN = {00063444},
	URL = {http://www.jstor.org/stable/2336276},
	abstract = {This paper is concerned with the investigation of reduced rank coefficient models for multiple time series. In particular, autoregressive processes which have a structure to their coefficient matrices similar to that of classical multivariate reduced rank regression are studied in detail. The estimation of parameters and associated asymptotic theory are derived. The exact correspondence between the reduced rank regression procedure for multiple autoregressive processes and the canonical analysis of Box & Tiao (1977) is briefly indicated. To illustrate the methods, U.S. hog data are considered.},
	author = {Raja P. Velu, and Gregory C. Reinsel, and Dean W. Wichern},
	journal = {Biometrika},
	number = {1},
	pages = {105-118},
	publisher = {[Oxford University Press, Biometrika Trust]},
	title = {Reduced Rank Models for Multiple Time Series},
	volume = {73},
	year = {1986}
}
@article{Villani2005,
	ISSN = {02664666, 14694360},
	URL = {http://www.jstor.org/stable/3533468},
	abstract = {A Bayesian reference analysis of the cointegrated vector autoregression is presented based on a new prior distribution. Among other properties, it is shown that this prior distribution distributes its probability mass uniformly over all cointegration spaces for a given cointegration rank and is invariant to the choice of normalizing variables for the cointegration vectors. Several methods for computing the posterior distribution of the number of cointegrating relations and distribution of the model parameters for a given number of relations are proposed, including an efficient Gibbs sampling approach where all inferences are determined from the same posterior sample. Simulated data are used to illustrate the procedures and for discussing the well-known issue of local nonidentification.},
	author = {Mattias Villani},
	journal = {Econometric Theory},
	number = {2},
	pages = {326-357},
	publisher = {Cambridge University Press},
	title = {Bayesian Reference Analysis of Cointegration},
	volume = {21},
	year = {2005}
}
@ARTICLE{Vounou2010,
	author = {Maria Vounou and Thomas E. Nichols and Giovanni Montana},
	title = {Discovering genetic associations with high-dimensional neuroimaging
	phenotypes: A sparse reduced-rank regression approach },
	journal = {NeuroImage },
	year = {2010},
	volume = {53},
	pages = {1147 - 1159},
	number = {3},
	doi = {http://dx.doi.org/10.1016/j.neuroimage.2010.07.002},
	issn = {1053-8119},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811910009535}
}

@ARTICLE{Vahid2002,
	author = {Vahid, Farshid, and Issler, Jo\tilde{a}o Victor },
	title = {The importance of common cyclical features in VAR analysis: a
	{Monte-Carlo} study },
	journal = {Journal of Econometrics },
	year = {2002},
	volume = {109},
	pages = {341 - 363},
	number = {2},
	doi = {http://dx.doi.org/10.1016/S0304-4076(02)00117-3},
	issn = {0304-4076},
	keywords = {Reduced rank models},
	url = {http://www.sciencedirect.com/science/article/pii/S0304407602001173}
}

%====================YYYYYY===============================
@article{Yu2005,
	issn = "0304-4076",
	abstract = "This paper is concerned with the specification for modelling financial leverage effect in the context of stochastic volatility (SV) models. Two alternative specifications co-exist in the literature. One is the Euler approximation to the well-known continuous time SV model with leverage effect and the other is the discrete time SV model of Jacquier et al. (J. Econometrics 122 (2004) 185). Using a Gaussian nonlinear state space form with uncorrelated measurement and transition errors, I show that it is easy to interpret the leverage effect in the conventional model whereas it is not clear how to obtain and interpret the leverage effect in the model of Jacquier et al. Empirical comparisons of these two models via Bayesian Markov chain Monte Carlo (MCMC) methods further reveal that the specification of Jacquier et al. is inferior. Simulation experiments are conducted to study the sampling properties of Bayes MCMC for the conventional model.",
	journal = "Journal of Econometrics",
	pages = "165--178",
	volume = "127",
	publisher = "Elsevier B.V.",
	number = "2",
	year = "2005",
	title = "On leverage in a stochastic volatility model",
	language = "eng",
	author = "Yu, Jun",
	keywords = "C11 ; C15 ; G12 ; Bayes Factors ; Leverage Effect ; Markov Chain Monte Carlo ; Nonlinear State Space Models ; Quasi Maximum Likelihood ; Particle Filter",
}
%====================ZZZZZZ===============================
@article{Zakoian1994,
	title = "Threshold heteroskedastic models",
	journal = "Journal of Economic Dynamics and Control",
	volume = "18",
	number = "5",
	pages = "931 - 955",
	year = "1994",
	issn = "0165-1889",
	doi = "https://doi.org/10.1016/0165-1889(94)90039-6",
	url = "http://www.sciencedirect.com/science/article/pii/0165188994900396",
	author = "Jean-Michel Zakoian",
	keywords = "GARCH models, Asymmetries in volatility, Stationarity"
}

@BOOK{Zellner1971,
	title = {{An Introduction to Bayesian Inference in Econometrics}},
	publisher = {Wiley, New York},
	year = {1971},
	author = {Zellner, A.},
	owner = {uqttrin2},
	timestamp = {2014.07.18}
}


@INCOLLECTION{Zellner1986,
	author = {Zellner,.A.},
	title = {On assessing prior distributions and Bayesian regression analysis
	with g-prior distributions},
	booktitle = {in P.K. Goel and A.Zellner, eds., Bayesian inference and decision
	techniques: Essays in hoor of Bruno de Finetti (North-Holland, Amsterdam)
	233-243},
	year = {1986},
	owner = {uqttrin2},
	timestamp = {2016.06.17}
}

@Comment{jabref-meta: databaseType:bibtex;}
